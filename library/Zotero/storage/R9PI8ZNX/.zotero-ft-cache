MACHINE LEARNING
A First Course for Engineers and Scientists
Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, Thomas B. Sch√∂n Draft version: April 30, 2021
This material will be published by Cambridge University Press. This pre-publication version is free to view and download for personal use only. Not
for re-distribution, re-sale or use in derivative works. ¬© The authors, 2021. Feedback and exercise problems: http://smlbook.org

Contents

Notation

5

1 Introduction

7

1.1 Machine learning exempliÔ¨Åed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

1.2 About this book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

1.3 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2 Supervised learning: a Ô¨Årst approach

17

2.1 Supervised machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

2.2 A distance-based method: k-NN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2.3 A rule-based method: Decision trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

2.4 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

3 Basic parametric models and a statistical perspective on learning

37

3.1 Linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

3.2 ClassiÔ¨Åcation and logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

3.3 Polynomial regression and regularization . . . . . . . . . . . . . . . . . . . . . . . . . 51

3.4 Generalized linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

3.5 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

3.A Derivation of the normal equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

4 Understanding, evaluating and improving the performance

57

4.1 Expected new data error ùê∏new: performance in production . . . . . . . . . . . . . . . . 57 4.2 Estimating ùê∏new . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 4.3 The training error‚Äìgeneralization gap decomposition of ùê∏new . . . . . . . . . . . . . . . 63 4.4 The bias-variance decomposition of ùê∏new . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.5 Additional tools for evaluating binary classiÔ¨Åers . . . . . . . . . . . . . . . . . . . . . . 75

4.6 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

5 Learning parametric models

79

5.1 Principles of parametric modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

5.2 Loss functions and likelihood-based models . . . . . . . . . . . . . . . . . . . . . . . . 82

5.3 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93

5.4 Parameter optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

5.5 Optimization with large datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

5.6 Hyperparameter optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

5.7 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

6 Neural networks and deep learning

113

6.1 The neural network model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113

6.2 Training a neural network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

6.3 Convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

6.4 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

6.5 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

6.A Derivation of the backpropagation equations . . . . . . . . . . . . . . . . . . . . . . . . 133

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

3

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Contents

7 Ensemble methods: Bagging and boosting

135

7.1 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

7.2 Random forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

7.3 Boosting and AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144

7.4 Gradient boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

7.5 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156

8 Nonlinear input transformations and kernels

157

8.1 Creating features by nonlinear input transformations . . . . . . . . . . . . . . . . . . . . 157

8.2 Kernel ridge regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

8.3 Support vector regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163

8.4 Kernel theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

8.5 Support vector classiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172

8.6 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175

8.A The representer theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175

8.B Derivation of support vector classiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . 176

9 The Bayesian approach and Gaussian processes

179

9.1 The Bayesian idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179

9.2 Bayesian linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181

9.3 The Gaussian process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186

9.4 Practical aspects of the Gaussian process . . . . . . . . . . . . . . . . . . . . . . . . . . 195

9.5 Other Bayesian methods in machine learning . . . . . . . . . . . . . . . . . . . . . . . 200

9.6 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200

9.A The multivariate Gaussian distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 201

10 Generative models and learning from unlabeled data

203

10.1 The Gaussian mixture model and discriminant analysis . . . . . . . . . . . . . . . . . . 203

10.2 Cluster analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212

10.3 Deep generative models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221

10.4 Representation learning and dimensionality reduction . . . . . . . . . . . . . . . . . . . 226

10.5 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232

11 User aspects of machine learning

235

11.1 DeÔ¨Åning the machine learning problem . . . . . . . . . . . . . . . . . . . . . . . . . . 235

11.2 Improving a machine learning model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238

11.3 What if we cannot collect more data? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243

11.4 Practical data issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247

11.5 Can I trust my machine learning model? . . . . . . . . . . . . . . . . . . . . . . . . . . 249

11.6 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250

12 Ethics in machine learning

251

12.1 Fairness and error functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251

12.2 Misleading claims about performance . . . . . . . . . . . . . . . . . . . . . . . . . . . 254

12.3 Limitations of training data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260

12.4 Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263

Notation

265

Bibliography

267

4

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Acknowledgments
There are many people that have helped us throughout the writing of this book. First of all we want to mention David Sumpter who, in addition for giving feedback from using the material for teaching, contributed with the entire chapter 12 on ethical aspects. We have also received valuable feedback from many students and other teacher colleagues. We are of course very grateful for each and every comment we have received, and we want in particular to mention David Widmann, Adrian Wills, Johannes Hendricks, Mattias Villani, Dmitrƒ≥s Kass and Joel Oskarsson. We have also received useful feedback on the technical content of the book, including the practical insights in chapter 11 from Agrin Hilmkil (at Peltarion), Salla Franz√©n and Alla Tarighati (at SEB), Lawrence Murray (at Uber), James Hensman and Alexis Boukouvalas (at Secondmind), Joel Kronander and Nand Dalal (at Nines) and Peter Lindskog och Jacob Roll (at Arriver). We did also receive valuable comments from Arno Solin on chapters 8 and 9, and Joakim Lindblad on Chapter 6. There are several people who helped us with the Ô¨Ågures illustrating the examples in Chapter 1, namely Ant√¥nio Ribeiro (Figure 1.1), Fredrik K. Gustafsson (Figure 1.4) and Theodoros Damoulas (Figure 1.5). Thank you all for your help!
During the writing of this book, we enjoyed Ô¨Ånancial support from AI Competence for Sweden, the Swedish Research Council (projects: 2016-04278, 2016-06079, 2017-03807, 2020-04122), the Swedish Foundation for Strategic Research (projects: ICA16-0015, RIT12-0012), the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, ELLIIT and the Kjell och M√§rta Beƒ≥er Foundation.
We are Ô¨Ånally thankful to Lauren Cowles at Cambridge University Press for helpful advice and guidance through the publishing process.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

5

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

1 Introduction
Machine learning is about learning, reasoning, and acting based on data. This is done by constructing computer programs that process the data, extract useful information, make predictions regarding unknown properties, and suggest actions to take or decisions to make. What turns data analysis into machine learning, is that the process is automated and that the computer program is learnt from data. This means that generic computer programs are used, which are adapted to application-speciÔ¨Åc circumstances by automatically adjusting the settings of the program based on observed, so-called, training data. It can therefore be said that machine learning is a way of programming by examples. The beauty of machine learning is that it is quite arbitrary what the data represent, and we can design general methods that are useful for a wide range of practical applications in diÔ¨Äerent domains. We illustrate this via a range of examples below.
The ‚Äúgeneric computer program‚Äù referred to above corresponds to a mathematical model of the data. That is, when we develop and describe diÔ¨Äerent machine learning methods, we do this using the language of mathematics. The mathematical model describes a relationship between the involved quantities, or variables, that correspond to the observed data and the properties of interest (such as predictions, actions, etc.). Hence, the model is a compact representation of the data that, in a precise mathematical form, captures the key properties of the phenomenon we are studying. Which model to make use of is typically guided by the machine learning engineer‚Äôs insights generated when looking at the available data and the practitioner‚Äôs general understanding of the problem. When implementing the method in practice, this mathematical model is translated into code that can be executed on a computer. However, to understand what the computer program actually does, it is important to also understand the underlying mathematics.
As mentioned above, the model (or computer program) is learnt based on the available training data. This is accomplished by using a learning algorithm which is capable of automatically adjusting the settings, or parameters, of the model to agree with the data. In summary, the three cornerstones of machine learning are:
1. The data 2. The mathematical model 3. The learning algorithm
In this introductory chapter we will give a taste of the machine learning problem by illustrating these cornerstones with a few examples. They come from diÔ¨Äerent application domains and have diÔ¨Äerent properties, but nevertheless, they can all be addressed using similar techniques from machine learning. We also give some advise on how to proceed through the rest of the book and, at the end, provide references to good books on machine learning for the interested reader who wants to dig further into this topic.

1.1 Machine learning exempliÔ¨Åed
Machine learning is a multifaceted subject. We gave a brief and high-level description of what it entails above, but this will become much more concrete as we proceed throughout this book and introduce speciÔ¨Åc methods and techniques for solving various machine learning problems. However, before digging into the details we will try to give an intuitive answer to the question ‚ÄúWhat is machine learning?‚Äù, by discussing a few application examples where it can (and has) been used.
We start with an example related to medicine, more precisely cardiology.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

7

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Introduction
Example 1.1: Automatically diagnosing heart abnormalities
The leading cause of death globally is conditions that aÔ¨Äect heart and blood vessels, collectively referred to as cardiovascular diseases. Heart problems often inÔ¨Çuence the electrical activity of the heart, which can be measured using electrodes attached to the body. The electrical signals are reported in an electrocardiograms (ECG). In Figure 1.1 we show examples of (parts of) the measured signals from three diÔ¨Äerent hearts. The measurements stem from a healthy heart (top), a heart suÔ¨Äering from atrial Ô¨Åbrillation (middle), and a heart suÔ¨Äering from right bundle branch block (bottom). Atrial Ô¨Åbrillation makes the heart beat without order making it hard for the heart to pump blood in a normal way. Right bundle branch block corresponds to a delay or blockage in the electric pathways of the heart.

Fig. 1.1
By analyzing the ECG signal, a cardiologist gains valuable information about the condition of the heart, that can be used to diagnose the patient and plan the treatment.
To improve the diagnostic accuracy, as well as save time for the cardiologists, we can ask ourselves if this process can be automated to some extent. That is, can we construct a computer program which reads in the ECG signals, analyses the data, and returns a prediction regarding the normality or abnormality of the heart? Such models, capable of accurately interpreting an ECG exam in an automated fashion, will Ô¨Ånd applications globally, but they are most acute in low- and middle-income countries. An important reason for this is that the population in these countries often do not have easy and direct access to highly skilled cardiologists capable of accurately carrying out ECG diagnosis. Furthermore, cardiovascular diseases in these countries are related to more than 75% of the deaths.
The key challenge in building such a computer program is that it is far from obvious which computations that are needed for turning the raw ECG signal into a predication about the heart condition. Even if an experienced cardiologist would try to explain to a software developer which patterns in the data to look for, translating the cardiologist‚Äôs experience into a reliable computer program would be extremely challenging.
To tackle this diÔ¨Éculty, the machine learning approach is to instead learn the computer program by examples. SpeciÔ¨Åcally, instead of asking the cardiologist to specify a set of rules for how to classify an ECG signal as normal or abnormal, we simply ask the cardiologist (or a group of cardiologists) to label a large number of recorded ECG signals with labels corresponding to the the underlying heart condition. This is a much easier (albeit possibly tedious) way for the cardiologists to communicate their experience and encode it in a way which is interpretable by a computer.
The task of the learning algorithm is then to automatically adapt the computer program so that its predictions agree with the cardiologists‚Äô labels on the labeled training data. The hope is that, if it succeeds on the training data (where we already know the answer), then it should be possible to use the predictions made the by program on previously unseen data (where we do not know the answer) as well.
This is the approach taken by Ribeiro et al. (2020) who developed a machine learning model for ECG prediction. In their study, the training data consists of more than 2 300 000 ECG records from almost 1 700 000 diÔ¨Äerent patients of the state of Minas Gerais/Brazil. More speciÔ¨Åcally, each ECG corresponds to 12 time series (one from each of the twelve electrodes that were used in conducting the exam) of a duration between seven to ten seconds each, sampled at frequencies ranging from 300 Hz to 600 Hz. These ECGs can

8

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Machine learning exempliÔ¨Åed

be used to provide a full evaluation of the electric activity of the heart and it is indeed the most commonly used exam in evaluating the heart. Importantly, each ECG in the dataset also comes with an output sorting it into diÔ¨Äerent classes‚Äîno abnormalities, atrial Ô¨Åbrillation, right bundle branch block, etc.‚Äîaccording to the status of the heart. Based on this data, a machine learning model is trained to automatically classify a new ECG recording without requiring a human doctor to be involved. The model used is a deep neural network, more speciÔ¨Åcally a so-called residual network that is commonly used for images. The researchers adapted this to also work for the ECG signals of relevance for this study. In Chapter 6 we introduce deep learning models and their training algorithms.
Evaluating how a model like this will perform in practice is not straightforward. The approach taken in this study was to ask three diÔ¨Äerent cardiologists with experience in electrocardiography to examine and classify 827 ECG recordings from distinct patients. This dataset was then evaluated by the algorithm, two 4th year cardiology residents, two 3rd year emergency residents, and two 5th year medical students. The average performance was then compared. The result was that the algorithm achieved better or the same result when compared to the human performance on classifying six types of abnormalities.

Before we move on, let us pause and reÔ¨Çect on the example introduced above. In fact, many concepts that are central to machine learning can be recognized in this example.
As we mentioned above, the Ô¨Årst cornerstone of machine learning is the data. Taking a closer look at what the data actually is, we note that it comes in diÔ¨Äerent forms. First, we have the training data which is used to learn the model. Each training data point consists of both the ECG signal, which we refer to as the input, and its label corresponding to the type of heart condition seen in this signal, which we refer to as the output. To train the model we need access to both the inputs and the outputs, where the latter had to be manually assigned by domain experts (or possibly some auxiliary examination). Training a model from labeled data points is therefore referred to as supervised learning. We think of the learning as being supervised by the domain expert, and the learning objective is to obtain a computer program that can mimic the labeling done by the expert. Second, we have the (unlabeled) ECG signals that will be fed to the program when it is used ‚Äúin production‚Äù. It is important to remember that the ultimate goal of the model is to obtain accurate predictions in this second phase. We say that the predictions made by the model must generalize beyond the training data. How to learn models that are capable of generalizing, and how to evaluate to what extent they do, is a central theoretical question studied throughout this book (see in particular Chapter 4).
We illustrate the training of the ECG prediction model in Figure 1.2. The general structure of the training procedure is however the same (or at least very similar) for all supervised machine learning problems.

Training data
Labels e.g. healty, art. Ô¨Åb., RBBB

Unseen data

?

Model

prediction

Learning algorithm

update model

Model

prediction

Figure 1.2: Illustrating the supervised machine learning process with training to the left and then the use of the trained model to the right. Left: Values for the unknown parameters of the model are set by the learning algorithm such that the model best describes the available training data. Right: The learned model is used on new, previously unseen data, where we hope to obtain a correct classiÔ¨Åcation. It is thus essential that the model is able to generalize
to new data that is not present in the training data.

Another key concept that we encountered in the ECG example is the notion of a classiÔ¨Åcation problem.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

9

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Introduction
ClassiÔ¨Åcation is a supervised machine learning task which amounts to predicting a certain class, or label, for each data point. SpeciÔ¨Åcally, for classiÔ¨Åcation problems there are only a Ô¨Ånite number of possible output values. In the ECG example, the classes correspond to the type of heart condition. For instance, the classes could be ‚Äônormal‚Äô or ‚Äôabnormal‚Äô, in which case we refer to it as a binary classiÔ¨Åcation problem (only two possible classes). More generally, we could design a model for classifying each signal as either ‚Äônormal‚Äô, or assign it to one of a predetermined set of abnormalities. We then face a (more ambitious) multi-class classiÔ¨Åcation problem.
ClassiÔ¨Åcation is however not the only application of supervised machine learning that we will encounter. SpeciÔ¨Åcally, we will also study another type of problems referred to as regression problems. Regression diÔ¨Äers from classiÔ¨Åcation in that the output (that is, the quantity that we want the model to predict) is a numerical value. We illustrate with an example from material science.
Example 1.2: Formation energy of crystals
Much of our technological development is driven by the discovery of new materials with unique properties. Indeed, technologies such as touch screens and batteries for electric vehicles have emerged due to advances in materials science. Traditionally, materials discovery was largely done through experiments, but this is both time consuming and costly, which limited the number of new materials that could be found. Over the past few decades, computational methods have therefore played an increasingly important role. The basic idea behind computational materials science is to screen a very large number of hypothetical materials, predict various properties of interest by computational methods, and then attempt to experimentally synthesize the most promising candidates.
Crystalline solids (or, simply, crystals) are a central type of inorganic materials. In a crystal, the atoms are arranged in a highly ordered microscopic structure. Hence, to understand the properties of such a material, it is not enough to know the proportion of each element in the material, but we also need to know how these elements (or atoms) are arranged into a crystal. A basic property of interest when considering a hypothetical material is therefore the formation energy of the crystal. The formation energy can be thought of as the energy that nature needs to spend to form the crystal from the individual elements. Nature strives for Ô¨Ånding a minimum energy conÔ¨Åguration. Hence, if a certain crystal structure is predicted to have a formation energy that is signiÔ¨Åcantly larger than alternative crystals composed of the same elements, then it is unlikely that it can be synthesized in a stable way in practice.
A classical method (going back to the 60‚Äôs) that can be used for computing the formation energy is, so-called, density functional theory (DFT). The DFT method, which is based on quantum mechanical modeling, paved the way for the Ô¨Årst breakthrough in computational materials science, enabling high throughput screening for materials discovery. That being said, the DFT method is computationally very heavy and even with modern supercomputers, only a small fraction of all potentially interesting materials have been analyzed.
To handle this limitation, much recent interest has been paid to using machine learning for materials discovery, with the potential of resulting in a second computational revolution. By training a machine learning model to, for instance, predict the formation energy‚Äîbut in a fraction of the computational time required by DFT‚Äîa much larger range of candidate materials can be investigated.
As a concrete example, Faber et al. (2016) used a machine learning method referred to as kernel ridge regression (see Chapter 8) to predict the formation energy of around 2 million, so-called, elpasolite crystals. The machine learning model is a computer program which takes a candidate crystal as input (essentially, a description of the positions and elemental types of the atoms in the crystal), and is asked to return a prediction of the formation energy. To train the model, 10 000 crystals were randomly selected and their formation energies were computed using DFT. The model was then trained to predict formation energies to agree as closely as possible with the DFT output on the training set. Once trained, the model was used to predict the energy on the remaining ‚àº 99.5% of the potential elpasolites. Among these, 128 new crystal structures were found to have a favorable energy, thereby being potentially stable in nature.
Comparing the two examples discussed above, we can make a few interesting observations. As already pointed out, a diÔ¨Äerence is that the ECG model is asked to predict a certain class (say, normal or abnormal), whereas the materials discovery model is asked to predict a numerical value (the formation energy of a crystal). These are the two main types of prediction problems that we will study in this book, referred to as classiÔ¨Åcation and regression, respectively. While conceptually similar, we often use slight variations of the underpinning mathematical models, depending on the problem type. It is therefore instructive to treat

10

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

them separately.

. Machine learning exempliÔ¨Åed

Both types are supervised learning problems, though. That is, we train a predictive model to mimic the predictions made by a ‚Äúsupervisor‚Äù. However, it is interesting to note that the supervision is not necessarily done by a human domain expert. Indeed, for the formation energy model, the training data was obtained by running automated (but costly) density functional theory computations. In other situations we might obtain the output values naturally when collecting the training data. For instance, assume that you want to build a model for predicting the outcome of a soccer match based on data about the players in the two teams. This is a classiÔ¨Åcation problem (the output is ‚Äôwin‚Äô, ‚Äôlose‚Äô, or ‚Äôtie‚Äô) but the training data does not have to be manually labeled, since we get the labels directly from historical matches. Similarly, if you want to build a regression model for predicting the price of an apartment based on its size, location, condition, etc., then the output (the price) is obtained directly from historical sales.

Finally, it is worth noting that, although the examples discussed above correspond to very diÔ¨Äerent application domains, the problems are quite similar from a machine learning perspective. Indeed, the general procedure outlined in Figure 1.2 is applicable, with minor modiÔ¨Åcations, also for the materials discovery problem. This generality and versatility of machine learning methodology is one of its main strengths and beauties.

In this book we will make use of statistics and probability theory to describe the models used for making predictions. Using probabilistic models allows us to systematically represent and cope with the uncertainty in the predictions. In the examples above it is perhaps not obvious why this is needed. It could (perhaps) be argued that there is a ‚Äúcorrect answer‚Äù both in the ECG problem and the formation energy problem. Therefore, we might expect that the machine learning model should be able to provide a deÔ¨Ånite answer in its prediction. However, even in situations when there is a correct answer, machine learning models rely on various assumptions and they are learned from data using computational learning algorithms. With probabilistic models, we are able to represent the uncertainty in the model‚Äôs predictions, whether it originates from the data, the modeling assumptions, or the computation. Furthermore, in many applications of machine learning, the output is uncertain in itself and there is no such thing as a deÔ¨Ånite answer. To highlight the need for probabilistic predictions, let us consider an example from sports analytics.

Example 1.3: Probability of scoring a goal in soccer
Soccer is a sport where lots of data has been collected on how individual players act throughout a match, how teams collaborate, how they perform over time, etc. All this data is used to better understand the game and to help players reach their full potential.
Consider the problem of predicting whether or not a shot results in a goal. To this end, we use a rather simple model, where the prediction is based only on the player‚Äôs position on the Ô¨Åeld when taking the shot. SpeciÔ¨Åcally, the input is given by the distance from the goal and the angle between two lines drawn from the player‚Äôs position to the goal posts; see Figure 1.3. The output corresponds to whether or not the shot results in a goal, meaning that this is a binary classiÔ¨Åcation problem.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

11

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Introduction

1

0.9

ùùã

0.8

0.7

Frequency of goals

0.6

ùùã

0.5

0.4

0.3

0.2

0.1

Fig.

1.3

ùùã

0.0

Clearly, knowing the player‚Äôs position is not enough to deÔ¨Ånitely say if the shot will be successful. Still, it is reasonable to assume that it provides some information about the chance of making a goal. Indeed, a shot close to the goal line with a large angle is intuitively more likely to result in a goal than one made from a position close to the sideline. To acknowledge this fact when constructing a machine learning model, we will not ask the model to predict the outcome of the shot, but rather to predict the probability of a goal. This is accomplished by using a probabilistic model which is trained by maximizing the total probability of the observed training data with respect to the probabilistic predictions. For instance, using a so-called logistic regression model (see Chapter 3) we obtain a predicted probability of scoring a goal form any position, illustrated using a heat map in the right panel in Figure 1.3.

The supervised learning problems mentioned above were categorized as either classiÔ¨Åcation or regression problems, depending on the type of output. These problem categories are the most common and typical instances of supervised machine learning, and they will constitute the foundation for most methods discussed in this book. However, machine learning is in fact much more general and can be used to build complex predictive models that do not naturally Ô¨Åt into either the classiÔ¨Åcation or the regression category. To whet the appetite for further exploration of the Ô¨Åeld of machine learning, we provide two such examples below. These examples go beyond the speciÔ¨Åc problem formulations that we explicitly study in this book, but they nevertheless build on the same core methodology.
In the Ô¨Årst of these two examples we illustrate a computer vision capability, namely how to classify each individual pixel of an image into a class describing the object that the pixel belongs to. This has important applications in for example autonomous driving and medical imaging. When compared to the earlier examples this introduces an additional level of complexity, in that the model needs to be able to handle spatial dependencies across the image in its classiÔ¨Åcations.
Example 1.4: Pixel-wise class prediction
When it comes to machine vision an important capability is to be able to associate each pixel in an image with a corresponding class, see Figure 1.4 for an illustration in an autonomous driving application. This is referred to as semantic segmentation. In autonomous driving it is used to separate cars, road, pedestrians, etc. The output is then used as input to other algorithms, for instance for collision avoidance. When it comes to medical imaging, semantic segmentation is used, for instance, to tell apart diÔ¨Äerent organs and tumors.

12

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Machine learning exempliÔ¨Åed

Fig. 1.4
To train a semantic segmentation model, the training data consist of a large number of images (inputs). For each such image there is a corresponding output image of the same size, where each pixel has been labeled by hand to belong to a certain class. The supervised machine learning problem then amounts to using this data to Ô¨Ånd a mapping that is capable of taking a new unseen image and produce a corresponding output in the form of a predicted class for each pixel. Essentially, this is a type of classiÔ¨Åcation problem, but all pixels need to be classiÔ¨Åed simultaneously while respecting the spatial dependencies across the image to result in a coherent segmentation.
The bottom part of Figure 1.4 shows the prediction generated by such an algorithm, where the aim is to classify each pixel as either car (blue), traÔ¨Éc sign (yellow), pavement (purple), or tree (green). The best performing solutions for this task are today relying on cleverly crafted deep neural networks, see Chapter 6.
In the Ô¨Ånal example we raise the bar even higher, since there the model needs to be able to explain dependencies not only over space, but also over time, in a so-called spatio-temporal problem. These problems are Ô¨Ånding more and more applications as we get access to more and more data. More precisely we look into the problem of how to build probabilistic models capable of better estimating and forecasting air pollution across time and space in a city, in this case London.
Example 1.5: Estimating air pollution levels across London
Roughly 91% of the world‚Äôs population live in places where the air quality levels are worse than those recommended by the world health organization. Recent estimates indicate that 4.2 million people die each year as a result of ambient air pollution due to stroke, heart disease, lunch cancer and chronic respiratory diseases.
A natural Ô¨Årst step in dealing with this problem is to develop technology to measure and aggregate information about the air pollution levels across time and space. Such information open up for the development of machine learning models to better estimate and accurately forecast air pollution, which in turn allow for suitable interventions. The work that we feature here sets out to do this for the city of London where more than 9 000 people die early every year as a result of air pollution.
Air quality sensors are now‚Äîas opposed to the situation in recent past‚Äîavailable at relatively low cost. This, combined with an increasing awareness of the problem, has made interested companies, individuals, non-proÔ¨Åt organizations and community groups to contribute by setting up sensors and making the data available. More speciÔ¨Åcally, the data in this example comes from a sensor network of ground sensors providing hourly readings of NO2 and hourly satellite data at a spatial resolution of 7 km √ó 7 km. The resulting supervised machine learning problem is to build a model that can deliver forecasts of the air pollution level across time and space. Since the output‚Äîpollution level‚Äîis a continuous variable this is a type of regression problem. The particularly challenging aspect here is that the measurements are reported at diÔ¨Äerent spatial resolutions and at varying time scales.
The technical challenge in this problem amounts to merging the information from many sensors of diÔ¨Äerent

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

13

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Introduction
kinds reporting their measurements on diÔ¨Äerent spatial scales, sometimes referred to as a multi-sensor multi-resolution problem. Besides the problem under consideration here problems of this kind Ô¨Ånd many diÔ¨Äerent applications. The basis for the solution providing estimates exempliÔ¨Åed in Figure 1.5 is the Gaussian process (see Chapter 9).

Fig. 1.5
Figure 1.5 illustrates the output from the Gaussian process model in terms of spatio-temporal estimation and forecasting of NO2 levels in London. To the left we have the situation on February 19, 2019 at 11:00 using observations from both ground sensors providing hourly readings of NO2 and satellite data. To the right we have the situation on February 19, 2019 at 17:00 using only the satellite data.
The Gaussian process is a non-parametric and probabilistic model for nonlinear functions. Non-parametric means that it does not rely on any particular parametric functional form to be postulated. The fact that it is a probabilistic model means that it is capable of representing and manipulating uncertainty in a systematic way.
1.2 About this book
The aim of this book is to convey the spirit of supervised machine learning, without requiring any previous experience in the Ô¨Åeld. We focus on the underlying mathematics as well as the practical aspects. This book is a textbook, it is not a reference work nor a programming manual. It therefore contains only a careful (yet comprehensive) selection of supervised machine learning methods and no programming code. There are by now many well-written and well-documented code packages available, and it is our Ô¨Årm belief that with a good understanding of the mathematics and the inner workings of the methods, the reader will be able to make the connection between this book and her favorite code package in her favorite programming language.
We take a statistical perspective in this book, meaning that we discuss and motivate methods in terms of their statistical properties. It therefore requires some previous knowledge in statistics and probability theory, as well as calculus and linear algebra. We hope that reading the book from start to end will give the reader a good starting point for working as a machine learning engineer and/or pursuing further studies within the subject.
The book is written such that it can be read back to back. There are, however, multiple possible paths through the book that are more selective depending on the interest of the reader. Figure 1.6 illustrates the major dependencies between the chapters. In particular the most fundamental topics are discussed in Chapter 2, 3 and 4, and we do recommend the reader to read those chapters before proceeding to the later chapters that contain technically more advanced topics (Chapter 5-9). Chapter 10 goes beyond the supervised setting of machine learning, and Chapter 11 focuses on some of the more practical aspects on designing a successful machine learning solution and has a less technical nature than the preceding chapters. Finally, Chapter 12 (written by David Sumpter) discusses certain ethical aspects of modern machine learning.

14

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. About this book

Advanced chapters Fundamental chapters

2: Supervised learning: a Ô¨Årst approach

3: Basic parametric models and a statistical perspective on learning

4: Understanding, evaluating and improving the performance

5: Learning parametric models

5.4,

6: Neural networks and deep learning

5.5

5.2 (for 8.3, 8.5)

8.1,

8.2,

8: Nonlinear input transforma- 8.4 9: The Bayesian approach and

tions and kernels

Gaussian processes

7: Ensemble methods: Bagging and boosting

Special chapters

10: Generative models and learning from unlabeled data

11: User aspects of machine learning

12: Ethics in machine learning

Figure 1.6: The structure of this book, illustrated by blocks (chapters) and arrows (recommended order to read the chapters). We do recommend everyone to read (or at least skim) the fundamental material in Chapter 2, 3 and 4 Ô¨Årst. The path through the technically more advanced Chapter 5-9, can be chosen to match the particular interest of the reader. For Chapter 11, 10 and 12 we recommend reading the fundamental chapters Ô¨Årst.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

15

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Introduction
1.3 Further reading
There are by now quite a few extensive textbooks available on the topic of machine learning which introduce the area in diÔ¨Äerent ways compared to what we do in this book. We will only mention a few here. The book of Hastie et al. 2009 introduce the area of statistical machine learning in a mathematically solid and accessible manner. A few years later the authors released a diÔ¨Äerent version of their book (James et al. 2013) which is mathematically signiÔ¨Åcantly lighter, conveying the main ideas in an even more accessible manner. These books do not venture long neither into the world of Bayesian methods nor the world of neural networks. However, there are several complementary books doing exactly that, see e.g. Bishop (2006) and Murphy (2021). MacKay (2003) provided a rather early account drawing interesting and useful connections to information theory. It is still very much worth looking into. The book by Shalev-Shwartz and Ben-David (2014) provides an introduction with a clear focus on the underpinning theoretical constructions, connecting very deep questions‚Äîsuch as ‚Äúwhat is learning?‚Äù and ‚Äúhow can a machine learn‚Äù‚Äîwith mathematics. It is a perfect book for those of our readers that would like to deepen their understanding of the theoretical background of the area. We also mention the work of Efron and Hastie (2016), where the authors take a constructive historical approach to the development of the area covering the revolution in data analysis that emerged with the computers. Contemporary introductions to the mathematics of machine learning are provided by Strang (2019) and Deisenroth et al. (2019).
For a full account of the work on automatic diagnosis of heart abnormalities, see Ribeiro et al. 2020 and for a general introduction to use of machine learning‚Äîin particular deep learning‚Äîin medicine we point to Topol (2019). The application of kernel ridge regression to elpasolite crystals was borrowed from Faber et al. (2016). Other applications of machine learning in materials science are reviewed in the collection edited by Sch√ºtt et al. (2020). The London air pollution study was published by Hamelƒ≥nck et al. (2019) where the authors introduce interesting and useful developments of the Gaussian process model that we explain in Chapter 9. When it comes to semantic segmentation, the ground-breaking work of Long et al. 2015 has received a massive interest. The two main base lines for the current development in semantic segmentation are Zhao et al. 2017 and L.-C. Chen et al. 2017. A thorough introduction to the mathematics of soccer is provided in the book by D. Sumpter (2016) and a starting point to recent ideas on how to assess the impact of player actions is given in Decroos et al. (2019).

16

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

2 Supervised learning: a Ô¨Årst approach
In this chapter we will introduce the supervised machine learning problem, as well as two basic machine learning methods for solving it. The methods we will introduce are called ùëò-NN and decision trees. These two methods are relatively simple and we will derive them on intuitive grounds. Still, these methods are useful in their own right and are therefore a good place to start. Understanding their inner workings, advantages and shortcomings also lays a good foundation for the more advanced methods that are to come in later chapters.
2.1 Supervised machine learning
In supervised machine learning we have some training data that contains examples of how some input1 variable x relates to an output2 variable ùë¶. By using some mathematical model or method, which we adapt to the training data, our goal is to predict the output ùë¶ for a new, previously unseen, test data for which only x is known. We usually say that we learn (or train) a model from the training data, and that process involves some computations implemented in a computer.
Learning from labeled data
In most interesting supervised machine learning applications, the relationship between input x and output ùë¶ is diÔ¨Écult to describe explicitly. It may be too cumbersome or complicated to fully unravel from application domain knowledge, or even unknown. The problem can therefore usually not be solved by writing a traditional computer program that takes x as input and returns ùë¶ as output from a set of rules. The supervised machine learning approach is instead to learn the relationship between x and ùë¶ from data, which contains examples of observed pairs of input and output values. In other words, supervised machine learning amounts to learning from examples.
The data used for learning is called training data, and it has to consist of several input-output data points (samples) (xùëñ, ùë¶ùëñ), in total ùëõ of them. We will compactly write the training data as T = {xùëñ, ùë¶ùëñ }ùëñùëõ=1. Each data point in the training data provides a snapshot of how ùë¶ depends on x, and the goal in supervised machine learning is to squeeze as much information as possible out of T . In this book we will only consider problems where the individual data points are assumed to be (probabilistically) independent. This excludes, for example, applications in time series analysis where it is of interest to model the correlation between xùëñ and xùëñ+1.
The fact that the training data contains not only input values xùëñ, but also output values ùë¶ùëñ, is the reason for the term ‚Äúsupervised‚Äù machine learning. We may say that each input xùëñ is accompanied with a label ùë¶ùëñ, or simply that we have labeled data. For some applications, it is only a matter of jointly recording x and ùë¶. In other applications, the output ùë¶ has to be created by labeling the training data inputs x by a domain expert. For instance, to construct a training dataset for the cardiovascular disease application introduced in Chapter 1, a cardiologist needs to look at all training data inputs (ECG signals) xùëñ and label them by assigning to the variable ùë¶ùëñ to correspond to the heart condition that is seen in the signal. The entire learning process is thus ‚Äúsupervised‚Äù by the domain expert.
We use a vector boldface notation x to denote the input, since we assume it to be a ùëù-dimensional vector, x = [ùë•1 ùë•2 ¬∑ ¬∑ ¬∑ ùë• ùëù]T, where T denotes the transpose. Each element of the input vector x represent
1The input is commonly also called feature, attribute, predictor, regressor, covariate, explanatory variable, controlled variable and independent variable.
2The output is commonly also called response, regressand, label, explained variable, predicted variable or dependent variable.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

17

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach

some information that is considered to be relevant for the application at hand, for example the outdoor temperature or the unemployment rate. In many applications the number of inputs ùëù is large, or put diÔ¨Äerently, the input x is a high-dimensional vector. For instance, in a computer vision application where the input is a grayscale image, x can be all pixel values in the image, so ùëù = ‚Ñé √ó ùë§ where ‚Ñé and ùë§ denote the height and width of the input image.3 The output ùë¶, on the other hand, is often of low dimension and throughout most of this book we will assume that it is a scalar value. The type of the output value, numerical or categorical, turns out to be important and is used to distinguish between two subtypes of the supervised machine learning problems: regression and classiÔ¨Åcation. We will discuss this next.
Numerical and categorical variables
The variables contained in our data (input as well as output) can be of two diÔ¨Äerent types, numerical or categorical. A numerical variable has a natural ordering. We can say that one instance of a numerical variable is larger or smaller than another instance of the same variable. A numerical variable could for instance be represented by a continuous real number, but it could also be discrete, such as an integer. Categorical variables, on the other hand, are always discrete and importantly they lack a natural ordering. In this book we assume that any categorical variable can take only a Ô¨Ånite number of diÔ¨Äerent values. A few examples are given in Table 2.1 below.

Table 2.1: Examples of numerical and categorical variables.

Variable type

Example

Handled as

Number (continuous) Number (discrete) with natural ordering Number (discrete) without natural ordering Text string

32.23 km/h, 12.50 km/h, 42.85 km/h 0 children, 1 child, 2 children 1 = Sweden, 2 = Denmark, 3 = Norway Hello, Goodbye, Welcome

Numerical Numerical Categorical Categorical

The distinction between numerical and categorical is sometimes somewhat arbitrary. We could for instance argue that having no children is qualitatively diÔ¨Äerent from having children, and use the categorical variable ‚Äúchildren: yes/no‚Äù, instead of the numerical ‚Äú0, 1 or 2 children‚Äù. It is therefore a decision for the machine learning engineer whether a certain variable is to be considered as numerical or categorical.
The notion of categorical vs. numerical applies to both the output variable ùë¶ and to the ùëù elements ùë• ùëó of the input vector x = [ùë•1 ùë•2 ¬∑ ¬∑ ¬∑ ùë• ùëù]T. All ùëù input variables do not have to be of the same type. It is perfectly Ô¨Åne (and common in practice) to have a mix of categorical and numerical inputs.

ClassiÔ¨Åcation and regression
We distinguish between diÔ¨Äerent supervised machine learning problems by the type of the output ùë¶.
Regression means that the output is numerical, and classiÔ¨Åcation means that the output is categorical.
The reason for this distinction is that the regression and classiÔ¨Åcation problems have somewhat diÔ¨Äerent properties, and diÔ¨Äerent methods are used for solving them.
Note that the ùëù input variables x = [ùë•1 ùë•2 ¬∑ ¬∑ ¬∑ ùë• ùëù]T can be either numerical or categorical for both regression and classiÔ¨Åcation problems. It is only the type of the output that determines whether a problem is a regression or a classiÔ¨Åcation problem. A method for solving a classiÔ¨Åcation problems is called a classiÔ¨Åer.
For classiÔ¨Åcation the output is categorical and can therefore only take values in a Ô¨Ånite set. We use ùëÄ to denote the number of elements in the set of possible output values. It can, for instance, be {false, true}
3For image-based problems it is often more convenient to represent the input as a matrix of size ‚Ñé √ó ùë§, than as a vector of length ùëù = ‚Ñéùë§, but the dimension is nevertheless the same. We will get back to this in Chapter 6 when discussing the convolutional neural network, a model structure tailored to image-type inputs.

18

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Supervised machine learning

(ùëÄ = 2) or {Sweden, Norway, Finland, Denmark} (ùëÄ = 4). We will refer to these elements as classes or labels. The number of classes ùëÄ is assumed to be known in the classiÔ¨Åcation problem. To prepare for a concise mathematical notation, we use integers 1, 2, . . . , ùëÄ to denote the output classes if ùëÄ > 2. The ordering of the integers is arbitrary, and does not imply any ordering of the classes. When there are only ùëÄ = 2 classes, we have the important special case of binary classiÔ¨Åcation. In binary classiÔ¨Åcation we use the labels ‚àí1 and 1 (instead of 1 and 2). Occasionally we will also use the equivalent terms negative and positive class. The only reason for using a diÔ¨Äerent convention for binary classiÔ¨Åcation is that it gives a more compact mathematical notation for some of the methods, and carries no deeper meaning. Let us now have a look at a classiÔ¨Åcation and a regression problem, both of which will be used throughout this book.
Example 2.1: Classifying songs
Say that we want to build a ‚Äúsong categorizer‚Äù app, where the user records a song and the app answers by telling if the song has the artistic style of either the Beatles, Kiss or Bob Dylan. At the heart of this Ô¨Åctitious app there has to be a machinery that takes an audio recording as an input and returns an artist name.
If we Ô¨Årst collect some recordings with songs from the three groups/artists (where we know which artist is behind each song; a labeled dataset), we could use supervised machine learning to learn the characteristics of their diÔ¨Äerent styles and therefrom predict the artist of the new user-provided song. In the supervised machine learning terminology, the artist name (the Beatles, Kiss or Bob Dylan) is the output ùë¶. In this problem ùë¶ is categorical, and we are hence facing a classiÔ¨Åcation problem.
One of the important design choices for a machine learning engineer is a detailed speciÔ¨Åcation of what the input x really is. It would in principle be possible to consider the raw audio information as input, but that would give a very high-dimensional x which (unless an audio-speciÔ¨Åc machine learning method is used) most likely would require an unrealistically large amount of training data in order to be successful (we will discuss this aspect in detail in Chapter 4). A better option could therefore be to deÔ¨Åne some summary statistics of audio recordings and use those, so called features, as input x instead. As input features we could for example use the length of the audio recording and the ‚Äúperceptual energy‚Äù of the song. The length of a recording is easy to measure. Since it can diÔ¨Äer quite a lot between diÔ¨Äerent songs we take the logarithm of the actual length (in seconds) to get values in the same range for all songs. Such feature transformations are commonly used in practice to make the input data more homogeneous.
The energy of a songa is a bit more tricky and the exact deÔ¨Ånition may even be ambiguous. However, we leave that to the audio experts and re-use a piece of software that they have written for this purposeb without bothering too much about its inner workings. As long as this piece of software returns a number for any recording that is fed to it, and always returns the same number for the same recording) we can use it as an input to a machine learning method.
In Figure 2.1 we have plotted a dataset with 230 songs from the three artists. Each song is represented by a dot, where the horizontal axis is the logarithm of its length (measured in seconds) and the vertical axis the energy (on a scale 0-1). When we later return to this example, and apply diÔ¨Äerent supervised machine learning methods to it, this data will be the training data.

1
Help!
0.8

Rock and roll all nite

The Beatles Kiss Bob Dylan

Energy (scale 0-1)

0.6

0.4

Fig. 2.1

0.2
A hard rain‚Äôs a-gonna fall
0 4.4 4.6 4.8 5 5.2 5.4 5.6 5.8 6 6.2 6.4 6.6 6.8 7
Length (ln s)

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

19

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach

aWe use this term to refer to the perceived musical energy, not the signal energy in a strict sense. bSpeciÔ¨Åcally, we use http://api.spotify.com/ here.

Example 2.2: Car stopping distances
Ezekiel and Fox (1959) presents a dataset with 62 observations of the distance needed for various cars at diÔ¨Äerent initial speeds to break to a full stop.a The dataset has the following two variables:
- Speed: The speed of the car when the break signal is given. - Distance: The distance traveled after the signal is given until the car has reached a full stop.

150

Data

100

Distance (feet)

50

0

Fig.

0

10

20

30

40

2.2

Speed (mph)

To make a supervised machine learning problem out of it, we interpret Speed as the input variable ùë•, and Distance as the output variable ùë¶. Note that we use a non-bold symbol for the input here since it is a scalar value and not a vector of inputs in this example. Since ùë¶ is numerical, this is a regression problem. We then ask ourselves what the stopping distance would be if the initial speed would be, for example, at 33 mph or 45 mph respectively (two speeds at which no data has been recorded). Another way to frame this question is to ask for the prediction ùë¶(ùë•‚òÖ) for ùë•‚òÖ = 33 and ùë•‚òÖ = 45.
aThe data is somewhat dated, so the conclusions are perhaps not applicable to modern cars.

Generalizing beyond training data
There are two primary reasons for why it can be of interest to mathematically model the input‚Äìoutput relationships from training data.
(i) To reason about and explore how input and output variables are connected. An often encountered task in sciences such as medicine and sociology is to determine whether a correlation between a pair of variables exists or not (‚Äúdoes sea food increase the life expectancy?‚Äù). Such questions can be addressed by learning a mathematical model and carefully reason about the chance that the learned relationships between input x and output ùë¶ are due only to random eÔ¨Äects in the data, or if there appear to be some substance to the found relationships.
(ii) To predict the output value ùë¶‚òÖ for some new, previously unseen input x‚òÖ. By using some mathematical method which generalizes the input‚Äìoutput examples seen in the training, we can make a prediction ùë¶(x‚òÖ) for a previously unseen test input x‚òÖ. The hat indicates that the prediction is an estimate of the output.
These two objectives are sometimes used to roughly distinguish between classical statistics, focusing more on objective (i), and machine learning, where objective (ii) is more central. However, this is not a clear-cut distinction since predictive modeling is a topic of classical statistics too, and explainable models are studied also in machine learning. The primary focus in this book, however, is on making predictions, objective (ii) above, which is the foundation of supervised machine learning. Our overall goal is to obtain

20

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. A distance-based method k-NN
as accurate predictions ùë¶(x‚òÖ) as possible (measured in some appropriate way) for a wide range of possible test inputs x‚òÖ. We say that we are interested in methods that generalize well beyond the training data.
A method that generalizes well for the music example above would be able to correctly tell the artist of a new song which was not in the training data (assuming that the artist of the new song is one of the three that was present in the training data, of course). The ability to generalize to new data is a key concept of machine learning. It is not diÔ¨Écult to construct models or methods that give very accurate predictions if they are only evaluated on the training data (we will see an example in the next section). However, if the model is not able to generalize, meaning that the predictions are poor when the model is applied to new test data points, then the model is of little use in practice for making predictions. If that is the case we say that the model is overÔ¨Åtting to the training data. We will illustrate the issue of overÔ¨Åtting for a speciÔ¨Åc machine learning model in the next section and in Chapter 4 we will return to this concept using a more general and mathematical approach.

2.2 A distance-based method: k-NN
It is now time to encounter our Ô¨Årst actual machine learning method. We will start with the relatively simple ùëò-nearest neighbors (ùëò-NN) method, which can be used for both regression and classiÔ¨Åcation. Remember that the setting is that we have access to training data {xùëñ, ùë¶ùëñ }ùëñùëõ=1, which consists of ùëõ data points with input xùëñ and corresponding output ùë¶ùëñ. From this we want to construct a prediction ùë¶(x‚òÖ) for what we believe the output ùë¶‚òÖ would be for a new x‚òÖ, which we have not seen previously.

The k-nearest neighbors method

Most methods for supervised machine learning builds on the intuition that if the test data point x‚òÖ is close to training data point xùëñ, then the prediction ùë¶(x‚òÖ) should be close to ùë¶ùëñ. This is a general idea, but one simple way to implement it in practice is the following: Ô¨Årst, compute the Euclidean distance4 between the test input and all training inputs, xùëñ ‚àí x‚òÖ 2 for ùëñ = 1, . . . , ùëõ; second, Ô¨Ånd the data point x ùëó with the shortest distance to x‚òÖ and use its output as the prediction, ùë¶(x‚òÖ) = ùë¶ ùëó.
This simple prediction method is referred to as the 1-nearest neighbor method. It is not very complicated,
but for most machine learning applications of interest it is too simplistic. In practice we can rarely say for certain what the output value ùë¶ will be. Mathematically, we handle this by describing ùë¶ as a random variable. That is, we consider the data as noisy, meaning that it is aÔ¨Äected by random errors referred to as noise. From this perspective, the shortcoming of 1-nearest neighbor is that the prediction relies on only
one data point from the training data, which makes it quite ‚Äúerratic‚Äù and sensitive to noisy training data.
To improve the 1-nearest neighbor method we can extend it to make use of the ùëò nearest neighbors instead. Formally we deÔ¨Åne the set N‚òÖ = {ùëñ : xùëñ is one of the ùëò training data points closest to x‚òÖ} and aggregate the information from the ùëò outputs ùë¶ ùëó for ùëó ‚àà N‚òÖ to make the prediction. For regression problems we take the average of all ùë¶ ùëó for ùëó ‚àà N‚òÖ, and for classiÔ¨Åcation problems use a majority vote5. We illustrate the ùëò-nearest neighbors (ùëò-NN) method by Example 2.3 and summarize by Method 2.1.
Methods that explicitly use the training data when making predictions are referred to as nonparametric, and the ùëò-NN method is one example of this. This is in contrast with parametric methods, where the prediction is given by some function (a model), governed by a Ô¨Åxed number of parameters. For parametric
methods the training data is used to learn the parameters in an initial training phase, but once the model has been learned, the training data can be discarded since it is not used explicitly when making predictions.
We will introduce parametric modeling in Chapter 3.

4The Euclidean distance between a test point x‚òÖ and a training data point xùëñ is

‚àöÔ∏Å xùëñ ‚àí x‚òÖ 2 = (ùë•ùëñ1 ‚àí ùë•‚òÖ1)2 + (ùë•ùëñ2 ‚àí ùë•‚òÖ2)2.

Other distance functions can also be used, and will be discussed in Chapter 8. Categorical input variable can be handled as

we will discuss in Chapter 3.

5Ties can be handled in diÔ¨Äerent ways, for instance by a coin-Ô¨Çip, or by reporting the actual vote count to the end user who gets

to decide what to do with it.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

21

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach

Data: Training data {xùëñ, ùë¶ùëñ }ùëñùëõ=1 and test input x‚òÖ Result: Predicted test output ùë¶(x‚òÖ)
1 Compute the distances xùëñ ‚àí x‚òÖ 2 for all training data points ùëñ = 1, . . . , ùëõ 2 Let N‚òÖ = {ùëñ : xùëñ is one of the ùëò data points closest to x‚òÖ} 3 Compute the prediction ùë¶(x‚òÖ) as

ùë¶(x‚òÖ) =

Average{ùë¶ ùëó : ùëó ‚àà N‚òÖ} MajorityVote{ùë¶ ùëó : ùëó ‚àà N‚òÖ}

(Regression problems) (ClassiÔ¨Åcation problems)

Method 2.1: ùëò-nearest neighbor, ùëò-NN

Example 2.3: Predicting colors with ùëò-NN
We consider a synthetic binary classiÔ¨Åcation problem (ùëÄ = 2). We are given a training dataset with ùëõ = 6 observations of ùëù = 2 input variables ùë•1, ùë•2 and one categorical output ùë¶, the color Red or Blue,
ùëñ ùë•1 ùë•2 ùë¶
1 ‚àí1 3 Red 2 2 1 Blue 3 ‚àí2 2 Red 4 ‚àí1 2 Blue 5 ‚àí1 0 Blue 6 1 1 Red
and we are interested in predicting the output for x‚òÖ = [1 2]T. For this purpose we will explore two diÔ¨Äerent ùëò-NN classiÔ¨Åers, one using ùëò = 1 and one using ùëò = 3.
First, we compute the Euclidean distance xùëñ ‚àí x‚òÖ 2 between each training data point xùëñ (red and blue dots) and the test data point x‚òÖ (black dot), and then sort them in ascending order.

ùëñ xùëñ ‚àí x‚òÖ 2 ùë¶ùëñ

‚àö

6

‚àö1

Red

2

‚àö2

Blue

4

‚àö4

Blue

1

‚àö5

Red

5

‚àö8

Blue

Fig.

3

9

Red

2.3

ùë•2

4
ùëñ=1

2

ùëñ=3

0

ùëñ=5

ùëñ=4

ùëò =3
ùëò =1
ùëñ=6 ùëñ=2

‚àí2

0

2

ùë•1

Since the closest training data point to x‚òÖ is the data point ùëñ = 6 (Red), it means that for ùëò-NN with ùëò = 1, we get the prediction ùë¶(x‚òÖ) = Red. For ùëò = 3, the 3 nearest neighbors are ùëñ = 6 (Red), ùëñ = 2 (Blue), and ùëñ = 4 (Blue). Taking a majority vote among these three training data points, Blue wins with 2 votes against 1, so our prediction becomes ùë¶(x‚òÖ) = Blue. In Figure Ô¨Åg:ex:knn1, ùëò = 1 is represented by the inner circle and ùëò = 3 by the outer circle.

Decision boundaries for a classiÔ¨Åer
In Example 2.3 we only computed a prediction for one single test data point x‚òÖ. That prediction might indeed be the ultimate goal of the application, but in order to visualize and better understand a classiÔ¨Åer

22

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. A distance-based method k-NN
we can also study its decision boundary, which illustrates the prediction for all possible test inputs. We introduce the decision boundary using Example 2.4. It is a general concept for classiÔ¨Åers, not only ùëò-NN, but it is only possible to visualize easily when the dimension of x is ùëù = 2.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

23

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach

Example 2.4: Decision boundaries for the color example

In Example 2.3 we computed the prediction for x‚òÖ = [1 2]T. If we would shift that test point by one unit to the left at x‚òÖalt = [0 2]T the three closest training data points would still include ùëñ = 6 and ùëñ = 4 but now ùëñ = 2 is exchanged for ùëñ = 1. For ùëò = 3 this would give two votes for Red and one vote for Blue and we would therefore predict ùë¶ = Red. In between these two test data points x‚òÖ and x‚òÖalt, at [0.5 2]T, it is equally far to ùëñ = 1 as to ùëñ = 2 and it is undecided if the 3-NN classiÔ¨Åer should predict Red or Blue. (In practice this is
most often not a problem, since the test data points rarely end up exactly at the decision boundary. If they do,
this can be handled by a coin-Ô¨Çip.) For all classiÔ¨Åers we always end up with such points in the input space
where the class prediction abruptly changes from one class to another. These points are said to be on the
decision boundary of the classiÔ¨Åer. Continuing in a similar way, changing the location of the test input across the entire input space and
recording the class prediction, we can compute the complete decision boundaries for Example 2.3. We plot the decision boundaries for ùëò = 1 and ùëò = 3 in Figure 2.4.

ùëò =1
4
ùë¶ = red

ùëò =3
4
ùë¶ = red

2

2

ùë•2 ùë•2

0

ùë¶ = blue

Fig.

‚àí2

0

2

2.4

ùë•1

0 ‚àí2

ùë¶ = blue

0

2

ùë•1

In Figure Ô¨Åg:ex:knn2 the decision boundaries are the points in input space where the class prediction changes, that is, the borders between red and blue. This type of Ô¨Ågure gives an concise summary of a classiÔ¨Åer. However, it is only possible to draw such a plot in the simple case when the problem has a 2-dimensional input x. As we can see, the decision boundaries of ùëò-NN are not linear. In the terminology we will introduce later, ùëò-NN is thereby a nonlinear classiÔ¨Åer.

Choosing k
The number of neighbors ùëò that are considered when making a prediction with ùëò-NN is an important choice the user has to make. Since ùëò is not learned by ùëò-NN itself, but a design choice left to the user, we refer to it as a hyperparameter. Throughout the book we will use the term hyperparameter for similar tuning parameters also for other methods.
The choice of the hyperparameter ùëò has a big impact on the predictions made by ùëò-NN. To understand the impact of ùëò, we study how the decision boundary changes as ùëò changes in Figure 2.5, where ùëò-NN is applied to the music classiÔ¨Åcation Example 2.1 and the car stopping distance Example 2.2, both with ùëò = 1 as well as ùëò = 20.
With ùëò = 1 all training data points will, by construction, be correctly predicted and the model is adapted to the exact x and ùë¶ values of the training data. In the classiÔ¨Åcation problem there are for instance small green (Bob Dylan) regions within the red (the Beatles) area that are most likely misleading when it comes to accurately predict the artist of a new song. In order to make good predictions it would probably be better to instead predict red (the Beatles) for a new song in the entire middle-left region since the vast majority of training data points in that area are red. For the regression problem ùëò = 1 gives a quite shaky behavior, and also for this problem it is intuitively clear that this does not describe an actual eÔ¨Äect, but rather that the prediction is adapted to the noise in the data.
The drawbacks of using ùëò = 1 is not speciÔ¨Åc to these two examples. In most real world problems there is a certain amount of randomness in the data, or at least insuÔ¨Écient information which can be though of

24

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

ùëò =1

1

Beatles

Kiss

Bob Dylan

0.5

. A distance-based method k-NN

ùëò = 20

1

Beatles

Kiss

Bob Dylan

0.5

Energy (scale 0-1)

Energy (scale 0-1)

0

4.5

5

5.5

6

6.5

7

Length (ln s)

(a) Decision boundaries for the music classiÔ¨Åcation problem using ùëò = 1. This is a typical example of overÔ¨Åtting, meaning that the model has adapted too much to the training data so that it does not generalize well to new previously unseen data.
ùëò =1

150

ùëò-NN, ùëò = 1

Data

100

0

4.5

5

5.5

6

6.5

7

Length (ln s)

(b) The music classiÔ¨Åcation problem again, now using ùëò = 20. A higher value of ùëò gives a more smooth behavior
which, hopefully, predicts the artist of new songs more
accurately.

ùëò = 20

150

ùëò-NN, ùëò = 20

Data

100

Distance (feet)

Distance (feet)

50

50

0

10

20

30

40

Speed (mph)

0 10

20

30

40

Speed (mph)

(c) The black dots are the car stopping distance data, and the blue line shows the prediction for ùëò-NN with ùëò = 1 for any ùë•. As for the classiÔ¨Åcation problem above, ùëò-NN with ùëò = 1
overÔ¨Åts to the training data.

(d) The car stopping distance , this time with ùëò = 20. Except for the boundary eÔ¨Äect at the right, this seems like a much more useful model which captures the interesting eÔ¨Äects of the data and ignores the noise.

Figure 2.5: ùëò-NN applied to the music classiÔ¨Åcation Example 2.1 (a and b) and the car stopping distance Example 2.2 (c and d). For both problems ùëò-NN is applied with ùëò = 1 and well as ùëò = 20.

as a random eÔ¨Äect. In the music example the ùëõ = 230 songs were selected from all songs ever recorded from these artists, and since we do not know how this selection was made we may consider it as random. Furthermore, and more importantly, if we want our classiÔ¨Åer to generalize to completely new data, like new releases from the artists in our example (overlooking the obvious complication for now), then it is not reasonable to assume that the length and energy of a song will give a complete picture of the artistic styles. Hence, even with the best possible model, there is some ambiguity about which artist that has recorded a song if we only look at these two input variables. This ambiguity is modeled as random noise. Also for the car stopping distance there appears to be a certain amount of random eÔ¨Äects, not only in ùë• but also in ùë¶. By using ùëò = 1 and thereby adapting very closely to the training data, the predictions will depend not only on the interesting patterns in the problem, but also on the (more or less) random eÔ¨Äects that has shaped the training data. Typically we are not interested in capturing these eÔ¨Äects, and we refer to this as overÔ¨Åtting.
With the ùëò-NN classiÔ¨Åer we can mitigate overÔ¨Åtting by increasing the region of the neighborhood used to compute the prediction, that is, increasing the hyperparameter ùëò. With, for example, ùëò = 20 the predictions are no longer based only on the closest neighbor, but instead a majority vote among the 20 closest neighbors. As a consequence all training data points are no longer perfectly classiÔ¨Åed, but some of

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

25

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach
the songs end up in the wrong region in Figure 2.5b. The predictions are however less adapted to the peculiarities of the training data and thereby less overÔ¨Åtted, and Figure 2.5b as well as Figure 2.5d are indeed less ‚Äúnoisy‚Äù than Figure 2.5a and Figure 2.5c. However, if we take ùëò to be too large, then the averaging eÔ¨Äect with wash out all interesting patterns in the data as well. Indeed, for suÔ¨Éciently large ùëò the neighborhood will include all training data points and the model will reduce to predicting the mean of the data for any input.
Selecting ùëò is thus a trade-oÔ¨Ä between Ô¨Çexibility and rigidity. Since selecting ùëò either too big or too small will lead to a meaningless classiÔ¨Åers, there must exist a sweet spot for some moderate ùëò (possibly 20, but it could be less or more) where the classiÔ¨Åer generalizes the best. Unfortunately, there is no general answer to for which ùëò this happens, and this is diÔ¨Äerent for diÔ¨Äerent problems. In the music classiÔ¨Åcation it seems reasonable that ùëò = 20 will predict new test data points better than ùëò = 1, but there might very well be an even better choice of ùëò. For the car stopping problem the behavior is also more reasonable for ùëò = 20 than ùëò = 1, except for the boundary eÔ¨Äect for large ùë• where ùëò-NN is unable to capture the trend in the data as ùë• increases (simply because the 20 nearest neighbors are the same for all test points ùë•‚òÖ around and above 35). A systematic way of choosing a good value for ùëò is to use cross-validation, which we will discuss in Chapter 4.
Time to reÔ¨Çect 2.1: The prediction ùë¶(x‚òÖ) obtained using the ùëò-NN method is a piecewise constant function of the input x‚òÖ. For a classiÔ¨Åcation problem this is natural, since the output is categorical (see, for example, Figure 2.5 where the colored regions correspond to areas of the input space where the prediction is constant according to the color of that region). However, also for regression problems, ùëò-NN will have piecewise constant predictions. Why?

Input normalization

A Ô¨Ånal important practical aspect when using ùëò-NN is the importance of normalization of the input data.

Imagine a training dataset with ùëù = 2 input variables x = [ùë•1 ùë•2]T where all values of ùë•1 are in the range

[100, 1100] and the values for ùë•2 are in the much smaller range [0, 1]. It could for example be that ùë•1

and ùë•2 are measured in d‚àöiÔ∏ÅÔ¨Äerent units. The Euclidean distance between a test point x‚òÖ and a training data point xùëñ is xùëñ ‚àí x‚òÖ 2 = (ùë•ùëñ1 ‚àí ùë•‚òÖ1)2 + (ùë•ùëñ2 ‚àí ùë•‚òÖ2)2. This expression will typically be dominated by the

Ô¨Årst term (ùë•ùëñ1 ‚àí ùë•‚òÖ1)2, whereas the second term (ùë•ùëñ2 ‚àí ùë•‚òÖ2)2 tends to have a much smaller eÔ¨Äect, simply

due to the diÔ¨Äerent magnitude of ùë•1 and ùë•2. That is, the diÔ¨Äerent ranges leads to ùë•1 being considered much

more important than ùë•2 by ùëò-NN.

To avoid this undesired eÔ¨Äect we can re-scale the input variables. One option, in the mentioned example,

could be to subtract 100 from ùë•1

and thereafter divide it by 1000 and create ùë•ùëñn1ew

=

ùë•ùëñ1‚àí100 1000

such that ùë•1new

and ùë•2 both are in the range [0, 1]. More generally, this normalization procedure for the input data can be

written as

ùë•ùëñnùëóew

=

ùë•ùëñ ùëó ‚àí min‚Ñì (ùë•‚Ñì ùëó ) max‚Ñì (ùë•‚Ñì ùëó ) ‚àí min‚Ñì (ùë•‚Ñì

ùëó

)

,

for all ùëó = 1, . . . , ùëù, ùëñ = 1, . . . , ùëõ.

(2.1)

Another common way of normalizing (sometimes called standardizing) is by using the mean and standard deviation in the training data:

ùë•ùëñnùëóew

=

ùë•ùëñ ùëó ‚àí ùúéùëó

ùë•¬Ø ùëó

,

‚àÄ ùëó = 1, . . . , ùëù, ùëñ = 1, . . . , ùëõ,

(2.2)

where ùë•¬Ø ùëó and ùúéùëó are the mean and standard deviation for each input variable, respectively. It is crucial for ùëò-NN to apply some type of input normalization (that was indeed done in Figure 2.5),
but it is a good practice to apply this also when using other methods, if nothing else for numerical stability. It is however important to compute the scaling factors (min‚Ñì (ùë•‚Ñì ùëó), ùë•¬Ø ùëó, etc) using training data only and apply that scaling also to future test data points. Failing this, for example by performing normalization before setting test data aside (which we will discuss more in Chapter 4), might lead to wrong conclusions on how well the method will perform in predicting future (not yet seen) data points.

26

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. A rule-based method Decision trees
2.3 A rule-based method: Decision trees
The ùëò-NN method results in a prediction ùë¶(x‚òÖ) that is a piecewise constant function of the input x‚òÖ. That is, the method partitions the input space into disjoint regions and each region is associated with a certain (constant) prediction. For ùëò-NN, these regions are given implicitly by the ùëò-neighborhood of each possible test input. An alternative approach, that we will study in this section, is to come up with a set of rules that deÔ¨Ånes the regions explicitly. For instance, considering the music data in Example 2.1, a simple set of high-level rules for constructing a classiÔ¨Åer would be: inputs to the right in Figure 2.1 are classiÔ¨Åed as green (Bob Dylan), in the left as red (The Beatles), and in the upper part as blue (Kiss). We will now see how such rules can be learned systematically from the training data.
The rule-based models that we consider here are referred to as decision trees. The reason is that the rules used to deÔ¨Åne the model can be organized in a graph structure referred to as a binary tree. The decision tree eÔ¨Äectively divides the input space into multiple disjoint regions and in each region a constant value is used for the prediction ùë¶(x‚òÖ). We illustrate this with an example.
Example 2.5: Predicting colors with a decision tree
We consider a classiÔ¨Åcation problem with two numerical input variables x = [ùë•1 ùë•2]T and one categorical output ùë¶, the color Red or Blue. For now we do not consider any training data or how to actually learn the tree, but only how an already existing decision tree can be used to predict ùë¶(x‚òÖ).
The rules deÔ¨Åning the model are organized in the graph in Figure 2.6 which is referred to as a binary tree. To use this tree to predict a label for the test input x‚òÖ = [ùë•‚òÖ1 ùë•‚òÖ2]T we start at the top, referred to as the root node of the tree (in the metaphor the tree is growing upside down, with the root at the top and the leaves at the bottom). If the condition stated at the root is true, that is, if ùë•‚òÖ2 < 3.0, then we proceed down the left branch, otherwise along the right branch. If we reach a new internal node of the tree, we check the rule associated with that node and pick the left or the right branch accordingly. We continue and work our way down until we reach the end of a branch, called a leaf node. Each such Ô¨Ånal node corresponds to a constant prediction ùë¶ùëö, in this case one of the two classes Red or Blue.
5.0 ùë•2 < 3.0

ùëÖ1 ùë¶1 = Blue

ùë•1 < 5.0

ùëÖ2

ùëÖ3

Fig.

ùë¶2 = Blue

ùë¶3 = Red

2.6

A classiÔ¨Åcation tree. At each internal node a rule of

the form ùë• ùëó < ùë†ùëò indicates the left branch coming

from that split and the right branch then conse-

quently corresponds to ùë• ùëó ‚â• ùë†ùëò . This tree has two

internal nodes (including the root) and three leaf

nodes.

ùëÖ2

ùëÖ3

ùë•2

3.0 ùëÖ1

Fig.

2.7

ùë•1

A region partition, where each region corresponds

to a leaf node in the tree. Each border between

regions correspond to a split in the tree. Each

region is colored with the prediction corresponding

to that region, and the boundary between red and

blue is therefore the decision boundary.

The decision tree partitions the input space into axis-aligned ‚Äúboxes‚Äù, as shown in Figure 2.7. By increasing the depth of the tree (the number of steps from the root to the leaves), the partitioning can be made Ô¨Åner and Ô¨Åner and thereby describing more complicated functions of the input variable.
A pseudo code for predicting a test input with the tree in Figure 2.6 would look like

if x_2 < 3.0 then return Blue
else

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

27

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach

if x_1 < 5.0 then return Blue
else return Red
end end
As an example if we have x‚òÖ = [2.5 3.5]T, in the Ô¨Årst split we would take the right branch since ùë•‚òÖ2 = 3.5 ‚â• 3.0 and in the second split we would take the left branch since ùë•‚òÖ1 = 2.5 < 5.0. The prediction for this test point would be ùë¶(x‚òÖ) = Blue.
To set the terminology, the endpoint of each branch ùëÖ1, ùëÖ2 and ùëÖ3 in Example 2.5 are called leaf nodes and the internal splits, ùë•2 < 3.0 and ùë•1 < 5.0 are known as internal nodes. The lines that connect the nodes are referred to as branches. The tree is referred to as binary since each internal node splits into exactly two branches.
With more than two input variables it is diÔ¨Écult to illustrate the partitioning of the inputs space into regions (Figure 2.7), but the tree representation can still be used in the very same way. Each internal node corresponds to a rule where one of the ùëù input variables ùë• ùëó, ùëó = 1, . . . , ùëù, is compared with a threshold ùë†. If ùë• ùëó < ùë† we continue along the left branch and if ùë• ùëó ‚â• ùë† we continue along the right branch.
The constant predictions that we associate with the leaf nodes can be either categorical (as in Example 2.5 above) or numerical. Decision trees can thus be used to address both classiÔ¨Åcation and regression problems.
Example 2.5 illustrated how a decision tree can be used for making a prediction. We will now turn to the question of how a tree can be learned from training data.

Learning a regression tree

We will start by discussing how to learn (or, equivalently, train) a decision tree for a regression problem.
The classiÔ¨Åcation problem is conceptually similar and will be explained later. As mentioned above, the prediction ùë¶(x‚òÖ) from a regression tree is a piecewise constant function of the
input x‚òÖ. We can write this mathematically as,

‚àëùêøÔ∏Å ùë¶(x‚òÖ) = ùë¶‚Ñì I{x‚òÖ ‚àà ùëÖ‚Ñì },
‚Ñì=1

(2.3)

where ùêø is the total number of regions (leaf nodes) in the tree, ùëÖ‚Ñì is the ‚Ñìth region, and ùë¶‚Ñì is the constant

prediction for the ‚Ñìth region. Note that in the regression setting ùë¶‚Ñì is a numerical variable, and we will

consider it to be a real number for simplicity. In the equation above we have used the indicator function,

I{x ‚àà ùëÖ‚Ñì } = 1 if x ‚àà ùëÖ‚Ñì and I{x ‚àà ùëÖ‚Ñì } = 0 otherwise.

Learning the tree from data corresponds to Ô¨Ånding suitable values for the parameters deÔ¨Åning the

function (2.3), namely the regions ùëÖ‚Ñì and the constant predictions ùë¶‚Ñì, ‚Ñì = 1, . . . , ùêø, as well as the total

size then

of the tree ùêø. If we start by assuming that we can compute the constants {ùë¶‚Ñì }‚Ñìùêø=1 in

the shape a natural

of the tree, the way, simply as

partition (ùêø, {ùëÖ‚Ñì }‚Ñìùêø=1) is known, the average of the training data

points falling in each region:

ùë¶‚Ñì = Average{ùë¶ùëñ : xùëñ ‚àà ùëÖ‚Ñì }

It remains to Ô¨Ånd the shape of the tree, the regions ùëÖ‚Ñì, which requires a bit more work. The basic idea is of course to select the regions so that the tree Ô¨Åts the training data. This means that the output predictions from the tree should match the output values in the training data. Unfortunately, even when restricting ourselves to seemingly simple regions such as the ‚Äúboxes‚Äù obtained from a decision tree, Ô¨Ånding the tree (a collection of splitting rules) that optimally partitions the input space to Ô¨Åt the training data as well as possible turns out to be computationally infeasible. The problem is that there is a combinatorial explosion in the number of ways in which we can partition the input space. Searching through all possible binary trees is not possible in practice unless the tree size is so small that it is practically useless.

28

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. A rule-based method Decision trees

To handle this situation we use a heuristic algorithm known as recursive binary splitting for learning the tree. The word recursive means that we will determine the splitting rules one after the other, starting with the Ô¨Årst split at the root and then build the tree from top to bottom. The algorithm is greedy, in the sense that tree is constructed one split at a time, without having the complete tree ‚Äúin mind‚Äù. That is, when determining the splitting rule at the root node, the objective is to obtain a model that explains the training data as well as possible after a single split, without taking into consideration that additional splits may be added before arriving at the Ô¨Ånal model. When we have decided on the Ô¨Årst split of the input space (corresponding to the root node of the tree), this split is kept Ô¨Åxed and we continue in a similar way for the two resulting half-spaces (corresponding to the two branches of the tree), etc.
To see in detail how one step of this algorithm works, consider the setting when we are about to do our very Ô¨Årst split at the root of the tree. Hence, we want to select one of the ùëù input variables ùë•1, . . . , ùë• ùëù and a corresponding cutpoint ùë† which divides the input space into two half-spaces,

ùëÖ1( ùëó, ùë†) = {x | ùë• ùëó < ùë†}

and

ùëÖ2( ùëó, ùë†) = {x | ùë• ùëó ‚â• ùë†}.

(2.4)

Note that the regions depend on the index ùëó of the splitting variable as well as the value of the cutpoint ùë†, which is why we write them as functions of ùëó and ùë†. This is the case also for the predictions associated with the two regions,

ùë¶1( ùëó, ùë†) = Average{ùë¶ùëñ : xùëñ ‚àà ùëÖ1( ùëó, ùë†)}

and

ùë¶2( ùëó, ùë†) = Average{ùë¶ùëñ : xùëñ ‚àà ùëÖ2( ùëó, ùë†)}

since the averages in these expression range over diÔ¨Äerent data points depending on the regions.

For each training data point (xùëñ, ùë¶ùëñ) we can compute a prediction error by Ô¨Årst determining which region

the data point falls in, and then computing the diÔ¨Äerence between ùë¶ùëñ and the constant prediction associated

with that region. Doing this for all training data points the sum of squared errors can be written as

‚àëÔ∏Å

‚àëÔ∏Å

(ùë¶ùëñ ‚àí ùë¶1( ùëó, ùë†))2 +

(ùë¶ùëñ ‚àí ùë¶2( ùëó, ùë†))2 .

(2.5)

ùëñ:xùëñ ‚ààùëÖ1 ( ùëó,ùë†)

ùëñ:xùëñ ‚ààùëÖ2 ( ùëó,ùë†)

The square is added to ensure that the expression above is non-negative and that both positive and negative errors are counted equally. The squared error is a common loss function used for measuring the closeness of a prediction and the training data, but other loss functions can also be used. We will discuss the choice of loss function in more detail in later chapters.
To Ô¨Ånd the optimal split we select the values for ùëó and ùë† that minimize the squared error (2.5). This minimization problem can be solved easily by looping through all possible values for ùëó = 1, . . . , ùëù. For each ùëó we can scan through the Ô¨Ånite number of possible splits, and pick the pair ( ùëó, ùë†) for which the expression above is minimized. As pointed out above, when we have found the optimal split at the root node, this splitting rule is Ô¨Åxed. We then continue in the same way for the left and right branch independently. Each branch (corresponding to a half-space) is split again by minimizing the squared prediction error over all training data points following that branch.
In principle, we can continue in this way until there is only a single training data point in each of the regions, that is, until ùêø = ùëõ. Such a fully grown tree will result in predictions that exactly match the training data points, and the resulting model is quite similar to ùëò-NN with ùëò = 1. As pointed out above, this will typically result in a too erratic model that has overÔ¨Åtted to (possibly noisy) training data. To mitigate this issue, it is common to stop the growth of the tree at an earlier stage using some stopping criterion, for instance by deciding on ùêø beforehand, limiting the maximum depth (number of splits in any branch) or adding a constraint on the minimum number of training data points associated with each leaf node. Forcing the model to have more training data points in each leaf will result in an averaging eÔ¨Äect, similarly to increasing the value of ùëò in the ùëò-NN method. Using such a stopping criterion means that the value of ùêø is not set manually, but determined adaptively based on the result of the learning procedure.
A high-level summary of the method is given in Method 2.2. Note that the learning in Method 2.2 includes a recursive call, where we in each recursion grow one branch of the tree one step further.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

29

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach

Learn a decision tree using recursive binary splitting
Data: Training data T = {xùëñ, ùë¶ùëñ }ùëñùëõ=1 Result: Decision tree with regions ùëÖ1, . . . , ùëÖùêø and corresponding predictions ùë¶1, . . . , ùë¶ùêø 1 Let ùëÖ denote the whole input space 2 Compute the regions (ùëÖ1, . . . , ùëÖùêø) = Split(ùëÖ,T ) 3 Compute the predictions ùë¶‚Ñì for ‚Ñì = 1, . . . , ùêø as

ùë¶‚Ñì =

Average{ùë¶ùëñ : xùëñ ‚àà ùëÖ‚Ñì } MajorityVote{ùë¶ùëñ : xùëñ ‚àà ùëÖ‚Ñì }

(Regression problems) (ClassiÔ¨Åcation problems)

4 Function Split(ùëÖ,T ):

5 if stopping criterion fulÔ¨Ålled then

6

return ùëÖ

7 else

8

Go through all possible splits ùë• ùëó < ùë† for all input variables ùëó = 1, . . . , ùëù.

9

Pick the pair ( ùëó, ùë†) that minimizes (2.5)/(2.6) for regression/classiÔ¨Åcation problems.

10

Split region ùëÖ into ùëÖ1 and ùëÖ2 according to (2.4).

11

Split data T into T1 and T2 accordingly.

12

return Split(ùëÖ1,T1), Split(ùëÖ2,T2)

13 end

14 end

Predict from a decision tree Data: Decision tree with regions ùëÖ1, . . . , ùëÖùêø, training data T = {xùëñ, ùë¶ùëñ }ùëñùëõ=1, test data point x‚òÖ Result: Predicted test output ùë¶(x‚òÖ)
1 Find the region ùëÖ‚Ñì which x‚òÖ belongs to. 2 Return the prediction ùë¶(x‚òÖ) = ùë¶‚Ñì.

Method 2.2: Decision trees

ClassiÔ¨Åcation trees
Trees can also be used for classiÔ¨Åcation. We use the same procedure of recursive binary splitting but with two main diÔ¨Äerences. Firstly, we use a majority vote instead of an average to compute the prediction associated with each region,

ùë¶‚Ñì = MajorityVote{ùë¶ùëñ : xùëñ ‚àà ùëÖ‚Ñì }.

Secondly, when learning the tree we need a diÔ¨Äerent splitting criterion than the squared prediction error to take into account the fact that the output is categorical. To deÔ¨Åne these criteria, note Ô¨Årst that the split at any internal node is computed by solving an optimization problem of the form,

min
ùëó,ùë†

ùëõ1ùëÑ1

+

ùëõ2ùëÑ2

(2.6)

where ùëõ1 and ùëõ2 denote the number of training data points in the left and right nodes of the current split, respectively, and ùëÑ1 and ùëÑ2 are the costs (derived form the prediction errors) associated with these two nodes. The variables ùëó and ùë† denote the index of the splitting variable and the cutpoint as before. All of
the terms ùëõ1, ùëõ2, ùëÑ1, and ùëÑ2 depend on these variables, but we have dropped the explicit dependence from the notation for brevity. Comparing (2.6) with (2.5) we see that we recover the regression case if ùëÑ‚Ñì corresponds to the mean-squared error in node ‚Ñì.

30

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. A rule-based method Decision trees

To generalize this to the classiÔ¨Åcation case, we still solve the optimization problem (2.6) to compute the split, but choose ùëÑ‚Ñì in a diÔ¨Äerent way which respects the categorical nature of a classiÔ¨Åcation problem. To this end, we Ô¨Årst introduce

ùúã‚Ñìùëö

=

1 ùëõ‚Ñì

‚àëÔ∏Å
ùëñ:xùëñ ‚ààùëÖ‚Ñì

I{ùë¶ùëñ

=

ùëö}

to be the proportion of training observations in the ‚Ñìth region that belong to the ùëöth class. We can then deÔ¨Åne the splitting criterion, that is ùëÑ‚Ñì, based on these class proportions. One simple alternative is the misclassiÔ¨Åcation rate

ùëÑ‚Ñì

=

1

‚àí

max
ùëö

ùúã‚Ñìùëö,

(2.7a)

which is simply the proportion of data points in region ùëÖ‚Ñì which do not belong to the most common class. Other common splitting criteria are the Gini index

and the entropy criterion,

‚àëùëÄÔ∏Å ùëÑ‚Ñì = ùúã‚Ñìùëö(1 ‚àí ùúã‚Ñìùëö)
ùëö=1

(2.7b)

‚àëùëÄÔ∏Å ùëÑ‚Ñì = ‚àí ùúã‚Ñìùëö ln ùúã‚Ñìùëö.
ùëö=1

(2.7c)

In Example 2.6 we illustrate how to construct a classiÔ¨Åcation tree using recursive binary splitting and with the entropy as the slitting criterion.
Example 2.6: Learning a classiÔ¨Åcation tree (continuation of Example 2.5)

We consider the same setup as in Example 2.5, now with the following dataset

10

ùë•1 ùë•2

ùë¶

9.0 2.0 Blue

1.0 4.0 Blue

4.0 6.0 Blue

4.0 1.0 Blue

1.0 2.0 Blue

1.0 8.0 Red

6.0 4.0 Red

7.0 9.0 Red

9.0 8.0 Red

9.0 6.0 Red

8

6

ùë•2

4

2

Fig.

0 0

2

4

6

8

10

2.8

ùë•1

We want to learn a classiÔ¨Åcation tree by using the entropy criterion in (2.7c) and growing the tree until
there are no regions with more than Ô¨Åve data points left.
First split: There are inÔ¨Ånitely many possible splits we can make, but all splits which gives the same partition of the data points are equivalent. Hence, in practice we only have nine diÔ¨Äerent splits to consider in
this dataset. The data (dots) and these possible splits (dashed lines) are visualized in Figure 2.8. We consider all nine splits in turn. We start with the split at ùë•1 = 2.5, which splits the input space into the
two regions, ùëÖ1 = ùë•1 < 2.5 and ùëÖ2 = ùë•1 ‚â• 2.5. In region ùëÖ1 we have two blue data points and one red, in total ùëõ1 = 3 data points. The proportion of the two classes in region ùëÖ1 will therefore be ùúã1B = 2/3 and

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

31

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach

ùúã1R = 1/3. The entropy is calculated as

ùëÑ1

=

‚àíùúã1B

ln(ùúã1B)

‚àí

ùúã1R

ln(ùúã1R)

=

‚àí2 3

ln(

2) 3

‚àí

1 3

ln(

1) 3

=

0.64.

In region ùëÖ2 we have ùëõ2 = 7 data points with the proportions ùúã2B = 3/7 and ùúã2R = 4/7. The entropy for this regions will be

ùëÑ2

=

‚àíùúã2B

ln(ùúã2B)

‚àí

ùúã2R

ln(ùúã2R)

=

‚àí3 7

ln(

3 7

)

‚àí

4 7

ln(

4) 7

=

0.68

and inserted in (2.6) the total weighted entropy for this split becomes

ùëõ1ùëÑ1 + ùëõ2ùëÑ2 = 3 ¬∑ 0.64 + 7 ¬∑ 0.68 = 6.69.

We compute the cost for all other splits in the same manner, and summarize it in the table below.

Split (ùëÖ1) ùëõ1 ùúã1B ùúã1R ùëÑ1 ùëõ2 ùúã2B ùúã2R ùëÑ2 ùëõ1ùëÑ1 + ùëõ2ùëÑ2

ùë•1 < 2.5 3 2/3 1/3 0.64 7 3/7 4/7 0.68

6.69

ùë•1 < 5.0 5 4/5 1/5 0.50 5 1/5 4/5 0.50

5.00

ùë•1 < 6.5 6 4/6 2/6 0.64 4 1/4 3/4 0.56

6.07

ùë•1 < 8.0 7 4/7 3/7 0.68 3 1/3 2/3 0.64

6.69

ùë•2 < 1.5 1 1/1 0/1 0.00 9 4/9 5/9 0.69

6.18

ùë•2 < 3.0 3 3/3 0/3 0.00 7 2/7 5/7 0.60

4.18

ùë•2 < 5.0 5 4/5 1/5 0.50 5 1/5 4/5 0.06

5.00

ùë•2 < 7.0 7 5/7 2/7 0.60 3 0/3 3/3 0.00

4.18

ùë•2 < 8.5 9 5/9 4/9 0.69 1 0/1 1/1 0.00

6.18

From the table we can read that the two splits at ùë•2 < 3.0 and ùë•2 < 7.0 are both equally good. We choose to continue with ùë•2 < 3.0.

After Ô¨Årst split
10

After second split
10

8

8

6

6

ùëÖ2

ùëÖ3

ùë•2 ùë•2

4

4

2

ùëÖ1

2

ùëÖ1

Fig. 2.9

0

0

2

4

6

8

10

ùë•1

0

0

2

4

6

8

10

ùë•1

Second split: We note that only the upper region has more than Ô¨Åve data points. Also there is no point splitting region ùëÖ1 further since it only contains data points from the same class. In the next step we therefore split the upper region into two new regions, ùëÖ2 and ùëÖ3. All possible splits are displayed in Figure 2.9 to the
left (dashed lines) and we compute their cost in the same manner as before.

Splits (ùëÖ1) ùëõ2 ùúã2B ùúã2R ùëÑ2 ùëõ3 ùúã3B ùúã3R ùëÑ3 ùëõ2ùëÑ2 + ùëõ3ùëÑ3

ùë•1 < 2.5 2 1/2 1/2 0.69 5 1/5 4/5 0.50

3.89

ùë•1 < 5.0 3 2/3 1/3 0.63 4 0/4 4/4 0.00

1.91

ùë•1 < 6.5 4 2/4 2/4 0.69 3 0/3 3/3 0.00

2.77

ùë•1 < 8.0 5 2/5 3/5 0.67 2 0/2 2/2 0.00

3.37

ùë•2 < 5.0 2 1/2 1/2 0.69 5 1/5 4/5 0.50

3.88

ùë•2 < 7.0 4 2/4 2/4 0.69 3 0/3 3/3 0.00

2.77

ùë•2 < 8.5 6 2/6 4/6 0.64 1 0/1 1/1 0.00

3.82

32

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. A rule-based method Decision trees
The best split is the one at ùë•1 < 5.0 visualized above to the right. None of the three regions has more than Ô¨Åve data points. Therefore, we terminate the training. The Ô¨Ånal tree and its partitions were displayed in Example 2.5. If we want to use the tree for prediction, we predict blue if x‚òÖ ‚àà ùëÖ1 or x‚òÖ ‚àà ùëÖ2 since the blue training data points are in majority in each of these two regions. Similarly, we predict red if x‚òÖ ‚àà ùëÖ3.
0.5
MisclassiÔ¨Åcation rate Gini index Entropy
0.25

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

ùëü

Figure 2.10: Three splitting criteria for classiÔ¨Åcation trees as a function of the proportion of the Ô¨Årst class ùëü = ùúã‚Ñì1 in a certain region ùëÖ‚Ñì as given in (2.8). The entropy criterion has been scaled such that it passes through (0.5,0.5).

When choosing between the diÔ¨Äerent splitting criteria mentioned above, the misclassiÔ¨Åcation rate sounds like a reasonable choice since that is typically the criterion we want the Ô¨Ånal model to do well on6.
However, one drawback is that it does not favor pure nodes. With pure nodes we mean nodes where most
of the data points belong to a certain class. It is usually an advantage to favor pure nodes in the greedy
procedure that we use to grow the tree, since it can lead to a total of fewer splits. Both the entropy criterion
and the Gini index favors node purity more than misclassiÔ¨Åcation rate does.
This advantage can also be illustrated in Example 2.6. Consider the Ô¨Årst split in this example. If we would use the misclassiÔ¨Åcation rate as the splitting criterion, both the split ùë•2 < 5.0 as well as the split ùë•2 < 3.0 would provide a total misclassiÔ¨Åcation rate of 0.2. However, the split at ùë•2 < 3.0, which the entropy criterion favored, provides a pure node ùëÖ1. If we would go with the split ùë•2 < 5.0 the misclassiÔ¨Åcation after the second split would still be 0.2. If we would continue to grow the tree until no
data points are misclassiÔ¨Åed we would need three splits if we used the entropy criterion whereas we would need Ô¨Åve splits if we would use the misclassiÔ¨Åcation criterion and started with the split at ùë•2 < 5.0.
To generalize this discussion, consider a problem with two classes where we denote the proportion of the Ô¨Årst class as ùúã‚Ñì1 = ùëü and hence the proportion of the second class as ùúã‚Ñì2 = 1 ‚àí ùëü, the three criteria (2.7) can then in terms of ùëü be expressed as

MisclassiÔ¨Åcation rate: ùëÑ‚Ñì = 1 ‚àí max(ùëü, 1 ‚àí ùëü),

Gini index: ùëÑ‚Ñì = 2ùëü (1 ‚àí ùëü),

(2.8)

Entropy: ùëÑ‚Ñì = ‚àíùëü ln ùëü ‚àí (1 ‚àí ùëü) ln(1 ‚àí ùëü).

These functions are shown in Figure 2.10. All three citeria are similar in the sense that they provide zero loss if all data points belongs to either of the two classes, and maximum loss if the data points are equally divided between the two classes. However, the Gini index and entropy have a higher loss for all other proportions. In other words, the gain of having a pure node (ùëü close to 0 or 1) is higher for the Gini index and the entropy than for the misclassiÔ¨Åcation rate. As a consequence the Gini index and the entropy both tend to favor making one of the two nodes pure (or close to pure) since that provides a smaller total loss, which can make a good combination with the greedy nature of the recursive binary splitting.

How deep should a decision tree be?
The depth of a decision tree (the maximum distance between the root node and any leaf node) has a big impact on the Ô¨Ånal predictions. The tree depth impacts the predictions in a somewhat similar way as the
6This is not always true, for example for imbalanced and asymmetric classiÔ¨Åcation problems, see Section 4.5

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

33

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Supervised learning a Ô¨Årst approach

Fully grown tree

1

Beatles

Kiss

Bob Dylan

0.5

Tree with max depth 4

1

Beatles

Kiss

Bob Dylan

0.5

Energy (scale 0-1)

Energy (scale 0-1)

0

4.5

5

5.5

6

6.5

7

Length (ln s)

(a) Decision boundaries for the music classiÔ¨Åcation problem for a fully grown classiÔ¨Åcation tree with Gini index. This model overÔ¨Åts the data.

Fully grown tree

150

Data

Decision tree

100

0

4.5

5

5.5

6

6.5

7

Length (ln s)

(b) The same problem and data as in 2.11a for which a tree restricted to depth 4 has been learned, again using Gini index. This models will hopefully make better predictions for new data.
Tree with max depth 3

150

Data

Decision tree (max depth 3)

100

Distance (feet)

Distance (feet)

50

50

0

10

20

30

40

Speed (mph)

0 10

20

30

40

Speed (mph)

(c) The prediction for a fully grown regression tree. As for the (d) The same problem and data as in 2.11c for which a tree classiÔ¨Åcation problem above, this model overÔ¨Åts to the training restricted to depth 3 has been learned. data.

Figure 2.11: Decision trees applied to the music classiÔ¨Åcation Example 2.1 (a and b) and the car stopping distance Example 2.2 (c and d).

hyperparameter ùëò in ùëò-NN. We again use the music classiÔ¨Åcation and car stopping distance problems from Example 2.1 and 2.2 to study how the decision boundaries change depending on the depth of the trees. In Figure 2.11 the decision boundaries are illustrated for two diÔ¨Äerent trees. In Figure 2.11a and Figure 2.11c we have not restricted the depth of the tree, and grown it until each regions contains only data points with the same output value, a so-called fully grown tree. In Figure 2.11b and Figure 2.11d the maximum depth is restricted to a 4 and 3, respectively.
Similar to choosing ùëò = 1 in ùëò-NN, for a fully grown tree all training data points will by construction be correctly predicted since each region only contains data points with the same output. As a result, for the music classiÔ¨Åcation problem we get thin and small regions adapted to single training data points and for the car stopping distance problem we get very irregular line passing exactly through the observations. Even though these trees give excellent performance on the training data, they are not likely to be the best models for new yet unseen data. As we discussed previously in the context of ùëò-NN, we refer to this as overÔ¨Åtting.
In decision trees we can mitigate overÔ¨Åtting by using more shallow trees. Consequently, we get fewer and larger regions with an increased averaging eÔ¨Äect, resulting in decision boundaries that are less adapted to the noise in the training data. This is illustrated in Figure 2.11b and Figure 2.11d for the two example problems. As for ùëò in ùëò-NN, the optimal size of the tree depends on many properties of the problem and it

34

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Further reading
is a trade-oÔ¨Ä between Ô¨Çexibility and rigidity. Similar trade-oÔ¨Äs have to be made for almost all methods presented in this book and will be discussed systematically in Chapter 4.
How can the user control the growth of the tree? Here we have diÔ¨Äerent strategies. The most straightforward strategy is to adjust the stopping criterion, that is, the condition that should be fulÔ¨Ålled for not proceeding with further splits in a certain node. As mentioned earlier, this criterion could be that we do not attempt further splits if there are less than a certain number of training data points in the corresponding region, or as in Figure 2.11, stop splitting when we reach a certain depth. Another strategy of controlling the depth is to use pruning. In pruning we start with a fully grown tree and then in a second post-processing step prune it back to a smaller one. We will, however, not discuss pruning further here.
2.4 Further reading
The reason why we start this book by ùëò-NN is that it is perhaps the most intuitive and straightforward way to solve a classiÔ¨Åcation problem. The ideas is, at least, a thousand years old and was described already by H. assan Ibn al-Haytham (latinized as Alhazen) around year 1030 in Kita¬Øb al-Mana¬Øz.ir (book of optics) (Pelillo 2014), as an explanation of how the human brain perceive objects. As with many good ideas, the nearest neighbor idea has been re-invented many times, and a more modern description of ùëò-NN is found in Cover and Hart (1967).
Also the basic idea of decision trees is relatively simple, but there are many possible ways to improve and extend them, as well as diÔ¨Äerent options in how to implement them in detail. A somewhat longer introduction to decision trees is found in Hastie et al. (2009), and an historically oriented overview is found in Loh (2014). Of particular signiÔ¨Åcance is perhaps CART (ClassiÔ¨Åcation and Regression Trees, Breiman et al. 1984) as well as ID3 and C4.5 (Quinlan 1986, Quinlan 1993).

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

35

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

3 Basic parametric models and a statistical perspective on learning

In the previous chapter we introduced the supervised machine learning problem, as well as two methods for solving it. In this chapter we will consider a generic approach to learning referred to as parametric modeling. In particular, we will introduce linear regression and logistic regression which are two such parametric models. The key point of a parametric model is that it contains some parameters ùúΩ, which are learned from training data. However, once the parameters are learned, the training data may be discarded, since the prediction only depends on ùúΩ.

3.1 Linear regression

Regression is one of the two fundamental tasks of supervised learning (the other one is classiÔ¨Åcation). We will now introduce the linear regression model, which might (at least historically) be the most popular method for solving regression problems. Despite its relative simplicity, it is a surprisingly useful and is an important stepping stone for more advanced methods, such as deep learning, see Chapter 6.
As discussed in the previous chapter, regression amounts to learning the relationships between some input variables x = [ùë•1 ùë•2 . . . ùë• ùëù]T and a numerical output variable ùë¶. The inputs can be either categorical or numerical, but we will start by assuming that all ùëù inputs are numerical as well, and discuss categorical inputs later. In a more mathematical framework, regression is about learning a model ùëì

ùë¶ = ùëì (x) + ùúÄ,

(3.1)

mapping the input to the output, where ùúÄ is an error term that describes everything about the input‚Äìoutput relationship that cannot be captured by the model. With a statistical perspective, we consider ùúÄ as a random variable, referred to as a noise, that is independent of x and has mean value of zero. As a running example of regression, we will use the car stopping distance regression problem introduced in the previous chapter as Example 2.2.

The linear regression model
The linear regression model assumes that the output variable ùë¶ (a scalar) can be described as an aÔ¨Éne1 combination of the ùëù input variables ùë•1, ùë•2, . . . , ùë• ùëù plus a noise term ùúÄ,

ùë¶ = ùúÉ0 + ùúÉ1ùë•1 + ùúÉ2ùë•2 + ¬∑ ¬∑ ¬∑ + ùúÉùëùùë•ùëù + ùúÄ.

(3.2)

We refer to the coeÔ¨Écients ùúÉ0, ùúÉ1, . . . ùúÉ ùëù as the parameters of the model, and we sometimes refer to ùúÉ0 speciÔ¨Åcally as the intercept (or oÔ¨Äset) term. The noise term ùúÄ accounts for random errors in the data
not captured by the model. The noise is assumed to have mean zero and to be independent of x. The zero-mean assumption is nonrestrictive, since any (constant) non-zero mean can be incorporated in the oÔ¨Äset term ùúÉ0.
To have a more compact notation, we introduce the parameter vector ùúΩ = [ùúÉ0 ùúÉ1 . . . ùúÉ ùëù]T and extend the vector x with a constant one in its Ô¨Årst position, such that we can write the linear regression model (3.2)

1An aÔ¨Éne function is a linear function plus a constant oÔ¨Äset.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

37

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning

compactly as

ùë¶ = ùúÉ0 + ùúÉ1ùë•1 + ùúÉ2ùë•2 + ¬∑ ¬∑ ¬∑ + ùúÉùëùùë•ùëù + ùúÄ = ùúÉ0

ùúÉ1

...

ùúÉ ùëù Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùë•ùë•1...ùëù1 Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª + ùúÄ = ùúΩTx + ùúÄ.

(3.3)

This notation means that the symbol x is used both for the ùëù + 1 and the ùëù dimensional version of the input vector, with or without the constant one in the leading position, respectively. This is only a matter of book-keeping for handling the intercept term ùúÉ0. Which deÔ¨Ånition that is used will be clear from context and carries no deeper meaning.
The linear regression model is a parametric function of the form (3.3). The parameters ùúΩ can take arbitrary values, and the actual values that we assign to them will control the input‚Äìoutput relationship described by the model. Learning of the model therefore amounts to Ô¨Ånding suitable values for ùúΩ based on observed training data. Before discussing how to do this, however, let us Ô¨Årst look at how to use the model for predictions once it has been learned.
The goal in supervised machine learning is making predictions ùë¶(x‚òÖ) for new, previously unseen, test inputs x‚òÖ = [1 ùë•‚òÖ1 ùë•‚òÖ2 ¬∑ ¬∑ ¬∑ ùë•‚òÖùëù]T. Let us assume that we have already learned some parameter values ùúΩ for the linear regression model (how this is done will be described next). We use the symbol to indicate that ùúΩ contains learned values of the unknown parameter vector ùúΩ. Since we assume that the noise term ùúÄ is random with zero mean and independent of all observed variables, it makes sense to replace ùúÄ with 0 in the prediction. That is, a prediction from the linear regression model takes the form

ùë¶(x‚òÖ) = ùúÉ0 + ùúÉ1ùë•‚òÖ1 + ùúÉ2ùë•‚òÖ2 + ¬∑ ¬∑ ¬∑ + ùúÉ ùëùùë•‚òÖ ùëù = ùúΩTx‚òÖ.

(3.4)

The noise term ùúÄ is often referred to as an irreducible error or an aleatoric2 uncertainty in the prediction. We illustrate the predictions made by a linear regression model in Figure 3.1.

prediction

output ùë¶

Linear regression model

ùë¶ (x‚òÖ)

Data

Prediction

ùë¶3

ùúÄ3

ùë¶2

ùúÄ2

ùë¶1

ùúÄ1

data

x1

x2

x3

x‚òÖ

data
input x

test input

Figure 3.1: Linear regression with ùëù = 1: The black dots represent ùëõ = 3 data points, from which a linear regression

model (blue line) is learned. The model does not Ô¨Åt the data perfectly, so there is a remaining error corresponding to

the noise ùúÄ (red). The model can be used to predict (blue circle) the output ùë¶(x‚òÖ) for a test input x‚òÖ.

Learning linear regression from training data Let us now discuss how to learn a linear regression model, that is learn ùúΩ, from training data T = {xùëñ, ùë¶ùëñ }ùëñùëõ=1. We collect the training data, which consists of ùëõ data point with inputs xùëñ and outputs ùë¶ùëñ, in the ùëõ √ó ( ùëù + 1)
2From the Latin word aleator, meaning dice-player.

38

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Linear regression

matrix X and ùëõ-dimensional vector y,

X = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞

xT1 x...T2 xTùëõ

Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª , y = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùë¶ùë¶ùë¶...ùëõ12Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª , where each xùëñ = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùë•ùë•ùë•1ùëñ...ùëñùëñùëù12 Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª .

(3.5)

Example 3.1: Car stopping distances

We continue Example 2.2 and learn a linear regression model for the car stopping distance data. We start by forming the matrices X and y. Since we only have one input and one output, both ùë•ùëñ and ùë¶ùëñ are scalar. We get

X = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞1111111...

334455599..........0901267Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª ,

ùúΩ=

ùúÉ0 ùúÉ1

,

and

y = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞11488423140........00000..00Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª .

(3.6)

Altogether we can use this vector and matrix notation to describe the linear regression model for all training data points xùëñ, ùëñ = 1, . . . , ùëõ, in one equation as a matrix multiplication

y = XùúΩ + ùùê,

(3.7)

where ùùê is a vector of errors/noise terms. Moreover we can also deÔ¨Åne a vector of predicted outputs for the training data y = ùë¶(x1) ùë¶(x2) . . . ùë¶(xùëõ) T which also allows a compact matrix formulation

y = XùúΩ.

(3.8)

Note that whereas y is a vector of recorded training data values, y is a vector whose entries are functions of ùúΩ. Learning the unknown parameters ùúΩ amounts to Ô¨Ånding values such that y is similar to y. That is, the predictions given by the model should Ô¨Åt the training data well. There are multiple ways to deÔ¨Åne what ‚Äòsimilar‚Äô or ‚Äòwell‚Äô actually means, but it somehow amounts to Ô¨Ånd ùúΩ such that y ‚àí y = ùùê is small. We will approach this by formulating a loss function, which gives a mathematical meaning to ‚ÄòÔ¨Åtting the data
well‚Äô. We will thereafter interpret the loss function from a statistical perspective, by understanding this as selecting the value of ùúΩ which makes the observed training data y as likely as possible with respect to the model‚Äîthe so-called maximum likelihood solution. Later, in Chapter 9, we will also introduce a conceptually diÔ¨Äerent way to learn ùúΩ.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

39

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning
Model Data ùúÄ

output ùë¶

input ùë•
Figure 3.2: A graphical explanation of the squared error loss function: the goal is to choose the model (blue line) such that the sum of the squares (light red) of each error ùúÄ is minimized. That is, the blue line is to be chosen so that the amount of red color is minimized. This motivates the name least squares. The black dots, the training data, are Ô¨Åxed.

Loss functions and cost functions
A principled way to deÔ¨Åne the learning problem is to introduce a loss function ùêø (ùë¶, ùë¶) which measures how close the model‚Äôs prediction ùë¶ is to the observed data ùë¶. If the model Ô¨Åts the data well, so that ùë¶ ‚âà ùë¶, then the loss function should take a small value, and vice versa. Based on the chosen loss function we also deÔ¨Åne the cost function as the average loss over the training data. Learning a model then amounts to Ô¨Ånding the parameter values that minimize the cost

loss function

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

ùêø (ùë¶(xùëñ;

ùúΩ),

ùë¶ùëñ)

.

cost function ùêΩ (ùúΩ)

(3.9)

Note that each term in the expression above corresponds to evaluating the loss function for the prediction ùë¶(xùëñ; ùúΩ), given by (3.4), for the training point with index ùëñ and the true output value ùë¶ùëñ at that point. To emphasize that the prediction depends on the parameters ùúΩ, we have included ùúΩ as an argument to ùë¶ for clarity. The operator arg minùúΩ means ‚Äúthe value of ùúΩ for which the cost function attains it minimum‚Äù. The
relationship between loss and cost functions (3.9) is general for all cost functions in this book.

Least squares and the normal equations

For regression, a commonly used loss function is the squared error loss

ùêø (ùë¶(x; ùúΩ), ùë¶) = ùë¶(x; ùúΩ) ‚àí ùë¶ 2.

(3.10)

This loss function is 0 if ùë¶(x; ùúΩ) = ùë¶, and grows fast (quadratic) as the diÔ¨Äerence between ùë¶ and the prediction ùë¶(x; ùúΩ) = ùúΩTx increases. The corresponding cost function for the linear regression model (3.7)
can be written with matrix notation as

ùêΩ(ùúΩ)

=

1 ùëõ

‚àëùëõÔ∏Å (ùë¶(xùëñ; ùúΩ)
ùëñ=1

‚àí

ùë¶ùëñ)2

=

1 ùëõ

y‚àíy

2 2

=

1 ùëõ

XùúΩ ‚àí y

2 2

=

1 ùëõ

ùùê

22,

(3.11)

where

¬∑ 2 denotes the usual Euclidean vector norm, and

¬∑

2 2

its

square.

Due

to

the

square,

this

particular

cost function is also commonly referred to as the least squares cost. It is illustrated in Figure 3.2. We will

discuss other loss functions in Chapter 5.

40

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Linear regression

When using the squared error loss for learning a linear regression model from T , we thus need to solve the problem

ùúΩ

=

arg min
ùúΩ

1 ùëõ

‚àëùëõÔ∏Å ( ùúΩ T xùëñ
ùëñ=1

‚àí

ùë¶ùëñ)2

=

arg min
ùúΩ

1 ùëõ

XùúΩ ‚àí y

22 .

(3.12)

From a linear algebra point of view, this can be seen as the problem of Ô¨Ånding the closest vector to y (in an Euclidean sense) in the subspace of Rùëõ spanned by the columns of X. The solution to this problem is the orthogonal projection of y onto this subspace, and the corresponding ùúΩ can be shown (see Section 3.A) to fulÔ¨Åll

XTXùúΩ = XTy.

(3.13)

Equation (3.13) is often referred to as the normal equations, and gives the solution to the least squares problem (3.12). If XTX is invertible, which is often the case, then ùúΩ has the closed form expression

ùúΩ = (XTX)‚àí1XTy.

(3.14)

The fact that this closed-form solution exists is important, and is probably the reason for why the linear regression with squared error loss is so extremely common in practice. Other loss functions lead to optimization problems that often lack closed-form solutions.
We now have everything in place for using linear regression, and we summarize it as Method 3.1 and illustrate by Example 3.2.

Time to reÔ¨Çect 3.1: What does it mean in practice if XTX is not invertible?

Time to reÔ¨Çect 3.2: If the columns of X are linearly independent and ùëù = ùëõ ‚àí 1, X spans the entire Rùëõ. If that is the case, X is invertible and (3.14) reduces to ùúΩ = X‚àí1y. Hence, a unique solution exists such that y = XùúΩ exactly, that is, the model Ô¨Åts the training data perfectly. Why would that
not be a desired property in practice?

Learn linear regression with squared error loss Data: Training data T = {xùëñ, ùë¶ùëñ }ùëñùëõ=1 Result: Learned parameter vector ùúΩ
1 Construct the matrix X and vector y according to (3.5). 2 Compute ùúΩ by solving (3.13).
Predict with linear regression Data: Learned parameter vector ùúΩ and test input x‚òÖ Result: Prediction ùë¶(x‚òÖ)
1 Compute ùë¶(x‚òÖ) = ùúΩTx‚òÖ.
Method 3.1: Linear regression

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

41

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning

Example 3.2: Car stopping distances

By inserting the matrices (3.6) from Example 3.1 into the normal equations (3.7), we obtain ùúÉ0 = ‚àí20.1 and ùúÉ1 = 3.14. Plotting the resulting model gives us Figure 3.3.

150

Linear regression model

Data

100

Distance (feet)

50

Fig.

0

0

10

20

30

40

3.3

Speed (mph)

This can be compared to how ùëò-NN and decision trees solves the same problem, Figure 2.5 and 2.11. Clearly

the linear regression model behaves diÔ¨Äerently than these models; linear regression does not share the ‚Äúlocal‚Äù

nature of ùëò-NN and decision trees (only training data points close to x‚òÖ aÔ¨Äects ùë¶(x‚òÖ)), which is related to

the fact that linear regression is a parametric model.

The maximum likelihood perspective

To get another perspective of the squared error loss we will now reinterpret the least squares method above as a maximum likelihood solution. The word ‚Äòlikelihood‚Äô refers to the statistical concept of the likelihood function, and maximizing the likelihood function amounts to Ô¨Ånding the value of ùúΩ that makes observing y as likely as possible. That is, instead of (somewhat arbitrarily) selecting a loss function, we start with the problem

ùúΩ = arg max ùëù(y | X; ùúΩ).
ùúΩ

(3.15)

Here ùëù(y | X; ùúΩ) is the probability density of all observed outputs y in the training data, given all inputs X and parameters ùúΩ. This determines mathematically what ‚Äòlikely‚Äô means, but we need to specify it in more detail. We do that by considering the noise term ùúÄ as a stochastic variable with a certain distribution.

A common assumption is that the noise terms are independent, each with a Gaussian distribution (also known as a normal distribution) with mean zero and variance ùúéùúÄ2,

ùúÄ ‚àº N 0, ùúéùúÄ2 .

(3.16)

This implies that the ùëõ observed training data points are independent, and ùëù(y | X; ùúΩ) factorizes as

ùëõ
ùëù(y | X; ùúΩ) = ùëù(ùë¶ùëñ | xùëñ, ùúΩ).
ùëñ=1

(3.17)

Considering the linear regression model from (3.3), ùë¶ = ùúΩTx + ùúÄ, together with the Gaussian noise assumption (3.16) we have

ùëù(ùë¶ùëñ | xùëñ, ùúΩ) = N ùë¶ùëñ; ùúΩTxùëñ, ùúéùúÄ2

= ‚àöÔ∏Å 1 exp 2ùúãùúéùúÄ2

‚àí

1 2ùúéùúÄ2

ùúΩTxùëñ ‚àí ùë¶ùëñ 2

.

(3.18)

Recall that we want to maximize the likelihood with respect to ùúΩ. For numerical reasons, it is usually better to work with the logarithm of ùëù(y | X; ùúΩ),

‚àëùëõÔ∏Å ln ùëù(y | X; ùúΩ) = ln ùëù(ùë¶ùëñ | xùëñ, ùúΩ).
ùëñ=1

(3.19)

42

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Linear regression

Since the logarithm is a monotonically increasing function, maximizing the log-likelihood (3.19) is equivalent to maximizing the likelihood itself. Putting (3.18) and (3.19) together, we get

ln

ùëù(y

|

X;

ùúΩ)

=

‚àí

ùëõ 2

ln (2ùúã ùúéùúÄ2 )

‚àí

1 2ùúéùúÄ2

‚àëùëõÔ∏Å
ùëñ=1

ùúΩTxùëñ ‚àí ùë¶ùëñ

2
.

(3.20)

Removing terms and factors independent of ùúΩ does not change the maximizing argument, and we see that we can rewrite (3.15) as

‚àëùëõÔ∏Å

ùúΩ = arg max ùëù(y | X; ùúΩ) = arg max ‚àí

ùúΩ

ùúΩ

ùëñ=1

ùúΩTxùëñ ‚àí ùë¶ùëñ

2

=

arg min
ùúΩ

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

ùúΩTxùëñ ‚àí ùë¶ùëñ

2
.

(3.21)

This is indeed linear regression with the least squares cost (the cost function implied by the squared error loss function (3.10)). Hence, using the squared error loss is equivalent to assuming a Gaussian noise distribution in the maximum likelihood formulation. Other assumptions on ùúÄ leads to other loss functions, as we will discuss more in Chapter 5.

Categorical input variables
The regression problem is characterized by a numerical output ùë¶, and inputs x of arbitrary type. We have, however, only discussed the case of numerical inputs so far. To see how we can handle categorical inputs in the linear regression model, assume that we have an input variable that only takes two diÔ¨Äerent values. We refer to those two values as A and B. We can then create a dummy variable ùë• as

ùë• = 0 if A, 1 if B,

(3.22)

and use this variable in any supervised machine learning method as if it was numerical. For linear regression, this eÔ¨Äectively gives us a model which looks like

ùë¶ = ùúÉ0 + ùúÉ1ùë• + ùúÄ =

ùúÉ0 + ùúÄ ùúÉ0 + ùúÉ1 + ùúÄ

if A, if B.

(3.23)

The model is thus able to learn and predict two diÔ¨Äerent values depending on whether the input is A or B. If the categorical variable takes more than two values, let us say A, B, C and D, we can make a so-called
one-hot encoding by constructing a four-dimensional vector

x = ùë•ùê¥ ùë•ùêµ ùë•ùê∂ ùë•ùê∑ T

(3.24)

where ùë•ùê¥ = 1 if A, ùë•ùêµ = 1 if B, and so on. That is, only one element of x will be 1, the rest are 0. Again, this construction can be used for any supervised machine learning method, and is not restricted to linear regression.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

43

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning

3.2 ClassiÔ¨Åcation and logistic regression
After presenting a parametric method for solving the regression problem, we now turn our attention to classiÔ¨Åcation. As we will see, with a modiÔ¨Åcation of the linear regression model we can apply it to the classiÔ¨Åcation problem as well, however at the cost of not being able to use the convenient normal equations. Instead we have to resort to numerical optimization for learning the parameters of the model.

A statistical view of the classiÔ¨Åcation problem
Supervised machine learning amounts to predicting the output from the input. With a statistical perspective, classiÔ¨Åcation amounts to predicting the conditional class probabilities

ùëù(ùë¶ = ùëö | x),

(3.25)

where ùë¶ is the output (1, 2, . . . , or ùëÄ) and x is the input.3 In words, ùëù(ùë¶ = ùëö | x) describes the probability for class ùëö given that we know the input x. Talking about ùëù(ùë¶ | x) implies that we think about the class label ùë¶ as a random variable. Why? Because we choose to model the real world, from where the data originates, as involving a certain amount of randomness (much like the random error ùúÄ in regression). Let
us illustrate with an example.

Example 3.3: Describing voting behavior using probabilities
We want to construct a model that can predict voting preferences (= ùë¶, the categorical output) for diÔ¨Äerent population groups (= x, the input). However, we then have to face the fact that not everyone in a certain population group will vote for the same political party. We can therefore think of ùë¶ as a random variable which follows a certain probability distribution. If we know that the vote count in the group of 45 year old women (= x) is 13% for the cerise party, 39% for the turquoise party and 48% for the purple party (here we have ùëÄ = 3), we could describe it as
ùëù(ùë¶ = cerise party | x = 45 year old women) = 0.13, ùëù(ùë¶ = turqoise party | x = 45 year old women) = 0.39,
ùëù(ùë¶ = purple party | x = 45 year old women) = 0.48.
In this way, the probabilities ùëù(ùë¶ | x) describe the non-trivial fact that
(a) all 45 year old women do not vote for the same party, but
(b) the choice of party does not appear to be completely random among 45 year old women either; the purple party is the most popular, and the cerise party is the least popular.
Thus, it can be useful to have a classiÔ¨Åer which predicts not only a class ùë¶ (one party), but a distribution over classes ùëù(ùë¶ | x).

We now aim to construct a classiÔ¨Åer which can not only predict classes, but also learn the class probabilities ùëù(ùë¶ | x). More speciÔ¨Åcally, for binary classiÔ¨Åcation problems (ùëÄ = 2, and ùë¶ is either 1 or ‚àí1), we learn a model ùëî(x) for which

ùëù(ùë¶ = 1 | x) is modeled by ùëî(x).

(3.26a)

By the laws of probabilities, it holds that ùëù(ùë¶ = 1 | x) + ùëù(ùë¶ = ‚àí1 | x) = 1, which means that

ùëù(ùë¶ = ‚àí1 | x) is modeled by 1 ‚àí ùëî(x).

(3.26b)

Since ùëî(x) is a model for a probability, it is natural to require that 0 ‚â§ ùëî(x) ‚â§ 1 for any x. We will see how this constraint can be enforced below.
3We use the notation ùëù(ùë¶ | x) to denote probability masses (ùë¶ discrete) as well as probability densities (ùë¶ continuous).

44

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. ClassiÔ¨Åcation and logistic regression

‚Ñé(ùëß)

1 0.8 0.6 0.4 0.2
0 ‚àí10

‚àí8

‚àí6

‚àí4

‚àí2

0

2

4

6

ùëß

Figure

3.4:

The

logistic

function

‚Ñé(ùëß)

=

ùëíùëß 1+ùëíùëß

.

8

10

For the multiclass problem, we instead let the classiÔ¨Åer return a vector-valued function g(x), where

Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞

ùëù(ùë¶ ùëù(ùë¶
ùëù(ùë¶

===...ùëÄ12 |||xxx)))Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª

is modeled by

Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùëîùëîùëîùëÄ12 ((...(xxx))) Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª

= g(x).

(3.27)

In words, each element ùëîùëö(x) of g(x) corresponds to the conditional class probability ùëù(ùë¶ = ùëö | x).

Since g(x) models a probability vector, we require that each element ùëîùëö(x) ‚â• 0 and that g(x) 1 =

ùëÄ ùëö=1

|ùëîùëö (x) |

=

1

for

any

x.

The logistic regression model for binary classiÔ¨Åcation

We will now introduce the logistic regression model, which is one possible way of modeling conditional class probabilities. Logistic regression can be viewed as a modiÔ¨Åcation of the linear regression model so that it Ô¨Åts the classiÔ¨Åcation (instead of the regression) problem.
Let us start with binary classiÔ¨Åcation. We wish to learn a function ùëî(x) that approximates the conditional probability of the positive class, see (3.26). To this end, we start with the linear regression model which, without the noise term, is given by

ùëß = ùúÉ0 + ùúÉ1ùë•1 + ùúÉ2ùë•2 + ¬∑ ¬∑ ¬∑ + ùúÉ ùëùùë•ùëù = ùúΩTx.

(3.28)

This is a mapping which takes x and returns ùëß, which in this context is called the logit. Note that ùëß takes

values on the entire real line, whereas we need a function which instead returns a value in the interval

[0, 1]. The key idea of logistic regression is to ‚Äòsqueeze‚Äô ùëß from (3.28) to the interval [0, 1] by using the

logistic

function

‚Ñé(ùëß)

=

ùëíùëß 1+ùëí

ùëß

,

see

Figure

3.4.

This

results

in

ùëî (x)

=

ùëíùúΩTx , 1 + ùëíùúΩTx

(3.29a)

which is restricted to [0, 1] and hence can be interpreted as a probability. The function (3.29a) is the logistic regression model for ùëù(ùë¶ = 1 | x). Note that this implicitly also gives a model for ùëù(ùë¶ = ‚àí1 | x),

1 ‚àí ùëî(x)

=1‚àí

ùëíùúΩTx 1 + ùëíùúΩTx

=

1 1 + ùëíùúΩTx

=

ùëí‚àíùúΩTx .
1 + ùëí‚àíùúΩTx

(3.29b)

In a nutshell, the logistic regression model is linear regression appended with the logistic function. This is the reason for the (somewhat confusing) name, but despite the name logistic regression is a method for classiÔ¨Åcation, not regression! The reason for why there is no noise term ùúÄ in (3.28), as we had in the linear regression model (3.3), is that the randomness in classiÔ¨Åcation is statistically modeled by the class probability construction ùëù(ùë¶ = ùëö | x) instead of an additive noise ùúÄ.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

45

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning

As for linear regression, we have a model (3.29) which contains unknown parameters ùúΩ. Logistic regression is thereby also a parametric model, and we need to learn the parameters from training data. How this can be done is the topic for the next section.

Learning the logistic regression model by maximum likelihood

By using the logistic function, we have transformed linear regression (a model for regression problems) into logistic regression (a model for classiÔ¨Åcation problems). The price to pay is that we will not be able to use the convenient normal equations for learning ùúΩ (as we could for linear regression if we used the squared error loss), due to the nonlinearity of the logistic function.
In order to derive a principled way of learning ùúΩ in (3.29) from training data T = {xùëñ, ùë¶ùëñ }ùëñùëõ=1, we start with the maximum likelihood approach. From a maximum likelihood perspective, learning a classiÔ¨Åer amounts to solving

‚àëùëõÔ∏Å

ùúΩ = arg max ùëù(y | X; ùúΩ) = arg max ln ùëù(ùë¶ùëñ | xùëñ; ùúΩ),

ùúΩ

ùúΩ ùëñ=1

(3.30)

where we, similarly to linear regression (3.19), assume that the training data points are independent and we consider the logarithm of the likelihood function for numerical reasons. We have also added ùúΩ explicitly
to the notation to emphasize the dependence on the model parameters. Remember that our model of ùëù(ùë¶ = 1 | x; ùúΩ) is ùëî(x; ùúΩ), which gives

ln ùëù(ùë¶ùëñ | xùëñ; ùúΩ) =

ln ùëî(xùëñ; ùúΩ) ln (1 ‚àí ùëî(xùëñ; ùúΩ))

if ùë¶ùëñ = 1, if ùë¶ùëñ = ‚àí1.

(3.31)

It is common to turn the maximization problem (3.30) into an equivalent minimization problem by using

the

negative

log-likelihood

as

cost

function,

ùêΩ(ùúΩ)

=

‚àí

1 ùëõ

ln ùëù(ùë¶ùëñ | xùëñ; ùúΩ), that is

ùêΩ(ùúΩ)

=

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

‚àí ln ùëî(xùëñ; ùúΩ) ‚àí ln (1 ‚àí ùëî(xùëñ; ùúΩ))

if ùë¶ùëñ = 1, if ùë¶ùëñ = ‚àí1.

(3.32)

Binary cross-entropy loss ùêø (ùëî (xùëñ;ùúΩ) ,ùë¶ùëñ)

The loss function in the expression above is called the cross-entropy loss. It is not speciÔ¨Åc to logistic regression, but can be used for any binary classiÔ¨Åer that predicts class probabilities ùëî(x; ùúΩ).
However, we will now consider speciÔ¨Åcally the logistic regression model, for which we can write out the cost function (3.32) in more detail. In doing so, the particular choice of labeling {‚àí1, 1} turns out to be convenient. For ùë¶ùëñ = 1 we can write

and for ùë¶ùëñ = ‚àí1

ùëî(xùëñ; ùúΩ) =

ùëíùúΩ T xùëñ 1 + ùëíùúΩTxùëñ

=

ùëí ùë¶ùëñùúΩTxùëñ , 1 + ùëíùë¶ùëñùúΩTxùëñ

(3.33a)

1 ‚àí ùëî(xùëñ; ùúΩ) =

ùëí ‚àíùúΩ T xùëñ 1 + ùëí‚àíùúΩTxùëñ

=

ùëí ùë¶ùëñùúΩTxùëñ . 1 + ùëíùë¶ùëñùúΩTxùëñ

(3.33b)

Hence, we get the same expression in both cases and can write (3.32) compactly as

ùêΩ(ùúΩ) =

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

‚àí

ln

1

ùëí ùë¶ùëñùúΩTxùëñ + ùëí ùë¶ùëñùúΩTxùëñ

=

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

‚àí

ln

1

+

1 ùëí ‚àí ùë¶ùëñ ùúΩ T xùëñ

=

1 ‚àëùëõÔ∏Å ùëõ ùëñ=1

ln

1 + ùëí‚àíùë¶ùëñùúΩTxùëñ

.

Logistic loss ùêø (xùëñ , ùë¶ùëñ , ùúΩ)

(3.34)

46

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. ClassiÔ¨Åcation and logistic regression

The loss function ùêø (x, ùë¶ùëñ, ùúΩ) above, which is a special case of the cross-entropy loss, is called the logistic loss (or sometimes binomial deviance). Learning a logistic regression model thus amounts to solving

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

ln

1 + ùëí‚àíùë¶ùëñùúΩTxùëñ

.

(3.35)

Contrary to linear regression with squared error loss, the problem (3.35) has no closed-form solution, so we have to use numerical optimization instead. Solving nonlinear optimization problems numerically is central to training of many machine learning models, not just logistic regression, and we will come back to this topic in Chapter 5. For now, however, it is enough to note that there exist eÔ¨Écient algorithms for solving (3.35) numerically for Ô¨Ånding ùúΩ.

Learn binary logistic regression Data: Training data T = {xùëñ, ùë¶ùëñ }ùëñùëõ=1 (with output classes ùë¶ = {‚àí1, 1}) Result: Learned parameter vector ùúΩ
1 Compute ùúΩ by solving (3.35) numerically.
Predict with binary logistic regression
Data: Learned parameter vector ùúΩ and test input x‚òÖ Result: Prediction ùë¶(x‚òÖ) 1 Compute ùëî(x‚òÖ) (3.29a). 2 If ùëî(x‚òÖ) > 0.5, return ùë¶(x‚òÖ) = 1, otherwise return ùë¶(x‚òÖ) = ‚àí1.

Method 3.2: Logistic regression

Predictions and decision boundaries
So far, we have discussed logistic regression as a method for predicting class probabilities for a test input x‚òÖ by Ô¨Årst learning ùúΩ from training data and thereafter computing ùëî(x‚òÖ), our model for ùëù(ùë¶ = 1 | x‚òÖ). However, sometimes we want to make a ‚Äúhard‚Äù prediction for the test input x‚òÖ, that is, predicting ùë¶(x‚òÖ) = ‚àí1 or ùë¶(x‚òÖ) = 1 in binary classiÔ¨Åcation, just like with ùëò-NN or decision trees. We then have to add a Ô¨Ånal step to the logistic regression model, in which the predicted probabilities are turned into a class prediction. The most common approach is to let ùë¶(x‚òÖ) be the most probable class. For binary classiÔ¨Åcation, we can express this4 as

ùë¶(x‚òÖ) =

1 ‚àí1

if ùëî(x) > ùëü , if ùëî(x) ‚â§ ùëü

(3.36)

with decision threshold ùëü = 0.5, which is illustrated in Figure 3.5. We now have everything in place for summarizing binary logistic regression in Method 3.2.
In some applications, however, it can be beneÔ¨Åcial to explore diÔ¨Äerent thresholds than ùëü = 0.5. It can be shown that if ùëî(x) = ùëù(ùë¶ = 1 | x), that is, the model provides a correct description of the real-world class probabilities, then the choice ùëü = 0.5 will give the smallest possible number of misclassiÔ¨Åcations on average. In other words, ùëü = 0.5 minimizes the so-called misclassiÔ¨Åcation rate. The misclassiÔ¨Åcation rate is, however, not always the most important aspect of classiÔ¨Åer. Many classiÔ¨Åcation problems are asymmetric (it is more important to correctly predict some classes than others) or imbalanced (the classes occur with very diÔ¨Äerent frequency). In a medical diagnosis application, for example, it can be more important not to falsely predict the negative class (that is, by mistake predict a sick patient being healthy)
4It is arbitrary what happens if ùëî(x) = 0.5.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

47

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning

ùëî (x)

1 0.75
0.5 0.25
0

ùë•1

ùë•2

Figure 3.5: In binary classiÔ¨Åcation (ùë¶ = ‚àí1 or 1) logistic regression predicts ùëî(x‚òÖ) (x is two-dimensional here), which is an attempt to determine ùëù(ùë¶ = 1 | x‚òÖ). This implicitly also gives a prediction for ùëù(ùë¶ = ‚àí1 | x‚òÖ) as 1 ‚àí ùëî(x‚òÖ). To turn these probabilities into actual class predictions (ùë¶(x‚òÖ) is either ‚àí1 or 1), the class which is modeled to have the highest probability can be taken as the prediction, as in Equation (3.36). The point(s) where the prediction
changes from from one class to another is the decision boundary (gray plane).

than to falsely predict the positive class (by mistake predict a healthy patient as sick). For such a problem, minimizing the misclassiÔ¨Åcation rate might not lead to the desired performance. Furthermore, the medical diagnosis problem could be imbalanced if the disorder is very rare, meaning that the vast majority of the data points (patients) belong to the negative class. By only considering the misclassiÔ¨Åcation rate in such a situation we implicitly value accurate predictions of the negative class higher than accurate predictions of the positive class, simply because the negative class is more common in the data. We will discuss how we can evaluate such situations more systematically in Section 4.5. In the end, however, the decision threshold ùëü is a choice that the user has to make.
The decision boundary for binary classiÔ¨Åcation can be computed by solving the equation

ùëî(x) = 1 ‚àí ùëî(x).

(3.37)

The solutions to this equation are points in the input space for which the two classes are predicted to be equally probable. These points therefore lie on the decision boundary. For binary logistic regression, this means

ùëíùúΩTx 1 + ùëíùúΩTx

=

1 1 + ùëíùúΩTx

‚áî

ùëíùúΩTx

=

1

‚áî

ùúΩTx

=

0.

(3.38)

The equation ùúΩTx = 0 parameterizes a (linear) hyperplane. Hence, the decision boundaries in logistic
regression always have the shape of a (linear) hyperplane. From the derivation above it can be noted that the sign of the expression ùúΩTx determines if we predict
the positive or the negative class. Hence, we can compactly write (3.36), with the threshold ùëü = 0.5, as

ùë¶(x‚òÖ) = sign(ùúΩTx‚òÖ).

(3.39)

In general we distinguish between diÔ¨Äerent types of classiÔ¨Åers by the shape of their decision boundaries.
A classiÔ¨Åer whose decision boundaries are linear hyperplanes is a linear classiÔ¨Åer.
All other classiÔ¨Åers are nonlinear classiÔ¨Åers. Logistic regression is an example of a linear classiÔ¨Åer, whereas ùëò-NN and decision trees are nonlinear classiÔ¨Åers. Note that the term ‚Äúlinear‚Äù is used in a diÔ¨Äerent

48

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. ClassiÔ¨Åcation and logistic regression

sense for linear regression; linear regression is a model which is linear in its parameters, whereas a linear classiÔ¨Åer is a model whose decision boundaries are linear.
The same arguments and constructions as above can be generalized to the multiclass setting. Predicting according to the most probable class then amounts to computing the prediction as

ùë¶ (x‚òÖ)

=

arg

max
ùëö

ùëîùëö (x‚òÖ).

(3.40)

As for the binary case, it is possible to modify this when working with an asymmetric or imbalanced problem. As for the the decision boundaries, they will be given by a combination of ùëÄ ‚àí 1 (linear)
hyperplanes for a multiclass logistic regression model.

Logistic regression for more than two classes

Logistic regression can be used also for the multiclass problem when there are more than two classes,
ùëÄ > 2. There are several ways of generalizing logistic regression to this setting. We will follow one
path using the so-called softmax function which will be useful also later when introducing deep learning
models in Chapter 6. For the binary problem, we used the logistic function to design a model for ùëî(x), a scalar-valued function
representing ùëù(ùë¶ = 1 | x). For the multiclass problem we instead have to design a vector-valued function g(x), whose elements should be non-negative and sum to one. For this purpose, we Ô¨Årst use ùëÄ instances of (3.28), each denoted ùëßùëö and each with a diÔ¨Äerent set of parameters ùúΩùëö, ùëßùëö = ùúΩTùëöx. We stack all ùëßùëö into a vector of logits z = [ùëß1 ùëß2 . . . ùëßùëÄ ]T and use the softmax function as a vector-valued generalization of the logistic function,

softmax(z)

1

ùëÄ ùëö=1

ùëíùëßùëö

Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùëíùëíùëíùëß...ùëßùëßùëÄ12 Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª .

(3.41)

Note that the argument z to the softmax function is an ùëÄ-dimensional vector, and that it also returns a vector of the same dimension. By construction, the output vector from the softmax function always sums to 1, and each element is always ‚â• 0. Similarly to how we combined linear regression and the logistic

function for the binary classiÔ¨Åcation problem (3.29), we have now combined linear regression and the

softmax function to model the class probabilities,

g(x) = softmax(z),

where z = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùúΩùúΩùúΩTùëÄT1T2... xxxÔ£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª .

(3.42)

Equivalently, we can write out the individual class probabilities, that is, the elements of the vector g(x), as

ùëîùëö (x) =

ùëíùúΩTùëöx

,

ùëÄ ùëó =1

ùëíùúΩTùëó x

ùëö = 1, . . . , ùëÄ.

(3.43)

This is the multiclass logistic regression model. Note that this construction uses ùëÄ parameter vectors
ùúΩ1, . . . , ùúΩ ùëÄ (one for each class), meaning that the number of parameters to learn grows with ùëÄ. As for binary logistic regression, we can learn those parameters using the maximum likelihood method. We use ùúΩ to denote all model parameters, ùúΩ = {ùúΩ1, . . . , ùúΩ ùëÄ }. Since ùëîùëö(xùëñ; ùúΩ) is our model for ùëù(ùë¶ùëñ = ùëö | xùëñ), the cost function for the cross-entropy (equivalently, negative log-likelihood) loss for the multiclass problem is

ùêΩ(ùúΩ)

=

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

‚àí ln ùëîùë¶ùëñ (xùëñ; ùúΩ)

.

Multiclass cross-entropy

loss ùêø (g(xùëñ;ùúΩ) ,ùë¶ùëñ)

(3.44)

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

49

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Energy (scale 0-1)

Basic parametric models and a statistical perspective on learning

1

Beatles

Kiss

Bob Dylan

0.5

0

5

6

7

Length (ln s)

Figure 3.6: Logistic regression applied to the music classiÔ¨Åcation problem from Example 2.1. The decision boundaries are linear, but unlike trees (Figure 2.11a) they are not perpendicular to the axes.

Note that we use the training data labels ùë¶ùëñ as index variables to select the correct conditional probability for the loss function. That is, the ùëñth term of the sum is the negative logarithm of the ùë¶ùëñth element of the vector g(xùëñ; ùúΩ). We illustrate the meaning of this in Example 3.4.
Inserting the model (3.43) into the loss function (3.44) gives the cost function to optimize when learning multiclass logistic regression,

ùêΩ(ùúΩ)

=

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

‚àíùúΩ

T ùë¶ùëñ

xùëñ

+

ln

‚àëùëÄÔ∏Å

ùëíùúΩTùëó xùëñ

ùëó =1

.

(3.45)

We apply this to the music classiÔ¨Åcation problem in Figure 3.6.

Example 3.4: The cross-entropy loss for multiclass problems

Consider the following (very small) data set with ùëõ = 6 data points, ùëù = 2 input dimensions and ùëÄ = 3 classes, which we want to use for learning a multiclass classiÔ¨Åer:

X = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞‚àí‚àí‚àí000000...492...283160521

‚àí‚àí100011....5581..85786843Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª ,

y = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞231213Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª .

Multiclass logistic regression with softmax (or any other multiclass classiÔ¨Åer which predicts a vector of conditional class probabilities) return a 3-dimensional probability vector g(x; ùúΩ) for any x and ùúΩ. If we stack the logarithms of the transpose of all vectors g(xùëñ; ùúΩ) for ùëñ = 1, . . . , 6, we obtain the matrix

G = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞

ln ùëî1 (x1; ùúΩ) ln ùëî1 (x2; ùúΩ) ln ùëî1 (x3; ùúΩ) ln ùëî1 (x4; ùúΩ) ln ùëî1 (x5; ùúΩ) ln ùëî1 (x6; ùúΩ)

ln ùëî2 (x1; ùúΩ) ln ùëî2 (x2; ùúΩ) ln ùëî2 (x3; ùúΩ) ln ùëî2 (x4; ùúΩ) ln ùëî2 (x5; ùúΩ) ln ùëî2 (x6; ùúΩ)

ln ùëî3 (x1; ùúΩ) ln ùëî3 (x2; ùúΩ) ln ùëî3 (x3; ùúΩ) ln ùëî3 (x4; ùúΩ) ln ùëî3 (x5; ùúΩ) ln ùëî3 (x6; ùúΩ)

Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª .

Computing the multi-class cross-entropy cost (3.44) now simply amounts to taking the average of all circled elements, and multiplying that with ‚àí1. The element that we have circled in row ùëñ is given by the training label ùë¶ùëñ. Training the model amounts to Ô¨Ånding ùúΩ such that this negated average is minimized.

50

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Polynomial regression and regularization

Time to reÔ¨Çect 3.3: Can you derive (3.32) as a special case of (3.44)?

Hint: think of the binary classiÔ¨Åer as a special case of the multiclass classiÔ¨Åer with g(x) =

ùëî (x) 1 ‚àí ùëî(x)

.

Time to reÔ¨Çect 3.4: The softmax-based logistic regression is actually over-parameterized, in the
sense that we can construct an equivalent model with fewer parameters. That is often not a problem in practice, but compare the multiclass model (3.42) for the case ùëÄ = 2 with binary logistic
regression (3.29) and see if you can spot the over-parametrization!

3.3 Polynomial regression and regularization

In comparison to ùëò-NN and decision trees studied in Chapter 2, linear and logistic regression might appear to be rigid and non-Ô¨Çexible models with their straight lines (such as Figures 3.1 and 3.5). However, both models are able to adapt to the training data well if the input dimension ùëù is large in relation to the number of data points ùëõ.
A common way of increasing the input dimension in linear and logistic regression, which we will discuss more thoroughly in Chapter 8, is to make a nonlinear transformation of the input. A simple nonlinear transformation is to replace a one-dimensional input ùë• with itself raised to diÔ¨Äerent powers, which makes the linear regression model a polynomial

ùë¶ = ùúÉ0 + ùúÉ1ùë• + ùúÉ2ùë•2 + ùúÉ3ùë•3 + ¬∑ ¬∑ ¬∑ + ùúÄ.

(3.46)

This is called polynomial regression. The same polynomial expansion can be applied also to the expression for the logit in logistic regression. Note that if we let ùë•1 = ùë•, ùë•2 = ùë•2 and ùë•3 = ùë•3, this is still a linear model (3.2) with input x = [1 ùë• ùë•2 ùë•3], but we have ‚Äòlifted‚Äô the input from being one-dimensional (ùëù = 1) to three-dimensional (ùëù = 3). Using nonlinear input transformations can be very useful in practice, but it
eÔ¨Äectively increases ùëù and we can easily end up with overÔ¨Åtting the model to the noise‚Äîrather than the
interesting patterns‚Äîin the training data, as in the example below.

Example 3.5: Car stopping distances with polynomial regression

We return to Example 2.2, but this time we also add the squared speed as an input and thereby use a 2nd order polynomial in linear regression. This gives the new matrices (compared to Example 3.1)

X = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞11111...

4.0 4.9 5.0
...
39.6 39.7

111225564567...86...000..21Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª ,

ùúΩ = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞ùúÉùúÉùúÉ012Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª ,

y = Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞114883140......000..00Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª ,

(3.47)

and when we insert them into the normal equations (3.13), the new parameter estimates are ùúÉ0 = 1.58, ùúÉ1 = 0.42 and ùúÉ2 = 0.07. (Note that also ùúÉ0 and ùúÉ1 change, compared to Example 3.2.)
In a completely analogous way we also learn a 10th order polynomial, and we illustrate them all in Figure
3.7.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

51

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning

150

Linear regression with ùë•

Linear regression with ùë• and ùë•2

Linear regression with ùë•, ùë•2, . . . , ùë•10

Data
100

Distance (feet)

50

Fig. 3.7

0

0

5

10

15

20

25

30

35

40

45

Speed (mph)

The second order polynomial (red line) appears sensible, and using a 2nd order polynomial seems to give some advantage compared to plain linear regression (blue line, from Example 3.2). However, using a 10th order polynomial (green line) seems to make the model less useful than even plain linear regression due to overÔ¨Åtting. In conclusion, there is merit to the idea of polynomial regression, but it has to be applied carefully.

One way to avoid issues with overÔ¨Åtting when augmenting the input with nonlinear transformation is to

carefully select which inputs (transformations) to include. There are various strategies for doing this, for

instance by adding inputs one at a time (forward selection) or by starting with a large number of inputs

and then gradually removing the ones that are considered redundant (backward elimination). DiÔ¨Äerent

candidate models can be evaluated and compared using cross-validation, as we discuss in Chapter 4. See

also Chapter 11 where we discuss input selection further.

An alternative approach to explicit input selection which can also be used to mitigate overÔ¨Åtting is

regularization. The idea of regularization can be described as ‚Äòkeeping the parameters ùúΩ small unless the

data really convinces us otherwise‚Äô, or alternatively ‚Äòif a model with small parameter values ùúΩ Ô¨Åts the

data almost as well as a model with larger parameter values, the one with small parameter values should

be preferred‚Äô. There are several ways to implement this idea mathematically, which leads to diÔ¨Äerent

regularization methods. We will give a more complete treatment of this in Section 5.3, and only discuss the so-called ùêø2 regularization for now. When paired with regularization, the idea of using nonlinear input

transformations can be very powerful and enables a whole family of supervised machine learning methods

that we will properly introduce and discuss in Chapter 8.

To keep ùúΩ small an extra penalty term ùúÜ

ùúΩ

2 2

is

added

to

the

cost

function

when

using

ùêø2

regularization.

Here, ùúÜ ‚â• 0, referred to as the regularization parameter, is a hyperparameter chosen by the user which

controls the strength of the regularization eÔ¨Äect. The purpose of the penalty term is to prevent overÔ¨Åtting.

Whereas the original cost function only rewards the Ô¨Åt to training data (which encourages overÔ¨Åtting), the

regularization term prevents overly large parameter values at the cost of a slightly worse Ô¨Åt. It is therefore

important to choose the regularization parameter ùúÜ wisely, to obtain the right amount of regularization.

With ùúÜ = 0 the regularization has no eÔ¨Äect, whereas ùúÜ ‚Üí ‚àû will force all parameters ùúΩ to 0. A common

approach is to use cross-validation (see Chapter 4) to select ùúÜ.

If we add ùêø2 regularization to the previously studied linear regression model with squared error loss (3.12), the resulting optimization problem becomes,5

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

XùúΩ ‚àí y

2 2

+

ùúÜ

ùúΩ

2 2

.

(3.48)

It turns out that, just like the non-regularized problem, (3.48) has a closed-form solution given by a

5In practice, it can be wise to exclude ùúÉ0, the intercept, from the regularization.

52

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Generalized linear models

modiÔ¨Åed version of the normal equations,

(XTX + ùëõùúÜIùëù+1)ùúΩ = XTy,

(3.49)

where Iùëù+1 is the identity matrix of size ( ùëù + 1) √ó ( ùëù + 1). This particular application of ùêø2 regularization is referred to as ridge regression.
Regularization is not restricted to linear regression, however. The same ùêø2 penalty can be applied
to any method that involves optimizing a cost function. For instance, for logistic regression we get the
optimization problem

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

‚àëùëõÔ∏Å ln
ùëñ=1

1 + exp

‚àíùë¶ùëñ ùúΩTxùëñ

+ùúÜ

ùúΩ

2 2

.

(3.50)

It is common in practice to learn logistic regression using (3.50) instead of (3.29). One reason is to decrease possible issues with overÔ¨Åtting, as discussed above. Another reason is that for the non-regularized cost function (3.29), the optimal ùúΩ is not Ô¨Ånite if the training data is linearly separable (meaning there exists a linear decision boundary which separates the classes perfectly). In practice it means that the logistic regression learning diverges with some datasets, unless (3.50) (with ùúÜ > 0) is used instead of (3.29). Finally, the regularization term implies that there is a unique solution to the optimization problem, despite the fact that the softmax model is overparameterized as discussed above.

3.4 Generalized linear models
In this chapter we have introduced two basic parametric models for regression and classiÔ¨Åcation, linear regression and logistic regression, respectively. The latter model was presented as a way of adapting linear regression to the categorical nature of the output ùë¶ encountered in a classiÔ¨Åcation problem. This was done by passing the linear regression through a nonlinear (in this case logistic) function, allowing us to interpret the output as a class probability.
The same principle can be generalized to adapt the linear regression model other properties of the output as well, resulting in what is referred to as generalized linear models. In the discussion above we have focused on two speciÔ¨Åc problems corresponding to two diÔ¨Äerent types of output data, real-valued regression (ùë¶ ‚àà R) and classiÔ¨Åcation (ùë¶ ‚àà {1, . . . , ùëÄ }). These are the most common instances of supervised learning problems and, indeed, they will be central to most of the discussion and methods presented in this book.
However, in various applications we might encounter data with other properties, not well described by either of the two standard problems. For instance, assume that the output ùë¶ corresponds to the count of some quantity, such as the number of earthquakes in a certain area during a Ô¨Åxed time interval, the number of persons diagnosed with a speciÔ¨Åc illness in a certain region, or the number of patents granted to a tech company. In such cases ùë¶ is a natural number taking one of the values 0, 1, 2, . . . (formally, ùë¶ ‚àà N). Such count data, despite being numerical in nature, is not well described by a linear regression model of the form (3.2)6. The reason is that the linear regression models is not restricted to discrete values, neither to be non-negative, even though we know that this is the case for the actual output ùë¶ that we are trying to model. Neither does this scenario correspond to a classiÔ¨Åcation setting, since ùë¶ is numerical (that is the values can be ordered) and there is no Ô¨Åxed upper limit on how large ùë¶ can be.
To address this issue we will extend our notion of parametric models to encompass various probability distribution that can be used to model the conditional output distribution ùëù(ùë¶ | x; ùúΩ). The Ô¨Årst step is to choose a suitable form for the conditional distribution ùëù(ùë¶ | x; ùúΩ). This is part of the model design, guided by the properties of the data. SpeciÔ¨Åcally, we should select a distribution with support corresponding to that of the data (such as the natural numbers). Naturally, we still want to allow the distribution to depend on the input variable x‚Äîmodeling the relationship between the input and the output variables is the fundamental task of supervised learning after all! However, this can be accomplished by Ô¨Årst computing
6Simply assuming that the distribution of the additive noise ùúÄ is discrete is not enough, since the regression function itself ùúΩTx can take arbitrary real values.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

53

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning

a linear regression term ùëß = ùúΩTx, and then letting the conditional distribution ùëù(ùë¶ | x; ùúΩ) depend on ùëß in some appropriate way. We illustrate with an example.
Example 3.6: Poisson regression

A simple model for count data is to use a Poisson likelihood. The Poisson distribution is supported on the natural numbers (including 0) and has probability mass function,

Pois(ùë¶; ùúÜ)

=

ùúÜùë¶ ùëí‚àíùúÜ ùë¶!

,

ùë¶ = 0, 1, 2, . . .

The so called rate parameter ùúÜ controls the shape of the distribution and also corresponds to its mean value, E[ùë¶] = ùúÜ. To use this likelihood in a regression model for count data we can let ùúÜ depend on the input variable x and the model parameters ùúΩ through a linear regression ùëß = ùúΩTx. However, the rate parameter ùúÜ is
restricted to be positive. To ensure that this constraint is satisÔ¨Åed, we model ùúÜ according to

ùúÜ = exp(ùúΩTx).

The exponential function maps the output from the linear regression component to the positive real line, resulting in a valid rate parameter for any x and ùúΩ. Thus, we get the model

ùëù(ùë¶ | x; ùúΩ) = Pois ùë¶; exp(ùúΩTx) ,

referred to a Poisson regression model.

In the Poisson regression model we can write the conditional mean of the output as

E[ùë¶ | x; ùúÉ] = ùúô‚àí1(ùëß),

where

ùëß = ùúΩTx

and ùúô(ùúá) d=ef log(ùúá). The idea of providing an explicit link between the linear regression term and the conditional mean of the output in this way is what underlies the generic framework of generalized linear models. SpeciÔ¨Åcally, a generalized linear model consists of:

1. A choice of output distribution ùëù(ùë¶ | x; ùúΩ) from the exponential family of distributions.7 2. A linear regression term ùëß = ùúΩTx. 3. A strictly increasing, so called, link function ùúô, such that E[ùë¶ | x; ùúÉ] = ùúô‚àí1(ùëß).

By convention, we map the linear regression output through the inverse of the link function to obtain the mean of the output. Equivalently, if ùúá denotes the mean of ùëù(ùë¶ | x; ùúΩ) we can express the model as ùúô(ùúá) = ùúΩTx.
DiÔ¨Äerent choices of conditional distributions and link functions result in diÔ¨Äerent models with varying properties. In fact, as hinted at above we have already seen another example of a generalized linear model, namely the logistic regression model. In binary logistic regression the output distribution ùëù(ùë¶ | x; ùúΩ) is a Bernoulli distribution8, the logit is computed as ùëß = ùúΩTx and the link function ùúô is given by the inverse of the logistic function, ùúô(ùúá) = log(ùúá/(1 ‚àí ùúá)). Other examples include negative binomial regression (a more Ô¨Çexible model for count data than Poisson regression) and exponential regression (for non-negative real-valued outputs). Hence, the generalized linear model framework can be used to model output variables ùë¶ with many diÔ¨Äerent properties, and it allows us to describe these models in a common language.
Since generalized linear models are deÔ¨Åned in terms of the conditional distribution ùëù(ùë¶ | x; ùúΩ), that is the likelihood, it is natural to adopt the maximum likelihood formulation for training. That is, we train
7The exponential family is a class of probability distributions that can be written on a particular exponential form. It includes many of the commonly used probability distributions, such as Gaussian, Bernoulli, Poisson, exponential, gamma, etc.
8The generalized linear model interpretation of logistic regression is more straightforward if we encode the classes as 0/1 instead of ‚àí1/1, in which case the output is modeled with a Bernoulli distribution with mean E[ùë¶ | ùë•; ùúΩ] = ùëù(ùë¶ = 1 | x; ùúΩ) = ùëîùúΩ (x).

54

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Further reading

the model by Ô¨Ånding the parameter values such that the negative log-likelihood of the training data is minimized,

ùúΩ = arg min
ùúΩ

‚àí

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

ln

ùëù(

ùë¶ùëñ

|

xùëñ ;

ùúΩ)

.

(3.51)

A regularization penalty can be added to the cost function, similarly to (3.50). Regularization is discussed in more detail in Section 5.3.
In general, just as for logistic regression, the training objective (3.51) is a nonlinear optimization problem without a closed form solution. However, an important aspect of generalized linear models is that eÔ¨Écient numerical optimization algorithms exist for solving the maximum likelihood problem. SpeciÔ¨Åcally, under certain assumptions on the link function the problem becomes convex and Newton‚Äôs method can be used to compute ùúΩ eÔ¨Éciently. These are concepts that we will return to in Section 5.4 when we discuss numerical optimization in more detail.

3.5 Further reading
Compared to the thousand year old ùëò-NN idea (Chapter 2), the linear regression model with least squares cost is much younger and can ‚Äúonly‚Äù be traced back a bit over two hundred years. It was introduced by Adrien-Marie Legendre in his 1805 book Nouvelles m√©thodes pour la d√©termination des orbites des com√©tes (New methods for the determination of the orbits of comets) as well as Carl Friedrich Gauss in his 1809 book Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium (Theory of the motion of the heavenly bodies moving about the sun in conic sections; in that book he claims to have been using it since 1795). Gauss also did the interpretation of it as the maximum likelihood solution when the noise was assumed to have a Gaussian distribution (hence the name of the distribution), although the general maximum likelihood approach was introduced much later by the work of Ronald Fisher (Fisher 1922). The history of logistic regression is almost as old as linear regression, and is described further by Cramer (2003). An in-depth account of generalized linear models is given in the classical textbook by McCullagh and Nelder (2018).

3.A Derivation of the normal equations

The normal equations (3.13)

XTXùúΩ = XTy.

can

be

derived

from

(3.12)

(the

scaling

1 ùëõ

does

not

aÔ¨Äect

the

minimizing

argument)

ùúΩ = arg min
ùúΩ

XùúΩ ‚àí y 22,

in diÔ¨Äerent ways. We will present one based on (matrix) calculus and one based on geometry and linear algebra.
No matter how (3.13) is derived, if XTX is invertible, it (uniquely) gives

ùúΩ = (XTX)‚àí1XTy,

If XTX is not invertible, then (3.13) has inÔ¨Ånitely many solutions ùúΩ, which all are equally good solutions to the problem (3.12).

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

55

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Basic parametric models and a statistical perspective on learning

A calculus approach

Let

ùëâ(ùúΩ) =

XùúΩ ‚àí y

2 2

=

(XùúΩ ‚àí y)T (XùúΩ ‚àí y) = yTy ‚àí 2yTXùúΩ + ùúΩTXTXùúΩ,

and diÔ¨Äerentiate ùëâ (ùúΩ) with respect to the vector ùúΩ,

(3.52)

ùúï ùúïùúΩ

ùëâ

(ùúΩ)

=

‚àí2XTy

+

2XT XùúΩ .

(3.53)

Since

ùëâ (ùúΩ)

is

a

positive

quadratic

form,

its

minimum

must

be

attained

at

ùúï ùúïùúΩ

ùëâ

(

ùúΩ

)

=

0,

which

characterizes

the solution ùúΩ as

ùúï ùúïùúΩ

ùëâ

(ùúΩ)

=

0

‚áî

‚àí2XTy

+

2XTXùúΩ

=

0

‚áî

XTXùúΩ

=

XTy,

(3.54)

which is the normal equations.

A linear algebra approach

Denote the ùëù + 1 columns of X as ùëê ùëó, ùëó = 1, . . . , ùëù + 1. We Ô¨Årst show that

XùúΩ ‚àí y

2 2

is

minimized

if

ùúΩ

is

chosen such that XùúΩ is the orthogonal projection of y onto the (sub)space spanned by the columns ùëê ùëó of X,

and then show that the orthogonal projection is found by the normal equations.

Let us decompose y as y‚ä• + y , where y‚ä• is orthogonal to the (sub)space spanned by all columns ùëêùëñ, and ùë¶ is in the (sub)space spanned by all columns ùëêùëñ. Since y‚ä• is orthogonal to both y and XùúΩ, it follows that

XùúΩ

‚àíy

2 2

=

XùúΩ ‚àí (y‚ä• + y

)

2 2

=

(XùúΩ ‚àí y

) ‚àí y‚ä•

2 2

‚â•

y‚ä• 22,

(3.55)

and the triangle inequality also gives us

XùúΩ

‚àíy

2 2

=

XùúΩ ‚àí y‚ä• ‚àí y

2 2

‚â§

y‚ä•

2 2

+

XùúΩ ‚àí y

2 2

.

(3.56)

This implies that if we choose ùúΩ such that XùúΩ = y , the criterion

XùúΩ ‚àí y

2 2

must

have

reached

its

minimum.

Thus, our solution ùúΩ must be such that XùúΩ ‚àí y is orthogonal to the (sub)space spanned by all columns ùëêùëñ,

meaning

(y ‚àí XùúΩ)Tùëê ùëó = 0, ùëó = 1, . . . , ùëù + 1

(3.57)

(remember that two vectors u, v are, by deÔ¨Ånition, orthogonal if their scalar product, uTv, is 0). Since the columns ùëê ùëó together form the matrix X, we can write this compactly as

(y ‚àí XùúΩ)TX = 0,

(3.58)

where the right hand side is the ùëù + 1-dimensional zero vector. This can equivalently be written as

XTXùúΩ = XTy,

which is the normal equations.

56

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

4 Understanding, evaluating and improving the performance

We have so far encountered four diÔ¨Äerent methods for supervised machine learning, and more are to come in later chapters. We always learn the models by adapting them to training data, and hope that the models thereby will give us good predictions also when faced with new, previously unseen, data. But can we really expect that to work? This is a very important question for the practical usefulness of machine learning. In this chapter we will discuss this question in a rather general sense, before diving into more advanced methods in later chapters. By doing so we will unveil some interesting concepts, and also discover some practical tools for evaluating, improving, and choosing between diÔ¨Äerent supervised machine learning methods.

4.1 Expected new data error ùê∏new: performance in production
We start by introducing some concepts and notation. First, we deÔ¨Åne an error function ùê∏ (ùë¶, ùë¶) which encodes the purpose of classiÔ¨Åcation or regression. The error function compares a prediction ùë¶(x) to a measured data point ùë¶, and returns a small value (possibly zero) if ùë¶(x) is a good prediction of ùë¶, and a larger value otherwise. Similarly to how we can use diÔ¨Äerent loss functions when training a model, we can consider many diÔ¨Äerent error functions, depending on what properties of the prediction that are most important for the application at hand. However, unless otherwise stated, our default choices are misclassiÔ¨Åcation and squared error, respectively:

MisclassiÔ¨Åcation: Squared error:

ùê∏(ùë¶, ùë¶) ùê∏(ùë¶, ùë¶)

I{ùë¶ ‚â† ùë¶} =

0 1

if ùë¶ = ùë¶ if ùë¶ ‚â† ùë¶

(ùë¶ ‚àí ùë¶)2

(classiÔ¨Åcation) (regression)

(4.1a) (4.1b)

When we compute the average misclassiÔ¨Åcation (4.1a), we usually refer to it as the misclassiÔ¨Åcation rate (or 1 minus the misclassiÔ¨Åcation rate as the accuracy). The misclassiÔ¨Åcation rate is often a natural quantity to consider in classiÔ¨Åcation, but for imbalanced or asymmetric problems other aspects might be more
important, as we discuss in Section 4.5. The error function ùê∏ (ùë¶, ùë¶) has similarities to a loss function ùêø(ùë¶, ùë¶). However, they are used diÔ¨Äerently:
A loss function is used when learning (or, equivalently, training) a model, whereas we use the error function to analyze performance of an already learned model. There are reasons for choosing ùê∏ (ùë¶, ùë¶) and ùêø(ùë¶, ùë¶) diÔ¨Äerently, which we will come back to soon.
In the end, supervised machine learning amounts to designing a method which performs well when
faced with an endless stream of new, unseen data. Imagine for example all real-time recordings of street
views that have to be processed by a vision system in a self-driving car once it is sold to a customer, or
all incoming patients that have to be classiÔ¨Åed by a medical diagnosis system once it is implemented
in clinical practice. The performance on fresh unseen data can in mathematical terms be understood as
the average of the error function‚Äîhow often the classiÔ¨Åer is right, or how good the regression method
predicts. To be able to mathematically describe the endless stream of new data, we introduce a distribution over data ùëù(x, ùë¶). In most other chapters, we only consider the output ùë¶ as a random variable whereas the input x is considered Ô¨Åxed. In this chapter, however, we have to think of also the input x as a random variable with a certain probability distribution. In any real-world machine learning scenario ùëù(x, ùë¶) can be extremely complicated and practically impossible to write down. We will nevertheless use ùëù(x, ùë¶)

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

57

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

to reason about supervised machine learning methods, and the bare notion of ùëù(x, ùë¶) (even though it is unknown in practice) will be helpful for that.
No matter which speciÔ¨Åc classiÔ¨Åcation or regression method we consider, once it has been learned from training data T = {xùëñ, ùë¶ùëñ }ùëñùëõ=1, it will return predictions ùë¶(x‚òÖ) for any new input x‚òÖ we feed to it. In this chapter we will write ùë¶(x; T ) to emphasize the fact that the model depends on the training data T . Indeed, if we would use a diÔ¨Äerent training data set to learn the same (type of) model, this would typically result
in a diÔ¨Äerent model with diÔ¨Äerent predictions.
In the other chapters we mostly discuss how a model predicts the output for one, or a few, test inputs x‚òÖ. Let us take that to the next level by integrating (averaging) the error function (4.1) over all possible test data points with respect to the distribution ùëù(x, ùë¶). We refer to this as the expected new data error

ùê∏new E‚òÖ [ùê∏ (ùë¶(x‚òÖ; T ), ùë¶‚òÖ)] ,

(4.2)

where the expectation E‚òÖ is the expectation over all possible test data points with respect to the distribution (x‚òÖ, ùë¶‚òÖ) ‚àº ùëù(x, ùë¶), that is,

‚à´

E‚òÖ [ùê∏ (ùë¶(x‚òÖ; T ), ùë¶‚òÖ)] = ùê∏ (ùë¶(x‚òÖ; T ), ùë¶‚òÖ) ùëù(x‚òÖ, ùë¶‚òÖ) ùëëx‚òÖùëëùë¶‚òÖ.

(4.3)

Remember that the model (regardless if it is linear regression, a classiÔ¨Åcation tree, an ensemble of trees, a neural network, or something else) is trained on a given training dataset T and represented by ùë¶(¬∑; T ). What is happening in equation (4.2) is an averaging over possible test data points (x‚òÖ, ùë¶‚òÖ). Thus, ùê∏new describes how well the model generalizes from the training data T to new situations.
We also introduce the training error

ùê∏train

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

ùê∏

(ùë¶

( xùëñ ;

T

),

ùë¶ùëñ),

(4.4)

where {xùëñ, ùë¶ùëñ }ùëñùëõ=1 is the training data T . ùê∏train simply describes how well a method performs on the speciÔ¨Åc data on which it was trained, but in general this gives no information on how well the method will perform for new unseen data points.1

Time to reÔ¨Çect 4.1: What is ùê∏train for ùëò-NN with ùëò = 1?

Whereas the training error ùê∏train describes how well the method is able to ‚Äúreproduce‚Äù the data from which it was learned, the expected new data error ùê∏new tells us how well a method performs when we put it ‚Äúin production‚Äù. For instance, what is the rate of false and missed pedestrian detections that we can expect a vision system in a self-driving car to make? Or, how large proportion of all future patients will a medical diagnosis system get wrong?
The overall goal in supervised machine learning is to achieve as small ùê∏new as possible.
This sheds some additional light on the comment we made previously, that the loss function ùêø(ùë¶, ùë¶) and the error function ùê∏ (ùë¶, ùë¶) do not have to be the same. As we will discuss thoroughly in this chapter, a model which Ô¨Åts the training data well and consequently has a small ùê∏train might still have a large ùê∏new when faced with new unseen data. The best strategy to achieve a small ùê∏new is therefore not necessarily to minimize ùê∏train. Besides the fact that the misclassiÔ¨Åcation (4.1a) is unsuitable to use as optimization objective (it is discontinuous and has derivative zero almost everywhere) it can also, depending on the method, be argued that ùê∏new can be made smaller by a more clever choice of loss function. Examples of when this is the case include gradient boosting (Chapter 7) and support vector machines (Chapter 8).
1The term ‚Äúrisk function‚Äù is used in some literature for the expected loss, which is the same as the new data error ùê∏new if the loss function and the error function are chosen to be the same. The training error ùê∏train is then referred to as ‚Äúempirical risk‚Äù, and the idea of minimizing the cost function as ‚Äúempirical risk minimization‚Äù.

58

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Estimating ùê∏new

Finally, it is worth noting that not all methods are trained by explicitly minimizing a loss function (ùëò-NN
is one such example), but the idea of evaluating the performance of the model using an error function still
applies, no matter how it is trained.
Unfortunately, in practical cases we can never compute ùê∏new to assess how well we are doing. The reason is that ùëù(x, ùë¶)‚Äîwhich we do not know in practice‚Äîis part of the deÔ¨Ånition of ùê∏new. However, ùê∏new is a too important quantity to be abandoned just because we cannot compute it exactly. Instead, we will spend quite some time and eÔ¨Äort on estimating ùê∏new from data, as well as analyzing how ùê∏new behaves to better understand how we can decrease it.
We emphasize that ùê∏new is a property of a trained model and a speciÔ¨Åc machine learning problem. That is, we cannot talk about ‚Äúùê∏new for logistic regression‚Äù in general, but instead we have to make more speciÔ¨Åc statements, like ‚Äúùê∏new for the handwritten digit recognition problem, with a logistic regression classiÔ¨Åer trained on the MNIST data2‚Äù.

4.2 Estimating ùê∏new
There are multiple reasons for a machine learning engineer to be interested in ùê∏new, such as:
‚Ä¢ judging if the performance is satisfying (whether ùê∏new is small enough), or if more work should be put into the solution and/or more training data should be collected,
‚Ä¢ choosing between diÔ¨Äerent methods, ‚Ä¢ choosing hyperparameters (such as ùëò in ùëò-NN, the regularization parameter in ridge regression, or
the number of hidden layers in deep learning) in order to minimize ùê∏new, ‚Ä¢ reporting the expected performance to the customer.
As discussed above, we can unfortunately not compute ùê∏new in any practical situation. We will therefore explore various ways of estimating ùê∏new, which will lead us to a very useful concept known as crossvalidation.

ùê∏train ùê∏new: We cannot estimate ùê∏new from training data

We have both introduced the expected new data error, ùê∏new, and the training error ùê∏train. In contrast to
ùê∏new, we can always compute ùê∏train. We assume for now that T consists of samples (data points) from ùëù(x, ùë¶). This means that the training

data is assumed to have been collected under similar circumstances as the ones the learned model will be

used under, which is a common assumption.

When an expected value, such as in the deÔ¨Ånition of ùê∏new in (4.2), cannot be computed in closed form, one option is to approximate the expected value by a sample average. EÔ¨Äectively this means that we

approximate the integral (expected value) by a Ô¨Ånite sum. Now, the question is if the integral in ùê∏new can be well approximated by the sum in ùê∏train, like

‚à´ ùê∏new =

ùê∏ (ùë¶(x; T ),

ùë¶)) ùëù(x,

ùë¶)ùëëxùëëùë¶

?‚âà?

1 ùëõ

‚àëùëõÔ∏Å ùê∏ (ùë¶(xùëñ; T ),
ùëñ=1

ùë¶ùëñ)

=

ùê∏ train .

(4.5)

Put diÔ¨Äerently: Can we expect a method to perform equally well when faced with new, previously unseen, data, as it did on the training data?
The answer is, unfortunately, no.

Time to reÔ¨Çect 4.2: Why can we not expect the performance on training data (ùê∏train) to be a good
approximation for how a method will perform on new, previously unseen data (ùê∏new), even though the training data is drawn from the distribution ùëù(x, ùë¶)?

2http://yann.lecun.com/exdb/mnist/

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

59

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance
All available data

Training data T

Hold-out validation data

Figure 4.1: The hold-out validation dataset approach: If we split the available data in two sets and train the model on the training set, we can compute ùê∏hold-out using the hold-out validation set. ùê∏hold-out is an unbiased estimate of ùê∏new, and the more data in the hold-out validation dataset the less variance (better estimate) in ùê∏hold-out, but the less data left for training the model (larger ùê∏new).

Equation (4.5) does not hold, and the reason is that the samples used to approximate the integral are given by the training data points. However, these data points are also used to train the model and, indeed, there is an explicit dependence on the complete training data set T in the Ô¨Årst factor of the integrand. We
can therefore not use these data points to also approximate the integral. Put diÔ¨Äerently, the expected value in (4.5) should be computed conditionally on a Ô¨Åxed training data set T .
In fact, as we will discuss more thoroughly later, the typical behavior is that ùê∏train < ùê∏new (although this is not always the case). Hence, a method often performs worse on new, unseen data, than on training data. The performance on training data ùê∏train is therefore not a reliable estimate of ùê∏new.

ùê∏hold-out ‚âà ùê∏new: We can estimate ùê∏new from hold-out validation data
We could not use the training data directly to approximate the integral in (4.2) (that is, estimating ùê∏new by ùê∏train) since this would imply that we eÔ¨Äectively use the training data twice: Ô¨Årst, to train the model (ùë¶ in (4.4)) and second, to evaluate the error function (the sum in (4.4)). A remedy is to set aside some hold-out validation data {x ùëó, ùë¶ ùëó }ùëõùëó=ùë£1, which are not in T used for training, and then use this only for estimating the model performance as the hold-out validation error

ùê∏hold-out

1 ùëõùë£

‚àëùëõÔ∏Åùë£
ùëó =1

ùê∏ (ùë¶ (x ùëó ;

T ),

ùë¶ ùëó).

(4.6)

In this way, not all data will be used for training, but some data points will be saved and used only for computing ùê∏hold-out. This simple procedure for estimating ùê∏new is illustrated by Figure 4.1.
Be aware! When splitting your data, always do it randomly, for instance by shuÔ¨Ñing the data points before the training‚Äìvalidation split! Someone might‚Äîintentionally or unintentionally‚Äîhave sorted the dataset for you. If you do not split randomly, your binary classiÔ¨Åcation problem might end up with one class in your training data and the other class in your hold-out validation data . . .

Assuming that all (training and validation) data points are drawn from ùëù(x, ùë¶), it follows that ùê∏hold-out is an unbiased estimate of ùê∏new (meaning that if the entire procedure is repeated multiple times, each time with new data, the average value of ùê∏hold-out will be ùê∏new). That is reassuring, at least on a theoretical level, but it does not tell us how close ùê∏hold-out will be to ùê∏new in a single experiment. However, the variance of ùê∏hold-out decreases when the size of hold-out validation data ùëõùë£ increases; a small variance of ùê∏hold-out means that we can expect it to be close to ùê∏new. Thus, if we take the hold-out validation dataset big enough, ùê∏hold-out will be close to ùê∏new. However, setting aside a big validation dataset means that the dataset left for training becomes small. It is reasonable to assume that the more training data, the smaller ùê∏new (which we will discuss later in Section 4.3). That is bad news since achieving a small ùê∏new is our ultimate goal.
Sometimes there is a lot of available data. When we really have a lot of data, we can often aÔ¨Äord to set
aside a few percent to create a reasonably large hold-out validation dataset, without sacriÔ¨Åcing the size
of the training dataset too much. In such data-rich situations, the hold-out validation data approach is
suÔ¨Écient.

60

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Estimating ùê∏new

If the amount of available data is more limited, this becomes more of a problem. We are in practice
faced with the following dilemma: the better we want to know ùê∏new (more hold-out validation data gives less variance in ùê∏hold-out), the worse we have to make it (less training data increases ùê∏new). That is not very satisfying, and we have to look for an alternative to the hold-out validation data approach.

k-fold cross-validation: ùê∏ùëò-fold ‚âà ùê∏new without setting aside validation data
To avoid setting aside validation data, but still obtain an estimate of ùê∏new, one could suggest a two-step procedure of
(i) splitting the available data in one training and one hold-out validation set, train the model on the training data and compute ùê∏hold-out using hold-out validation data (as in Figure 4.1), and then
(ii) training the model again, this time using the entire dataset.
By such a procedure, we get an estimate of ùê∏new at the same time as a model trained on the entire dataset. That is not bad, but not perfect either. Why? To achieve small variance in the estimate, we have to put lots of data in the hold-out validation dataset in step (i). Unfortunately that means the model trained in (i) will possibly be quite diÔ¨Äerent from the one obtained in step (ii), and the estimate of ùê∏new concerns the model from step (i), not the possibly very diÔ¨Äerent model from step (ii). Hence, this will not give us a good estimate of ùê∏new. However, we can build on this idea to obtain the useful ùëò-fold cross-validation method.
We would like to use all available data to train a model, and at the same time have a good estimate of ùê∏new for that model. By ùëò-fold cross-validation we can approximately achieve this goal. The idea of ùëò-fold cross-validation is simply to repeat the hold-out validation dataset approach multiple times with a diÔ¨Äerent hold-out dataset each time, in the following way:
(i) split the dataset in ùëò batches of similar size (see Figure 4.2), and let ‚Ñì = 1
(ii) take batch ‚Ñì as the hold-out validation data, and the remaining batches as training data
(iii) train the model on the training data, and compute ùê∏h(o‚Ñìl)d-out as the average error on the hold-out validation data, as in (4.6)
(iv) if ‚Ñì < ùëò, set ‚Ñì ‚Üê ‚Ñì + 1 and return to (ii). If ‚Ñì = ùëò, compute the ùëò-fold cross-validation error

ùê∏ ùëò -fold

1 ùëò

‚àëùëòÔ∏Å
‚Ñì=1

ùê∏h(o‚Ñìl)d-out

(4.7)

(v) train the model again, this time using the entire dataset
This procedure is illustrated in Figure 4.2. With ùëò-fold cross-validation, we get a model which is trained on all data, as well as an approximation of
ùê∏new for that model, namely ùê∏ùëò-fold. Whereas ùê∏hold-out (Section 4.2) was an unbiased estimate of ùê∏new (to the cost of setting aside hold-out validation data), this is not the case for ùê∏ùëò-fold. However, with ùëò large enough, it turns out to often be a suÔ¨Éciently good approximation. Let us try to understand why ùëò-fold cross-validation works.
First, we have to distinguish between the Ô¨Ånal model, which is trained on all data in step (v), and the intermediate models which are trained on all except a 1/ùëò fraction of the data in step (iii). The key in ùëò-fold cross-validation is that if ùëò is large enough, the intermediate models are quite similar to the Ô¨Ånal model (since they are trained on almost the same dataset, only a fraction 1/ùëò of the data is missing). Furthermore, each intermediate ùê∏h(o‚Ñìl)d-out is an unbiased but high-variance estimate of ùê∏new for the corresponding ‚Ñìth intermediate model. Since all intermediate and the Ô¨Ånal model are similar, ùê∏ùëò-fold (4.7) is approximately vthaeriaanveceradgeecorefaùëòsehs iagnhd-vùê∏aùëòri-afonldcewielslttihmuastebsecoofmùê∏eneawbfeottretrheestÔ¨Åimnaaltemoofdùê∏enl.ewWthhaennthaeveinrategrimngedeisattiemùê∏ath(eo‚Ñìsl)d,-otuhte.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

61

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

batch 1

batch 2

All available data

batch 3

¬∑¬∑¬∑

batch ùëò

Validation data
‚Ñì=1

Training data
¬∑¬∑¬∑

‚Üí ùê∏h(o1l)d-out

‚Ñì=2

¬∑¬∑¬∑

‚Üí ùê∏h(o2l)d-out

...

‚Ñì=ùëò

¬∑¬∑¬∑

‚Üí ùê∏h(oùëòl)d-out

Training data

Validation data
average = ùê∏ùëò-fold

Figure 4.2: Illustration of ùëò-fold cross-validation. The data is split in ùëò batches of similar sizes. When looping over

‚Ñì = 1, 2, . . . , ùëò, batch ‚Ñì is held out as validation data, and the model is trained on the remaining ùëò ‚àí 1 data batches.

Each

time,

the

trained

model

is

used

to

compute

the

average

error

ùê∏

(‚Ñì) ùëò -fold

for

the

validation

data.

The

Ô¨Ånal

model

is

trained using all available data, and the estimate of ùê∏new for that model is ùê∏ùëò-fold, the average of all ùê∏ùëò(‚Ñì-f)old.

Be aware! For the same reason as with the hold-out validation data approach, it is important to always split the data randomly for cross-validation to work! A simple solution is to Ô¨Årst randomly permute the entire dataset, and thereafter split it into batches.
We usually talk about training (or learning) as a procedure that is executed once. However, in ùëò-fold cross-validation the training is repeated ùëò (or even ùëò + 1) times. A special case is ùëò = ùëõ which is also called leave-one-out cross-validation. For methods such as linear regression, the actual training (solving the normal equations) is usually done within milliseconds on modern computers, and doing it an extra ùëõ times might not be a problem in practice. If the training is computationally demanding (as for deep neural networks, for instance), it becomes a rather cumbersome procedure, and a choice like ùëò = 10 might be more practically feasible. If there is much data available, it is also an option to use the computationally less demanding hold-out validation approach.

Using a test dataset
A very important use of ùê∏ùëò-fold (or ùê∏hold-out) in practice is to choose between methods and select diÔ¨Äerent types of hyperparameters such that ùê∏ùëò-fold (or ùê∏hold-out) becomes as small as possible. Typical hyperparameters to choose in this way are ùëò in ùëò-NN, tree depths or regularization parameters. However, much like we cannot use the training data error ùê∏train to estimate the new data error ùê∏new, selecting models and hyperparameters based on ùê∏ùëò-fold (or ùê∏hold-out) will invalidate its use as an estimator of ùê∏new. Indeed, if hyperparameters are selected to minimize ùê∏ùëò-fold there is a risk of overÔ¨Åtting to the validation data, resulting in ùê∏ùëò-fold being an overly optimistic estimate of the actual new data error. If it is important to have a good estimate of the Ô¨Ånal ùê∏new, it is wise to Ô¨Årst set aside another hold-out dataset, which we refer to as a test set. This test set should be used only once (after selecting models and hyperparameters) to estimate ùê∏new for the Ô¨Ånal model.

62

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. The training error‚Äìgeneralization gap decomposition of ùê∏new

In problems where the training data is expensive, it is common to increase the training dataset using
more or less artiÔ¨Åcial techniques. Such techniques can be to duplicate the data and add noise to the
duplicated versions, to use simulated data, or to use data from a diÔ¨Äerent but related problem, as we
discuss in more depth in Chapter 11. With such techniques (which indeed can be very successful), the training data T is no longer drawn from ùëù(x, ùë¶). In the worst case (if the artiÔ¨Åcial training data is very poor), T might not provide any information about ùëù(x, ùë¶), and we can not really expect the model to learn anything useful. It can therefore be very useful to have a good estimate of ùê∏new if such techniques were used during training, but a reliable estimate of ùê∏new can only be achieved from data that we know are drawn from ùëù(x, ùë¶) (that is, collected under ‚Äúproduction-like‚Äù circumstances). If the training data is extended artiÔ¨Åcially, it is therefore extra important to set aside a test dataset before that extension is done.
The error function evaluated on the test data set could indeed be called ‚Äútest error‚Äù. To avoid confusion
we do however not use the term ‚Äútest error‚Äù since it commonly is used (ambiguously) as a name both for the error on the test dataset as well as another name for ùê∏new.

4.3 The training error‚Äìgeneralization gap decomposition of ùê∏new

Designing a method with small ùê∏new is a central goal in supervised machine learning, and cross-validation
helps in estimating ùê∏new. However, we can gain valuable insights and better understand the behavior of
supervised machine learning methods by further analyzing ùê∏new mathematically. To be able to reason
about ùê∏new, we have to introduce another abstraction level, namely the training-data averaged versions of ùê∏new and ùê∏train. To make the notation more explicit, we here write ùê∏new (T ) and ùê∏train (T ) to emphasize the fact that they both are conditional on a speciÔ¨Åc training dataset T . Let us now introduce

ùê∏¬Ønew ùê∏¬Øtrain

ET [ùê∏new (T )] , and ET [ùê∏train (T )] .

(4.8a) (4.8b)

Here, ET assuming

denotes the expected value with respect to the training dataset T that this consists of independent draws from ùëù(x, ùë¶). Thus ùê∏¬Ønew

= is

{xùëñ, ùë¶ùëñ }ùëñùëõ=1 (of a Ô¨Åxed size ùëõ), the average ùê∏new if we would

train the model multiple times on diÔ¨Äerent training datasets, all of size ùëõ, and similarly for ùê∏¬Øtrain. The

point of introducing these quantities is that it is easier to reason about the average behavior ùê∏¬Ønew and ùê∏¬Øtrain,

than about the errors ùê∏new and ùê∏train obtained when the model is trained on one speciÔ¨Åc training dataset T .

Even though we most often care about ùê∏new in the end (the training data is usually Ô¨Åxed), insights from

studying ùê∏¬Ønew are still useful.

Time to reÔ¨Çect 4.3: ùê∏new(T ) is the new data error when the model is trained on a speciÔ¨Åc training dataset T , whereas ùê∏¬Ønew is averaged over all possible training dataset. Considering the fact that in the procedure of ùëò-fold cross-validation the model is each time trained on a (at least slightly) diÔ¨Äerent training dataset, does ùê∏ùëò-fold actually estimates ùê∏new or is it rather ùê∏¬Ønew? And is that diÔ¨Äerent for diÔ¨Äerent values of ùëò? How could ùê∏¬Øtrain be estimated?

We have already discussed the fact that ùê∏train cannot be used in estimating ùê∏new. In fact, it usually holds that

ùê∏¬Øtrain < ùê∏¬Ønew,

(4.9)

Put in words, this means that on average, a method usually performs worse on new, unseen data, than on
training data. A method‚Äôs ability to perform well on unseen data after being trained is often referred to as its ability to generalize from training data. We consequently call the diÔ¨Äerence between ùê∏¬Ønew and ùê∏¬Øtrain the generalization gap, 3, as

generalization gap ùê∏¬Ønew ‚àí ùê∏¬Øtrain.

(4.10)

3With a more strict terminology we should perhaps refer to ùê∏¬Ønew ‚àí ùê∏¬Øtrain as the expected generalization gap, whereas ùê∏new ‚àí ùê∏train would be the conditional generalization gap. We will however use the same term for both.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

63

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

UnderÔ¨Åtting

OverÔ¨Åtting

ùê∏¬Ønew

Error Generalization gap

ùê∏¬Øtrain

Model complexity
Figure 4.3: Behavior of ùê∏¬Øtrain and ùê∏¬Ønew for many supervised machine learning methods, as a function of model complexity. We have not made a formal deÔ¨Ånition of complexity, but a rough proxy is the number of parameters that are learned from the data. The diÔ¨Äerence between the two curves is the generalization gap. The training error ùê∏¬Øtrain decreases as the model complexity increases, whereas the new data error ùê∏¬Ønew typically has a U-shape. If the model is so complex that ùê∏¬Ønew is larger than it had been with a less complex model, the term overÔ¨Åt is commonly used. Somewhat less commonly is the term underÔ¨Åtting used for the opposite situation. The level of model complexity which gives the minimum ùê∏¬Ønew (at the dotted line) could be called a balanced Ô¨Åt. When we, for example, use cross-validation to select hyperparameters (that is, tuning the model complexity), we are searching for a balanced Ô¨Åt.

The generalization gap is the diÔ¨Äerence between the expected performance on training data and the
expected performance ‚Äòin production‚Äô on new, previously unseen data. With the decomposition of ùê∏¬Ønew into

ùê∏¬Ønew = ùê∏¬Øtrain + generalization gap,

(4.11)

we also have an opening for digging deeper and trying to understand what aÔ¨Äects ùê∏¬Ønew in practice. We will refer to (4.11) as the training error‚Äìgeneralization gap decomposition of ùê∏¬Ønew.

What aÔ¨Äects the generalization gap
The generalization gap depends on the method and the problem. Concerning the method, one can typically say that the more a method adapts to training data, the larger the generalization gap. A theoretical framework for how much a method adapts to training data is given by the so-called Vapnik‚ÄìChervonenkis (VC) dimension. From the VC dimension framework, probabilistic bounds on the generalization gap can be derived, but those bounds are unfortunately rather conservative, and we will not pursue that approach any further. Instead, we only use the vague terms model complexity or model Ô¨Çexibility (we use them interchangeably), by which we mean the ability of a method to adapt to patterns in the training data. A model with high complexity (such as a fully connected deep neural network, deep trees and ùëò-NN with small ùëò) can describe complicated input‚Äìoutput relationships, whereas a model with low complexity (such as logistic regression) is less Ô¨Çexible in what functions it can describe. For parametric models, the model complexity is somewhat related to the number of learnable parameters, but is also aÔ¨Äected by regularization techniques. As we will come back to later, the idea of model complexity is an oversimpliÔ¨Åcation and does not capture the full nature of various supervised machine learning methods, but it nevertheless carries some useful intuition.
Typically, higher model complexity implies a larger generalization gap. Furthermore, ùê∏¬Øtrain decreases as the model complexity increases, whereas ùê∏¬Ønew typically attains a minimum for some intermediate

64

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. The training error‚Äìgeneralization gap decomposition of ùê∏new
model complexity value: too small and too high model complexity both raises ùê∏¬Ønew. This is illustrated in Figure 4.3. A too high model complexity, meaning that ùê∏¬Ønew is higher than it had been with a less complex model, is called overÔ¨Åtting. The other situation, when the model complexity is too low, is sometimes called underÔ¨Åtting. In a consistent terminology, the point where ùê∏¬Ønew attains it minimum could be referred to as a balanced Ô¨Åt. Since the goal is to minimize ùê∏¬Ønew, we are interested in Ô¨Ånding this sweet spot. We also illustrate this by Example 4.1.
Note that we discuss the usual behavior of ùê∏¬Ønew, ùê∏¬Øtrain and the generalization gap. We use the term ‚Äòusual‚Äô because there are so many supervised machine learning methods and problems that it is almost impossible to make any claim that is always true for all possible situations, and pathological counter-examples also exist. One should also keep in mind that claims about ùê∏¬Øtrain and ùê∏¬Ønew are about the average behavior when the model is retrained and evaluated on (hypothetical) new training data sets, see Example 4.1.
Example 4.1: The training error‚Äìgeneralization gap decomposition for ùëò-NN
We consider a simulated binary classiÔ¨Åcation example with a two-dimensional input x. Contrary to all real world machine learning problems, in a simulated example like this we know ùëù(x, ùë¶). In this example, ùëù(x) is a uniform distribution on the square [‚àí1, 1]2, and ùëù(ùë¶ | x) is deÔ¨Åned as follows: all points above the dotted curve in Figure 4.4 are blue with probability 0.8, and points below the curve are red with probability 0.8. (The optimal classiÔ¨Åer, in terms of minimal ùê∏new, would have the dotted line as its decision boundary and achieve ùê∏new = 0.2.)
1

ùë•2

0

Fig.

‚àí1

‚àí1

0

1

4.4

ùë•1

We have ùëõ = 200 in the training data, and learn three classiÔ¨Åers: ùëò-NN with ùëò = 70, ùëò = 20 and ùëò = 2,

respectively. In model complexity sense, ùëò = 70 gives the least Ô¨Çexible model, and ùëò = 2 the most Ô¨Çexible

model. We plot their decision boundaries, together with the training data, in Figure 4.5.

ùëò-NN, ùëò = 70
1

ùëò-NN, ùëò = 20
1

ùëò-NN, ùëò = 2
1

ùë•2

ùë•2

ùë•2

0

0

0

Fig.

‚àí1 ‚àí1

0

‚àí1

1

‚àí1

0

‚àí1

1

‚àí1

0

1

4.5

ùë•1

ùë•1

ùë•1

We see in Figure 4.5 that ùëò = 2 (right) adapts too much to the data. With ùëò = 70 (left), on the other hand, the
model is rigid enough not to adapt to the noise, but appears to possibly be too inÔ¨Çexible to adapt well to the
true dotted line in Figure 4.4. We can compute ùê∏train by counting the fraction of misclassiÔ¨Åed training data points in Figure 4.5. From left
to right, we get ùê∏train = 0.27, 0.24, 0.22. Since this is a simulated example, we can also access ùê∏new (or rather estimate it numerically by simulating a lot of test data), and from left to right we get ùê∏new = 0.26, 0.23, 0.33. This pattern resembles Figure 4.3, except for the fact that ùê∏new is actually smaller than ùê∏train for some values of ùëò. However, this is not contradicting the theory. What we have discussed in the main text is the average ùê∏¬Ønew and ùê∏¬Øtrain, not the situation with ùê∏new and ùê∏train for one particular set of training data. To study ùê∏¬Ønew and ùê∏¬Øtrain, we therefore repeat this entire experiment 100 times, and compute the average over those 100 experiments:

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

65

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

ùê∏¬Øtrain ùê∏¬Ønew

ùëò-NN with ùëò = 70
0.24 0.25

ùëò-NN with ùëò = 20
0.22 0.23

ùëò-NN with ùëò = 2
0.17 0.30

This table follows Figure 4.3 well: The generalization gap (diÔ¨Äerence between ùê∏¬Ønew and ùê∏¬Øtrain) is positive and increases with model complexity (decreasing ùëò in ùëò-NN), whereas ùê∏¬Øtrain decreases with model complexity. Among these values for ùëò, ùê∏¬Ønew has its minimum for ùëò = 20. This suggests that ùëò-NN with ùëò = 2 suÔ¨Äers from overÔ¨Åtting for this problem, whereas ùëò = 70 is a case of underÔ¨Åtting.

We have so far been concerned about the relationship between the generalization gap and the model
complexity. Another very important aspect is the size of the training dataset, ùëõ. We can in general expect that the more training data, the smaller the generalization gap. On the other hand, ùê∏¬Øtrain typically increases
as ùëõ increases, since most models are unable to Ô¨Åt all training data points well if there are too many of them. A typical behavior of ùê∏¬Øtrain and ùê∏¬Ønew is sketched in Figure 4.6.

ùê∏¬Ønew ùê∏¬Øtrain

ùê∏¬Ønew

Error Error

ùê∏¬Øtrain

Size of training data ùëõ

Size of training data ùëõ

(a) Simple model

(b) Complex model

Figure 4.6: Typical relationship between ùê∏¬Ønew, ùê∏¬Øtrain and the number of data points ùëõ in the training dataset for a simple model (low model Ô¨Çexibility, left) and a complex model (high model Ô¨Çexibility, right). The generalization gap (diÔ¨Äerence between ùê∏¬Ønew and ùê∏¬Øtrain) decreases, at the same time as ùê∏¬Øtrain increases. Typically, a more complex model (right panel) will for large enough ùëõ attain a smaller ùê∏¬Ønew than a simpler model (left panel) would on the same problem (the Ô¨Ågures should be thought of as having the same scales on the axes). However, the generalization
gap is typically larger for a more complex model, in particular when the training dataset is small.

Reducing ùê∏new in practice
Our overall goal is to achieve a small error ‚Äúin production‚Äù, that is, a small ùê∏new. To achieve that, according to the decomposition ùê∏new = ùê∏train + generalization gap, we need to have ùê∏train as well as the generalization gap small. Let us draw two conclusions from what we have seen so far.
‚Ä¢ The new data error ùê∏new will on average not be smaller than the training error ùê∏train. Thus, if ùê∏train is much bigger than the ùê∏new you need for your model to be successful for the application at hand, you do not even need to waste time on implementing cross-validation for estimating ùê∏new. Instead, you should re-think the problem and which method you are using.
‚Ä¢ The generalization gap and ùê∏new typically decreases as ùëõ increases. Thus, if possible, increasing the size of the training data may help a lot with reducing ùê∏new.
Making the model more Ô¨Çexible decreases ùê∏train, but often increases the generalization gap. Making the model less Ô¨Çexible, on the other hand, typically decreases the generalization gap but increases ùê∏train. The

66

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. The training error‚Äìgeneralization gap decomposition of ùê∏new
optimal trade-oÔ¨Ä, in terms of small ùê∏new, is often obtained when neither the generalization gap nor the training error ùê∏train is zero. Thus, by monitoring ùê∏train and estimating ùê∏new with cross-validation we also get the following advice:
‚Ä¢ If ùê∏hold-out ‚âà ùê∏train (small generalization gap; possibly underÔ¨Åtting), it might be beneÔ¨Åcial to increase the model Ô¨Çexibility by loosening the regularization, increasing the model order (more parameters to learn), etc.
‚Ä¢ If ùê∏train is close to zero and ùê∏hold-out is not (possibly overÔ¨Åtting), it might be beneÔ¨Åcial to decrease the model Ô¨Çexibility by tightening the regularization, decreasing the order (fewer parameters to learn), etc.
Shortcomings of the model complexity scale
When having only one hyperparameter to choose, the situation sketched in Figure 4.3 is often a relevant picture. However, when having multiple hyperparameters (or even competing methods) to choose, it is important to realize that the one-dimensional model complexity scale in Figure 4.3 does not make justice for the space of all possible choices. For a given problem, one method can have a smaller generalization gap than another method without having a larger training error. Some methods are simply better for certain problems. The one-dimensional complexity scale can be particularly misleading for intricate deep learning models, but as we illustrate in Example 4.2 it is not even suÔ¨Écient for the relatively simple problem of jointly choosing the degree of polynomial regression (higher degree means more Ô¨Çexibility) and regularization parameter (more regularization means less Ô¨Çexibility).

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

67

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

Example 4.2: Training error and generalization gap for a regression problem

To illustrate how the training error and generalization gap can behave, we consider a simulated problem so that we can compute ùê∏new. We let ùëõ = 10 data points be generated as ùë• ‚àº U [‚àí5, 10], ùë¶ = min(0.1ùë•2, 3) + ùúÄ, and ùúÄ ‚àº N (0, 1), and consider the following regression methods:
‚Ä¢ Linear regression with ùêø2 regularization ‚Ä¢ Linear regression with a quadratic polynomial and ùêø2 regularization ‚Ä¢ Linear regression with a third order polynomial and ùêø2 regularization
‚Ä¢ Regression tree
‚Ä¢ A random forest (Chapter 7) with 10 regression trees
For each of these methods, we try a few diÔ¨Äerent values of the hyperparameters (regularization parameter and tree depth, respectively), compute ùê∏¬Øtrain and the generalization gap.

5 4

1.5

3

5 4

3

1

Linear regression, ùêø2 regularization Linear regression, 2rd order polynomial, ùêø2 regularization Linear regression, 3rd order polynomial, ùêø2 regularization Regression tree Random forest
2
2

Generalization gap

0.5

0.1 1

1 1 0.11

10

100

10 0.1 1 100

1 000

10

1 000

0

Fig.

0

0.5

1

1.5

2

4.7

ùê∏¬Øtrain

For each method the hyperparameter that minimizes ùê∏¬Ønew is the value which is closest (in the 1-norm sense) to the origin, since ùê∏¬Ønew = ùê∏¬Øtrain + generalization gap. Having decided on a certain model, and only having one hyperparameter left to choose, corresponds well to the situation in Figure 4.3.
However, when we compare the diÔ¨Äerent methods, a more complicated situation is revealed than what is described by the one-dimensional model complexity scale. Compare, for example, the second (red) to the third order polynomial (green) linear regression: for some values of the regularization parameter, the training error decreases without increasing the generalization gap. Similarly, the generalization gap is smaller, while the training error remains the same, for the random forest (purple) than for the tree (black) for a maximum tree depth of 2. The main takeaway from this is that these relationships are quite intricate, problem-dependent and impossible to describe using the simpliÔ¨Åed picture in Figure 4.3. However, as we shall see, the picture becomes somewhat more clear when we next will introduce another decomposition of ùê∏¬Ønew, namely the bias-variance decomposition, in particular in Example 4.4.

In any real problem we can not make a plot such as in Example 4.2. This is only possible for simulated examples where we have full control over the data generating process. In practice we instead have to make a decision based on the much more limited information available. It is good to choose models that are known to work well for a speciÔ¨Åc type of data and use experience from similar problems. We can also use cross-validation for selecting between diÔ¨Äerent models and choosing hyperparameters. Despite the simpliÔ¨Åed picture, the intuition about under- and overÔ¨Åtting from Figure 4.3 can still be very helpful when deciding on what method or hyperparameter value to explore next with cross-validation.

68

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. The bias-variance decomposition of ùê∏new

4.4 The bias-variance decomposition of ùê∏new

We will now introduce another decomposition of ùê∏¬Ønew into a (squared) bias and a variance term, as well as an unavoidable component of irreducible noise. This decomposition is somewhat more abstract than the training-generalization gap, but provides some additional insights into ùê∏new and how diÔ¨Äerent models behave.
Let us Ô¨Årst make a short reminder of the general concepts of bias and variance. Consider an experiment with an unknown constant ùëß0, which we would like to estimate. To our help for estimating ùëß0 we have a random variable ùëß. Think, for example, of ùëß0 as being the (true) position of an object, and ùëß of being noisy GPS measurements of that position. Since ùëß is a random variable, it has some mean E[ùëß] which we denote by ùëß¬Ø. We now deÔ¨Åne

Bias: ùëß¬Ø ‚àí ùëß0 Variance: E (ùëß ‚àí ùëß¬Ø)2 = E ùëß2 ‚àí ùëß¬Ø2.

(4.12a) (4.12b)

The variance describes how much the experiment varies each time we perform it (the amount of noise in the GPS measurements), whereas the bias describes the systematic error in ùëß that remains no matter how many times we repeat the experiment (a possible shift or oÔ¨Äset in the GPS measurements). If we consider the expected squared error between ùëß and ùëß0 as a metric of how good the estimator ùëß is, we can re-write it in terms of the variance and the squared bias,

E (ùëß ‚àí ùëß0)2 = E (ùëß ‚àí ùëß¬Ø) + (ùëß¬Ø ‚àí ùëß0) 2 = = E (ùëß ‚àí ùëß¬Ø)2 +2 (E[ùëß] ‚àí ùëß¬Ø) (ùëß¬Ø ‚àí ùëß0) + (ùëß¬Ø ‚àí ùëß0)2 .

(4.13)

Variance

0

bias2

In words, the average squared error between ùëß and ùëß0 is the sum of the squared bias and the variance. The main point here is that to obtain a small expected squared error, we have to consider both the bias and the variance. Only a small bias or little variance in the estimator is not enough, but both aspects are important.
We will now apply the bias and variance concept to our supervised machine learning setting. For
mathematical simplicity we will consider the regression problem with the squared error function. The intuition, however, carries over also to the classiÔ¨Åcation problem. In this setting, ùëß0 corresponds to the true relationship between inputs and output, and the random variable ùëß corresponds to the model learned
from training data. Note that, since the training data collection includes randomness, the model learned
from it will also be random. We Ô¨Årst make the assumption that the true relationship between input x and output ùë¶ can be described
as some (possibly very complicated) function ùëì0(x) plus independent noise ùúÄ,

ùë¶ = ùëì0(x) + ùúÄ, with E[ùúÄ] = 0 and var(ùúÄ) = ùúé2.

(4.14)

In our notation, ùë¶(x; T ) represents the model when it is trained on training data T . This is our random variable, corresponding to ùëß above. We now also introduce the average trained model, corresponding to ùëß¬Ø,

ùëì¬Ø(x) ET [ùë¶(x; T )] .

(4.15)

As before, ET denotes the expected value over ùëõ training data points drawn from ùëù(x, ùë¶). Thus, ùëì¬Ø(x) is
the (hypothetical) average model we would achieve, if we could re-train the model an inÔ¨Ånite number of
times on diÔ¨Äerent training datasets, each one of size ùëõ, and compute the average. Remember that the deÔ¨Ånition of ùê∏¬Ønew (for regression with squared error) is

ùê∏¬Ønew = ET E‚òÖ (ùë¶(x‚òÖ; T ) ‚àí ùë¶‚òÖ)2 .

(4.16)

We can change the order of integration and write (4.16) as

ùê∏¬Ønew = E‚òÖ ET (ùë¶(x‚òÖ; T ) ‚àí ùëì0 (x‚òÖ) ‚àí ùúÄ)2

(4.17)

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

69

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

With a slight extension of (4.13) to also include the zero-mean noise term ùúÄ (which is independent of ùë¶(x‚òÖ; T )), we can rewrite the expression inside the expected value E‚òÖ in (4.17) as

ET ùë¶(x‚òÖ; T ) ‚àí ùëì0 (x‚òÖ) ‚àíùúÄ 2 = ùëì¬Ø(x‚òÖ) ‚àí ùëì0 (x‚òÖ) 2 + ET ùë¶(x‚òÖ; T ) ‚àí ùëì¬Ø(x‚òÖ) 2 + ùúÄ2.

(4.18)

‚Äúùëß‚Äù

‚Äúùëß0‚Äù

This is (4.13) applied to supervised machine learning. In ùê∏¬Ønew, which we are interested in decomposing, we also have the expectation over new data points E‚òÖ. By incorporating also that expected value in the expression, we can decompose ùê∏¬Ønew as

ùê∏¬Ønew = E‚òÖ

ùëì¬Ø(x‚òÖ) ‚àí ùëì0 (x‚òÖ) 2 + E‚òÖ ET
Bias2

ùë¶(x‚òÖ; T ) ‚àí ùëì¬Ø(x‚òÖ) 2
Variance

+ ùúé2 .
Irreducible error

(4.19)

The squared bias term E‚òÖ ùëì¬Ø(x‚òÖ) ‚àí ùëì0(x‚òÖ) 2 now describes how much the average trained model ùëì¬Ø(x‚òÖ)
diÔ¨Äers from the true ùëì0(x‚òÖ), averaged over all possible test data points x‚òÖ. In a similar fashion the variance term E‚òÖ ET [ ùë¶(x‚òÖ; T ) ‚àí ùëì¬Ø(x‚òÖ) 2] describes how much ùë¶(x; T ) varies each time the model is trained
on a diÔ¨Äerent training dataset. For the bias term to be small, the model has to be Ô¨Çexible enough such that ùëì¬Ø(x) can be close to ùëì0(x), at least in regions where ùëù(x) is large. If the variance term is small, the model is not very sensitive to exactly which data points that happened to be in the training data, and vice versa. The irreducible error ùúé2 is simply an eÔ¨Äect of the assumption (4.14)‚Äîit is not possible to predict ùúÄ since it is a random error independent of all other variables. There is not so much more to say about the irreducible error, so we will focus on the bias and variance terms.

What aÔ¨Äects the bias and variance
We have not properly deÔ¨Åned model complexity, but we can actually use the bias and variance concept to give it a more concrete meaning: A high model complexity means low bias and high variance, and a low model complexity means high bias and low variance, as illustrated by Figure 4.8.
This resonates well with the intuition. The more Ô¨Çexible a model is, the more it will adapt to the training data T ‚Äînot only to the interesting patterns, but also to the actual data points and noise that happened to be in T . That is exactly what is described by the variance term. On the other hand, a model with low Ô¨Çexibility can be too rigid to capture the true relationship ùëì0(x) between inputs and outputs well. This eÔ¨Äect is described by the squared bias term.
Figure 4.8 can be compared to Figure 4.3, which builds on the training error‚Äìgeneralization gap decomposition of ùê∏¬Ønew instead. From Figure 4.8 we can talk about the challenge of Ô¨Ånding the right model complexity level also as the bias-variance tradeoÔ¨Ä. We give an example of this in Example 4.3.

70

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

UnderÔ¨Åtting Bias2

. The bias-variance decomposition of ùê∏new OverÔ¨Åtting
ùê∏¬Ønew Irreducible error
Variance

Error

Model complexity
Figure 4.8: The bias-variance decomposition of ùê∏¬Ønew, instead of the training error‚Äìgeneralization gap decomposition in Figure 4.3. Low model complexity means high bias. The more complicated the model is, the more it adapts to (noise in) the training data, and the higher the variance. The irreducible error is independent of the particular choice of model and is therefore constant. The problem of achieving a small ùê∏new by selecting a suitable model complexity level is often called the bias‚Äìvariance tradeoÔ¨Ä.

Error Error

Irreducible error

ùê∏¬Ønew

Variance

Bias2

Variance

ùê∏¬Ønew
Irreducible error

Bias2

Size of training data ùëõ

Size of training data ùëõ

(a) Simple model

(b) Complex model

Figure 4.9: The typical relationship between bias, variance and the size ùëõ of the training dataset. The bias is (approximately) constant, whereas the variance decreases as the size of the training dataset increases. This Ô¨Ågure can be compared with Figure 4.6.

The squared bias term is more a property of the model than of the training dataset, and we may think4 of
the bias term as independent of the number of data points ùëõ in the training data. The variance term, on the other hand, varies highly with ùëõ. As we know, ùê∏¬Ønew typically decreases as ùëõ increases, and the reduction in ùê∏¬Ønew is largely because of the reduction of the variance. Intuitively, the more data, the more information we have about the parameters, resulting in a smaller variance. This is summarized by Figure 4.9, which
can be compared to Figure 4.6.

4This is not exactly true. The average model ùëì¬Ø might indeed be diÔ¨Äerent if all training datasets (which we average over) contain ùëõ = 2 or ùëõ = 100 000 data points, but we neglect that eÔ¨Äect here.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

71

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

Example 4.3: The bias‚Äìvariance tradeoÔ¨Ä for ùêø2 regularized linear regression

Let us consider a simulated regression example. We let ùëù(ùë•, ùë¶) follow from ùë• ‚àº U [0, 1] and

ùë¶ = 5 ‚àí 2ùë• + ùë•3 + ùúÄ, ùúÄ ‚àº N (0, 1) .

(4.20)

We let the training data consist of only ùëõ = 10 data points. We now try to model the data using linear regression with a 4th order polynomial

ùë¶ = ùõΩ0 + ùõΩ1ùë• + ùõΩ2ùë•2 + ùõΩ3ùë•3 + ùõΩ4ùë•4 + ùúÄ.

(4.21)

Since (4.20) is a special case of (4.21) and the squared error loss corresponds to Gaussian noise, we actually
have zero bias for this model if we train it using squared error loss. However, learning 5 parameters from
only 10 data points leads to very high variance, so we decide to train the model with squared error loss and ùêø2 regularization, which will decrease the variance (but increase the bias). The more regularization (bigger ùúÜ), the more bias and less variance.
Since this is a simulated example, we can repeat the experiment multiple times and estimate the bias and
variance terms (since we can simulate as much training and test data as needed). We plot them in Figure
4.10 using the same style as Figures 4.3 and 4.8 (note the reversed x-axis: a smaller regularization parameter corresponds to a higher model complexity). For this problem the optimal value of ùúÜ would have been about 0.7 since ùê∏¬Ønew attains its minimum there. Finding this optimal ùúÜ is a typical example of the bias‚Äìvariance tradeoÔ¨Ä.

6

Error

4

2
Bias2

0

Fig.

101

4.10

ùê∏¬Ønew

ùê∏¬Øtrain

Irreducible error

100

10‚àí1

10‚àí2

Regularization parameter

Variance
10‚àí3

Connections between bias, variance and the generalization gap

The bias and variance are theoretically well deÔ¨Åned properties, but often intangible in practice since they are deÔ¨Åned in terms of ùëù(x, ùë¶). In practice, we mostly have an estimate of the generalization gap (for example as ùê∏hold-out ‚àí ùê∏train), whereas the bias and variance requires additional tools for being estimated5. It is therefore interesting to explore what ùê∏train and the generalization gap says about the bias and variance.
Consider the regression problem. Assume that the squared error is used both as error function and loss
function and that the global minimum is found during training. We can then write

ùúé2 + bias2 = E‚òÖ

( ùëì¬Ø(x‚òÖ) ‚àí ùë¶‚òÖ)2

‚âà

1 ùëõ

‚àëùëõÔ∏Å ( ùëì¬Ø(xùëñ) ‚àí ùë¶ùëñ)2
ùëñ=1

‚â•

1 ùëõ

‚àëùëõÔ∏Å (ùë¶(xùëñ; T )
ùëñ=1

‚àí

ùë¶ùëñ)2

=

ùê∏ train .

(4.22)

5The bias and variance can, to some extent, be estimated using the bootstrap, as we will introduce in Chapter 7.

72

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. The bias-variance decomposition of ùê∏new

In the approximate equality, we approximate the expected value by a sample average using the training data points6. If furthermore assuming that ùë¶ possibly can be ùëì¬Ø, together with the above assumption of having
the squared error as loss function and the learning of ùë¶ always Ô¨Ånding the global minimum, we have the inequality in the next step. Remembering that ùê∏¬Ønew = ùúé2 + bias2 + variance, and allowing ourselves to write ùê∏¬Ønew ‚àí ùê∏train = generalization gap, we have

generalization gap variance, ùê∏train bias2 + ùúé2.

(4.23a) (4.23b)

The assumptions in this derivation are not always met in practice, but it at least gives us some rough idea. As we discussed previously, the choice of method is crucial for what ùê∏new is obtained. Again the
one-dimensional scale in Figure 4.8 and the notion of a bias-variance tradeoÔ¨Ä is a simpliÔ¨Åed picture; decreased bias does not always lead to increased variance, and vice versa. However, in contrast to the decomposition of ùê∏new into training error and generalization gap, the bias and variance decomposition can shed some more light over why ùê∏new decreases for diÔ¨Äerent methods: sometimes, the superiority of one method over another can be attributed to either a lower bias or a lower variance.
A simple (and useless) way to increase the variance without decreasing the bias in linear regression, is to Ô¨Årst learn the parameters using the normal equations and thereafter add zero-mean random noise to them. The extra noise does not aÔ¨Äect the bias, since the noise has zero mean and hence leaves the average model ùëì¬Ø unchanged, but the variance increases. (This eÔ¨Äects also the training error and the generalization gap, but in a less clear way.) This way of training linear regression would be pointless in practice since it increases ùê∏new, but it illustrates the fact that increased variance does not automatically leads to decreased bias.
A much more useful way of dealing with bias and variance is the meta-method called bagging, discussed in Chapter 7. It makes use of several copies (an ensemble) of a base model, each trained on a slightly diÔ¨Äerent version of the training dataset. Since bagging averages over many base models, it reduces the variance, but the bias remains essentially unchanged. Hence, by using bagging instead of the base model, the variance is decreased without signiÔ¨Åcantly increasing the bias, often resulting in an overall decrease in ùê∏new.
To conclude, the world is more complex than just the one-dimensional model complexity scale used in Figure 4.3 and 4.8, which we illustrate by Example 4.4.

Time to reÔ¨Çect 4.4: Can you modify linear regression such that the bias increases, without decreasing the variance?

6Since neither ùëì¬Ø(ùë•‚òÖ) nor ùë¶‚òÖ depends on the training data {xùëñ, ùë¶ùëñ }ùëñùëõ=1, we can use {xùëñ, ùë¶ùëñ }ùëñùëõ=1for approximating the integral.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

73

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

Example 4.4: Bias and variance for a regression problem
We consider the exact same setting as in Example 4.2, but decompose ùê∏¬Ønew into bias and variance instead. This gives us Figure Ô¨Åg:trainingerror:ex2:1.

ùë¶ Variance

5
1
4
0.8 3

Linear regression, 1st order polynomial, ùêø2 regularization Linear regression, 2nd order polynomial, ùêø2 regularization Linear regression, 3rd order polynomial, ùêø2 regularization Regression tree Random forest

Fig. 4.11

2 0.6 5
4
3

0.4

2 0.1

1 0.1

1

11

0.2

10 100

10 0.1

100

1

1 000

10

1 000

100

0

0

0.2 0.4 0.6 0.8

1

1.2 1.4 1.6 1.8

2

2.2 2.4 2.6

Bias2

There are clear resemblances to Example 4.2, as expected from (4.23). The eÔ¨Äect of bagging (used in the
random forest; see Chapter 7) is, however, more clear, namely that it reduces the variance compared to the
regression tree with no noteworthy increase in bias.
For another illustration of what bias and variance means, we illustrate some of these cases in more detail in Figure 4.12. First we plot some of the linear regression models. The dashed red line is the true ùëì0 (x), the dotted blue lines are diÔ¨Äerent models ùë¶(x‚òÖ; T ) learned from diÔ¨Äerent training datasets T , and the solid blue line their mean ùëì¬Ø(x). In these Ô¨Ågures, bias is the diÔ¨Äerence between the dashed red and solid blue lines, whereas variance is the spread of the dotted blue lines around the solid blue. The variance appears to be
roughly the same for all three models, perhaps somewhat smaller for the Ô¨Årst order polynomial, whereas the
bias is clearly smaller for the higher order polynomials. This can be compared to Figure 4.11.

Linear regression, ùõæ = 0.1

2nd order polynomial, ùõæ = 0.1 3rd order polynomial, ùõæ = 0.1

4

4

2

2

0

0

‚àí5

0

5

10

‚àí5

0

5

10

2nd order polynomial, ùõæ = 1 000 Regression tree, max depth 5

4 2 0

‚àí5

0

5

10

Random forest, max depth 5

4

4

4

2

2

2

0

0

0

Fig. 4.12

‚àí5

0

5

ùë•

10

‚àí5

0

5

ùë•

10

‚àí5

0

5

ùë•

10

Comparing the second order polynomial with little (ùõæ = 0.1) and heavy (ùõæ = 1 000) regularization, it is clear that regularization reduces variance, but also increases bias. Furthermore, the random forest has a smaller variance than the regression tree, but without any noticeable change in the solid line ùëì¬Ø(x), and hence no change in bias.

74

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Additional tools for evaluating binary classiÔ¨Åers

4.5 Additional tools for evaluating binary classiÔ¨Åers
For classiÔ¨Åcation, and in particular binary classiÔ¨Åcation, there exists a wide range of additional tools for inspecting the performance beyond the misclassiÔ¨Åcation rate. For simplicity we consider the binary problem and use a hold-out validation dataset approach, but some of the ideas can be extended to the multiclass problem as well as to ùëò-fold cross-validation.
Some of them are in particular useful for imbalanced and/or asymmetric problems, that we will discuss later in this section. Remember that we in binary classiÔ¨Åcation have either ùë¶ = {‚àí1, 1}. If a binary classiÔ¨Åer is used to detect the presence of something, such as a disease, an object on the radar, etc the convention is that ùë¶ = 1 (positive) denotes presence, and ùë¶ = ‚àí1 (negative) denotes absence. This convention is the background for a lot of the terminology we will introduce now.

The confusion matrix and the ROC curve

If we learn a binary classiÔ¨Åer and evaluate it on a hold-out validation dataset, a simple yet useful way
to inspect the performance more than just computing ùê∏hold-out is a confusion matrix. By separating the validation data in four groups depending on ùë¶ (the actual output) and ùë¶(x) (the output predicted by the classiÔ¨Åer), we can make the confusion matrix,

ùë¶ = ‚àí1

ùë¶=1

total

ùë¶(x) = ‚àí1 True neg (TN) False neg (FN) N*

ùë¶(x) = 1 False pos (FP) True pos (TP) P*

total

N

P

ùëõ

Of course, TN, FN, FP, TP (and also N*, P*, N, P and ùëõ) should be replaced by the actual numbers, as in Example 4.5. Note that P (N) denote the total number of positive (negative) examples in the data set, whereas P* (N*) denote the total number of positive (negative) predictions made by the model. The confusion matrix provides a quick and informative overview of the characteristics of a classiÔ¨Åer. For asymmetric problems, that we will soon introduce, it is important to distinguish between false positive (FP, also called type I error) and false negative (FN, also called type II error). Ideally they should both be 0, but in practice there is usually a tradeoÔ¨Ä between these two errors and the confusion matrix is a helpful tool in visualizing them both. That tradeoÔ¨Ä between false negatives and false positives can often be done by tuning a decision threshold ùëü that is present in many binary classiÔ¨Åers (3.36).
There is also a wide body of terminology related to the confusion matrix, which is summarized in Table 4.1. Some particularly common terms are the

recall

=

TP P

=

TP TP + FN

and the

precision

=

TP P*

=

TP TP + FP

.

Recall describes how large proportion among the positive data points that are correctly predicted as positive. A high recall (close to 1) is good, and a low recall (close to 0) indicates a problem with many false negatives. Precision describes what the ratio of true positive points are among the ones predicted as positive. A high precision (close to 1) is good, and a low precision (close to 0) indicates a problem with many false positives.
Many classiÔ¨Åers contains a threshold ùëü (3.36). If we want to compare diÔ¨Äerent classiÔ¨Åers for a certain problem without specifying a certain decision threshold ùëü, the ROC curve can be useful. The abbreviation ROC means ‚Äùreceiver operating characteristics‚Äù, and is due to its history from communications theory.
To plot a ROC curve, the recall/true positive rate (TP/P, a large value is good) is drawn against the false positive rate (FP/N, a small value is good) for all values of ùëü ‚àà [0, 1]. The curve typically looks as shown in Figure 4.13a. A ROC curve for a perfect classiÔ¨Åer (always predicting the correct value for all ùëü ‚àà (0, 1)) touches the upper left corner, whereas a classiÔ¨Åer which only assigns random guesses7 gives a straight diagonal line.
7That is, predicts ùë¶ = ‚àí1 with probability ùëü.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

75

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

Ratio

Name

FP/N TN/N TP/P FN/P TP/P* FP/P* TN/N* FN/N* P/ùëõ (FN+FP)/ùëõ (TN+TP)/ùëõ 2TP/(P*+P) (1+ùõΩ2)TP/((1+ùõΩ2)TP+ùõΩ2FN+FP)

False positive rate, Fall-out, Probability of false alarm True negative rate, SpeciÔ¨Åcity, Selectivity True positive rate, Sensitivity, Power, Recall, Probability of detection False negative rate, Miss rate Positive predictive value, Precision False discovery rate Negative predictive value False omission rate Prevalence MisclassiÔ¨Åcation rate Accuracy, 1 ‚àí misclassiÔ¨Åcation rate F1 score FùõΩ score

Table 4.1: Some common terms related to the quantities (TN, FN, FP, TP) in the confusion matrix. The terms written in italics are discussed in the text.

True positive rate Precision

1
decreasing ùëü ‚Üí

1
decreasing ùëü ‚Üí

0.5

0.5

ùëÉ/ùëõ

Typical example Perfect classiÔ¨Åer Random guess

Typical example Perfect classiÔ¨Åer Random guess

0

0

0

0.5

1

0

0.5

1

False positive rate

Recall

(a) The ROC curve

(b) The precision-recall curve

Figure 4.13: The ROC (left) and the precision-recall (right) curve. Both plots summarizes the performance of a classiÔ¨Åer for all decision thresholds ùëü (see (3.36)), but the ROC curve is most relevant for balanced problems,
whereas the precision-recall curve is more informative for imbalanced problems.

A compact summary of the ROC curve is the area under the ROC curve, ROC-AUC. From Figure 4.13a, we conclude that a perfect classiÔ¨Åer has ROC-AUC = 1, whereas a classiÔ¨Åer which only assigns random guesses has ROC-AUC = 0.5. The ROC-AUC is thus summarizing the performance of a classiÔ¨Åer for all possible values of the decision threshold ùëü in a single number.
The F 1 score and the precision-recall curve
Many binary classiÔ¨Åcation problems have a particular characteristics, in that they are imbalanced, or asymmetric, or both. We say that a problem is
(i) imbalanced if the vast majority of the data points belongs to one class, typically the negative class ùë¶ = ‚àí1. This imbalance implies that a (useless) classiÔ¨Åer which always predicts ùë¶(x) = ‚àí1 will score very well in terms of misclassiÔ¨Åcation rate (4.1a).
(ii) asymmetric if a false negative is considered more severe than a false positive, or vice versa. That asymmetry is not taken into account in the misclassifcation rate (4.1a).
A typical imbalanced problem is the prediction of a rare disease (most patients do not have it, that is, most data points are negative). That problem could also be an asymmetric problem if it is, say, considered more

76

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Additional tools for evaluating binary classiÔ¨Åers

problematic to predict an infected patient as healthy, than vice versa.
To start with, the confusion matrix oÔ¨Äers a good opportunity to explicitly inspect the false negatives
and positives in a more explicit fashion. It can, however, sometimes be useful to also summarize the
performance into a single number. For that purpose the misclassiÔ¨Åcation rate is not very helpful; in a severely imbalanced problem it can for instance favor a useless predictor that always predicts ‚àí1 over any
realistically useful predictor. For imbalanced problems where the negative class ùë¶ = ‚àí1 is the most common class, the F1 score is
therefore preferable over the misclassiÔ¨Åcation rate (or accuracy). The F1 score summarizes the precision and recall by their harmonic mean,

ùêπ1

=

2 precision ¬∑ recall precision + recall

,

(4.24)

which is a number between zero and one (higher is better).
For asymmetric problems, however, the F1 score is not suÔ¨Écient since it does not weigh in the preference of having one type of error considered more serious than the other. For that purpose a generalization of
the F1 score, namely the FùõΩ score, can be used. The FùõΩ score weigh together precision and recall by considering recall ùõΩ times as important as precision,

ùêπùõΩ

=

(1 + ùõΩ2

ùõΩ2)precision ¬∑ precision +

¬∑ recall recall

.

(4.25)

Much like the the misclassiÔ¨Åcation rate might be misleading for imbalanced problems, the ROC curve might also be misleading for such problems. Instead the precision‚Äìrecall curve can (for imbalanced problems where ùë¶ = ‚àí1 is the most common class) be more useful. As the name suggests, the precision‚Äì recall curve plots the precision (TP/P*, a large value is good) against the recall (TP/P, a large value is good) for all values of ùëü ‚àà [0, 1], much like the ROC curve. The precision-recall curve for the perfect classiÔ¨Åer touches the upper right corner, and a classiÔ¨Åer which only assigns random guesses gives a horizontal line at the level ùëÉ/ùëõ, as shown in Figure 4.13b.
Also for the precision-recall curve, we can deÔ¨Åne area under the precision-recall curve, PR-AUC. The best possible PR-AUC is 1, and the classiÔ¨Åer which only makes random guesses has PR-AUC equal to ùëÉ/ùëõ.
We summarize this section by an example of an imbalanced and asymmetric problem in medicine. The evaluation of real-world classiÔ¨Åcation problems, which most often are both imbalanced and asymmetric, is however a challenging topic with certain dilemmas that are discussed more in Chapter 12.

Example 4.5: The confusion matrix in thyroid disease detection

The thyroid is an endocrine gland in the human body. The hormones it produces inÔ¨Çuences the metabolic rate and the protein synthesis, and thyroid disorders may have serious implications. We consider the problem of detecting thyroid diseases, using the dataset provided by UCI Machine Learning Repository (Dheeru and Karra Taniskidou 2017). The dataset contains 7200 data points, each with 21 medical indicators as inputs (both qualitative and quantitative). It also contains the qualitative diagnosis {normal, hyperthyroid, hypothyroid}. For simplicity we convert this into the binary problem with the output classes {normal, abnormal}. The dataset is split into training and hold-out validation sets, with 3772 and 3428 data points respectively. The problem is imbalanced since only 7% of the data points are abnormal. Hence, the naive (and useless) classiÔ¨Åer which always predicts will obtain a misclassiÔ¨Åcation rate of around 7%. The problem is possibly also asymmetric, if false negatives (not indicating the disease) are considered more problematic than false positives (falsely indicating the disease). We train a logistic regression classiÔ¨Åer and evaluate it on the validation dataset (using the default decision threshold ùëü = 0.5, see (3.36)). We obtain the confusion matrix

ùë¶(x) = normal ùë¶(x) = abnormal

ùë¶ = normal 3177 1

ùë¶ = abnormal 237 13

Most validation data points are correctly predicted as normal, but a large part of the abnormal data is also falsely predicted as normal. This might indeed be undesired in the application. The accuracy (1-misclassiÔ¨Åcation) rate is 0.931 and the F1 score is 0.106. (The useless predictor of always predicting

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

77

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Understanding, evaluating and improving the performance

normal has a very similar accuracy of 0.927, but worse F1 score of 0.) To change the picture, we lower the decision threshold to ùëü = 0.15 in (3.36). That is, we predict the
positive (abnormal) class whenever the predicted class probability exceeds this value, ùëî(x) > 0.15. This results in new predictions with the following confusion matrix:

ùë¶ = normal ùë¶ = abnormal

ùë¶ = normal 3067 111

ùë¶ = abnormal 165 85

This change gives more true positives (85 instead of 13 patients are correctly predicted as abnormal), but
this happens at the expense of more false positives (111 instead of 1 patients are now falsely predicted as abnormal). As expected, the accuracy is now lower at 0.919, but the F1 score is higher at 0.381. Remember, however, that the F1 score does not take the asymmetry into account, but only the imbalance. We have to decide ourselves whether this classiÔ¨Åer is a good tradeoÔ¨Ä between the false negative and false positive rates,
by considering which type of error has the most severe consequences.

4.6 Further reading
This chapter was to a large extent inspired by the introductory machine learning textbook by Abu-Mostafa et al. (2012). There are also several other textbooks on machine learning, including Vapnik (2000) and Mohri et al. (2018), in which the central theme is understanding the generalization gap using formal deÔ¨Ånitions of model Ô¨Çexibility such as the VC dimension or Rademacher complexity. The understanding of model Ô¨Çexibility for deep neural networks (Chapter 6) is, however, still subject to research, see for example C. Zhang et al. (2017), Neyshabur et al. (2017), Belkin et al. (2019) and B. Neal et al. (2019) for some directions. Furthermore, the bias-variance decomposition is most often (including this chapter) presented only for regression, but a possible generalization to the classiÔ¨Åcation problem is suggested by Domingos (2000). An alternative to the precision-recall curve, the so-called precision-recall-gain-curve, is presented by Flach and Kull (2015).

78

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

5 Learning parametric models

In this chapter we elaborate on the concept of parametric modeling. We start by generalizing the notion of a parametric model and outline basic principles for learning these models from data. The chapter then resolves around three central concepts, namely loss functions, regularization and optimization. We have touched upon all of them already, mostly in connection to the parametric models in Chapter 3, linear and logistic regression. These topics are however central for many supervised machine learning methods, in fact even beyond parametric models, and deserve a more elaborate discussion.

5.1 Principles of parametric modeling
In Chapter 3 we introduced two basic parametric models for regression and classiÔ¨Åcation, linear regression and logistic regression, respectively. We also brieÔ¨Çy discussed how generalized linear models could be used to handle diÔ¨Äerent types of data. The concept of parametric modeling is however not restricted to these cases. We therefore start this chapter by introducing a general framework for parametric modeling, and discuss basic principles for learning these models from data.

Nonlinear parametric functions Consider the regression model (3.1), repeated here for convenience:

ùë¶ = ùëìùúΩ (x) + ùúÄ.

(5.1)

Here we have introduced an explicit dependence on the parameters ùúΩ in the notation to emphasize that the
equation above should be viewed as our model of the true input‚Äìoutput relationship. To turn this model into a linear regression, that could be trained using least squares with a closed form solution, we made two assumptions in Chapter 3. First, the function ùëìùúΩ was assumed to be linear in the model parameters, ùëìùúΩ (x) = ùúΩTx. Second, the noise term ùúÄ was assumed to be Gaussian, ùúÄ ‚àº N 0, ùúéùúÄ2 . The latter assumption is sometimes implicit, but as we saw in Chapter 3 it makes the maximum likelihood formulation equivalent
to least squares.
Both of these assumptions can be relaxed. Based on the expression above, the perhaps most obvious generalization is to allow the function ùëìùúΩ to be some arbitrary nonlinear function. Since we still want the function to be learnt from training data we require that it is adaptable. Similarly to the linear case, this can be accomplished by letting the function depend on some model parameters ùúΩ that control the shape of the function. DiÔ¨Äerent values of the model parameters will then result in diÔ¨Äerent functions ùëìùúΩ (¬∑). Learning the model amounts to Ô¨Ånding a suitable value for the parameter vector ùúΩ, such that the function ùëìùúΩ accurately describes the true input‚Äìoutput relationship. In mathematical terms, we say that we have a parametric family of functions

{ ùëìùúΩ (¬∑) : ùúΩ ‚àà Œò}

where Œò is the space containing all possible parameter vectors. We illustrate with an example: Example 5.1: Michaelis-‚ÄìMenten kinetics A simple example of a nonlinear parametric function is the Michaelis‚ÄìMenten equation for modeling enzyme

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

79

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models

kinetics. The model is given by

ùë¶

=

ùúÉ1ùë• ùúÉ2 + ùë•

+ùúÄ

= ùëìùúΩ ( ùë•)

where ùë¶ corresponds to a reaction rate and ùë• a substrate concentration. The model is parameterized by the maximum reaction rate ùúÉ1 > 0 and the so called Michaelis constant of the enzyme ùúÉ2 > 0. Note that ùëìùúΩ (ùë•) depends nonlinearly on the parameter ùúÉ2 appearing in the denominator.
Typically the model is written as a deterministic relationship without the noise term ùúÄ, but here we include
the noise as an error term for consistency with our statistical regression framework.

In the example above the parameters ùúÉ1 and ùúÉ2 have physical interpretations and are restricted to be positive. Thus, Œò corresponds to the positive quadrant in R2. However, in machine learning we typically
lack such physical interpretations of the parameters. The model is more of a ‚Äúblack box‚Äù which is adapted to Ô¨Åt the training data as well as possible. For simplicity we will therefore assume that Œò = Rùëë, meaning that ùúΩ is a ùëë-dimensional vector of real-valued parameters. The archetype of such nonlinear black-box
models are neural networks, which we will discuss in more detail in Chapter 6. If we need to restrict a
parameter value in some way, for example to be positive, then this can be accomplished by a suitable transformation of that parameter. For instance, in the Michaelis‚ÄìMenten equation we can replace ùúÉ1 and ùúÉ2 with exp(ùúÉ1) and exp(ùúÉ2), respectively, where the parameters are now allowed to take arbitrary real values.
Note that the likelihood corresponding to the model (5.1) is governed by the noise term ùúÄ. As long as we stick with the assumption that the noise is additive and Gaussian with zero mean and variance ùúéùúÄ2, we obtain a Gaussian likelihood function

ùëù(ùë¶ | x; ùúΩ) = N ùëìùúΩ (x), ùúéùúÄ2 .

(5.2)

The only diÔ¨Äerence between this expression and the likelihood used in the linear regression model (3.18)

is that the mean of the Gaussian distribution now is given by the arbitrary nonlinear function ùëìùúΩ (x).

Nonlinear classiÔ¨Åcation models can be constructed in a very similar way, as a generalization of the

logistic regression model (3.29). In binary logistic regression we Ô¨Årst compute the logit ùëß = ùúΩTx. The

probability of the positive class, that is ùëù(ùë¶ = 1 | x), is then obtained by mapping the logit value through

the

logistic

function,

‚Ñé(ùëß)

=

ùëíùëß 1+ùëíùëß

.

To

turn

this

into

a

nonlinear

classiÔ¨Åcation

model

we

can

simply

replace

the expression for the logit with ùëß = ùëìùúΩ (x) for some arbitrary real-valued nonlinear function ùëìùúΩ. Hence,

the nonlinear logistic regression model becomes

ùëî (x)

=

1

ùëí ùëìùúΩ (x) + ùëí ùëìùúΩ (x)

.

(5.3)

Analogously, we can construct a multiclass nonlinear classiÔ¨Åer by generalizing the multiclass logistic regression model (3.42). That is, we compute a vector of logits z = [ùëß1 ùëß2 . . . ùëßùëÄ ]T according to z = fùúΩ (x) where fùúΩ is some arbitrary function that maps x to an ùëÄ-dimensional real-valued vector z.
Propagating this logit vector through the softmax function results in a nonlinear model for the conditional class probabilities, gùúΩ (x) = softmax(fùúΩ (x)). We will return to nonlinear classiÔ¨Åcation models of this form
in Chapter 6, where we use neural networks to construct the function fùúΩ.

Loss minimization as a proxy for generalization
Having speciÔ¨Åed a certain model class‚Äîthat is a parametric family of functions deÔ¨Åning the model‚Äî learning amounts to Ô¨Ånding a suitable value for the parameters so that the model as accurately as possible describes the actual input‚Äìoutput relationship. For parametric models this learning objective is typically

80

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Principles of parametric modeling

formulated as an optimization problem, such as

loss function

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

ùêø(ùë¶ùëñ,

ùëìùúΩ (xùëñ))

.

(5.4)

cost function ùêΩ (ùúΩ)

That is, we seek to minimize a cost function deÔ¨Åned as the average of some (user-chosen) loss function ùêø evaluated on the training data. In some special cases (such as linear regression with squared loss) we can compute the the solution to this optimization problem exactly. However, in most cases, and in particular when working with nonlinear parametric models, this is not possible and we need to resort to numerical optimization. We will discuss such algorithms in more detail in Section 5.4, but it is useful to note already now that these optimization algorithms are often iterative. That is, the algorithm is run over many iterations and at each iteration the current approximate solution to the optimization problem (5.4) is updated into a new (hopefully better) approximate solution. This leads to a computational trade-oÔ¨Ä. The longer we run the algorithm the better solution we expect to Ô¨Ånd, but at the cost of a longer training time.
Finding the value of ùúΩ which is such that the model Ô¨Åts the training data as well as possible is a natural idea. However, as we discussed in the previous chapter the ultimate goal of machine learning is not to Ô¨Åt the training data as well as possible, but rather to Ô¨Ånd a model that can generalize to new data, not used for training the model. Put diÔ¨Äerently, the problem that we are actually interested in solving is not (5.4) but rather

ùúΩ = arg min ùê∏new (ùúΩ)
ùúΩ

(5.5)

where ùê∏new(ùúΩ) = E‚òÖ [ùê∏ (ùë¶(x‚òÖ; ùúΩ), ùë¶‚òÖ)] is the expected new data error (for some error function ùê∏ of interest; see Chapter 4). The issue is of course that the expected new data error is unknown to us. The ‚Äútrue‚Äù data generating distribution is not available and we can thus not compute the expected error with respect to new data, nor can we optimize the objective (5.5) explicitly. However, this insight is still of practical importance, because is means that

the training objective (5.4) is only a proxy for the actual objective of interest, (5.5).

This view of the training objective as a proxy has implications for how we approach the optimization problem (5.4) in practice. We make the following observations.
Optimization accuracy vs. statistical accuracy: The cost function ùêΩ (ùúΩ) is computed based on the training data and is thus subject to the noise in the data. It can be viewed as a random approximation of the ‚Äútrue‚Äù expected loss (obtained as ùëõ ‚Üí ‚àû). Hence, it is not meaningful to optimize ùêΩ (ùúΩ) with greater accuracy than the statistical error in the estimate. This is particularly relevant in situations when we need to spend a lot of computational eÔ¨Äort to obtain a very accurate solution to the optimization problem. This is unnecessary as long as we are within the statistical accuracy of the estimate. In practice it can diÔ¨Écult to determine what the statistical accuracy is though (and we will not elaborate on methods that can be used for estimating it in this book), but this trade-oÔ¨Ä between optimization accuracy and statistical accuracy is still useful to have in the back of the mind.
Loss function ‚â† error function As discussed in Chapter 4 we can use an error function ùê∏ for evaluating the performance of a model which is diÔ¨Äerent from the loss function ùêø used during training. In words, when training the model we minimize an objective which is diÔ¨Äerent from the one that we are actually interested in. This might seem counter-intuitive, but based on the proxy view on the training objective it makes perfect sense and, in fact, provides the machine learning engineer with additional Ô¨Çexibility in designing a useful training objective. There are many reasons for why we might want to use a loss function that is diÔ¨Äerent from the error function. First, it can result in a model which is expected to generalize better. The typical example is when evaluating a classiÔ¨Åcation

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

81

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models
model based on accuracy (equivalently, misclassiÔ¨Åcation error). If we were to train the model by minimizing the misclassiÔ¨Åcation loss we would only care about placing the decision boundaries to get as many training data points as possible on the right side, but without taking the distances to the decision boundaries into account. However, due the noise in the data having some margin to the decision boundary can result in better generalization, and there are loss functions that explicitly encourage this. Second, we can choose the loss function with the aim of making the optimization problem (5.4) is easier to solve, for instance by using a convex loss function (see Section 5.4). Third, certain loss functions can encourage other favorable properties of the Ô¨Ånal model, such as making the model less computationally demanding to use ‚Äúin production‚Äù.
Early stopping When optimizing the objective (5.4) using an iterative numerical optimization method, this can be thought of as generating a sequence of candidate models. At each iteration of the optimzation algorithm we have access to the current estimate of the parameter vector and we thus obtain a ‚Äúpath‚Äù of parameter values. Interestingly, it is not necessarily the end point of this path that is closest to the solution of (5.5). Indeed, the path of parameter values can pass a useful solution (with good generalization properties) before drifting oÔ¨Ä to a worse solution (for example due to overÔ¨Åtting). Based on this observation, there is another reason for stopping the optimization algorithm prematurely, apart from the purely computational reason mentioned above. By early stopping of the algorithm we can obtain a Ô¨Ånal model with superior performance to the one we would obtain if we run the algorithm until convergence. We refer to this as implicit regularization and discuss the details of how it can be implemented in practice in Section 5.3.
Explicit regularization Another strategy is to explicitly modify the cost function (5.4) by adding a term independent of the training data. We refer to this technique as explicit regularization. The aim is to make the Ô¨Ånal model generalize better, that is, we hope to make the solution to the modiÔ¨Åed problem closer to the solution of (5.5). We saw an example of this already in Section 3.3 where we introduced ùêø2 regularization. The underlying idea is the law of parsimony, that the simplest explanation of an observed phenomena is usually the right one. In the context of machine learning this means that if both a ‚Äúsimple‚Äù and a ‚Äúcomplicated‚Äù model Ô¨Åt the data (more or less) equally well, then the ‚Äúsimple‚Äù one will typically have superior generalization properties and should therefore be preferred. In explicit regularization the vague notion of ‚Äúsimple‚Äù and ‚Äúcomplicated‚Äù is reduced to simply mean small and large parameter values, respectively. In order to favor a simple model an extra term is added to the cost function that penalizes large parameter values. We will discuss regularization further in Section 5.3.
5.2 Loss functions and likelihood-based models
Which loss function ùêø to use in the training objective (5.4) is a design choice and diÔ¨Äerent loss functions will give rise to diÔ¨Äerent solutions ùúΩ. This will in turn result in models with diÔ¨Äerent characteristics. There is in general no ‚Äúright‚Äù or ‚Äúwrong‚Äù loss function, but for a given problem one particular choice can be superior to another in terms of small ùê∏new (note that there is a similar design choice involved in ùê∏new, namely how to choose the error function which deÔ¨Ånes how we measure the performance of the model). Certain combinations of models and loss functions have proven particularly useful and have historically been branded as speciÔ¨Åc methods. For example the term ‚Äúlinear regression‚Äù most often refers to the combination of a linear-in-the-parameter model and the squared error loss, whereas the term ‚Äúsupport vector classiÔ¨Åcation‚Äù (see Chapter 8) refers to a linear-in-the-parameter model trained using the hinge loss. In this section, however, we provide a general discussion about diÔ¨Äerent loss functions and their properties, without connections to a speciÔ¨Åc method.
One important aspect of a loss function is its robustness. Robustness is tightly connected to outliers, meaning spurious data points that do not describe the relationship we are interested in modeling. If outliers in the training data only have a minor impact on the learned model, we say that the loss function is robust. Conversely, a loss function is not robust if the outliers have a major impact on the learned model.

82

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Loss functions and likelihood-based models

Robustness is therefore a very important property in applications where the training data is contaminated with outliers. It is not a binary property, however, and loss functions can be robust to a greater or smaller degree. Some of the commonly used loss functions, including the squared error loss, are unfortunately not particularly robust, and it is therefore important for the user to make an active and informed decision before resorting to these ‚Äúdefault‚Äù options.
From a statistical perspective we can link the loss function to statistical properties of the learnt model. Firstly, the maximum likelihood approach provides a formal connection between the loss function and the probabilistic assumptions on the (noise in the) data. Secondly, even for loss functions that are not derived from a likelihood perspective, we can relate the so-called asymptotic minimizer of the loss function to the statistical properties of the model. The asymptotic minimizer refers to the model that minimizes the expected loss when averaged over the true data generating distribution. Equivalently, we can think about the asymptotic minimizer as the solution to the optimization problem (5.4) as the number of training data points ùëõ ‚Üí ‚àû (hence the name ‚Äúasymptotic‚Äù). If there is a unique asymptotic minimizer from which we can recover the true conditional distribution ùëù(ùë¶ | x), then the loss function is said to be strictly proper. We will return to this concept below, speciÔ¨Åcally in the context of binary classiÔ¨Åcation.

Loss functions for regression In Chapter 3 we introduced the squared error loss1

ùêø(ùë¶, ùë¶) = ùë¶ ‚àí ùë¶ 2,

(5.6)

which is the default choice for linear regression since it simpliÔ¨Åes the training to only solving the normal equations. The squared error loss is often used also for other regression models, such as neural networks. Another common choice is the absolute error loss,

ùêø(ùë¶, ùë¶) = |ùë¶ ‚àí ùë¶|.

(5.7)

The absolute error loss is more robust to outliers than the squared error loss since it grows slower for large
errors, see Figure 5.1. In Chapter 3 we introduced the maximum likelihood motivation of the squared error loss by assuming that the output ùë¶ is measured with additive noise ùúÄ from a Gaussian distribution, ùúÄ ‚àº N 0, ùúéùúÄ2 . We can similarly motivate the absolute error loss by instead assuming ùúÄ to have a Laplace distribution, ùúÄ ‚àº L (0, ùëè ùúÄ). We elaborate and expand on the idea of deriving the loss function from the maximum likelhood objective and certain statistical modeling assumptions below. However, there are also
some commonly used loss functions for regression which are not very natural to derive from a maximum
likelihood perspective.
It is sometimes argued that the squared error loss is a good choice because of its quadratic shape which penalizes small errors (ùúÄ < 1) less than linearly. After all, the Gaussian distribution appears (at least approximately) quite often in nature. However, the quadratic shape for large errors (ùúÄ > 1) is the reason
for its non-robustness, and the Huber loss has therefore been suggested as a hybrid between the absolute loss and squared error loss:

ùêø(ùë¶, ùë¶) =

1 2

(ùë¶

‚àí

ùë¶)2

|ùë¶

‚àí

ùë¶|

‚àí

1 2

if |ùë¶ ‚àí ùë¶| < 1, otherwise.

(5.8)

Another extension to the absolute error loss is the ùúñ-insensitive loss,

ùêø(ùë¶, ùë¶) =

0 |ùë¶ ‚àí ùë¶| ‚àí ùúñ

if |ùë¶ ‚àí ùë¶| < ùúñ, otherwise,

(5.9)

1As you might already have noticed, the arguments to the loss function (here ùë¶ and ùë¶) varies with context. The reason for this is that diÔ¨Äerent loss functions are most naturally expressed in terms of diÔ¨Äerent quantities, for example the prediction ùë¶, the predicted conditional class probability ùëî(x), the classiÔ¨Åer margin, etc.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

83

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models

4

Squared error loss

Absolute error loss

3

Huber loss

ùúñ-insensitive loss

2

Loss

1

0 ‚àí2

‚àí1 ‚àíùúñ

0

ùúñ1

2

error ùë¶ ‚àí ùë¶

Figure 5.1: The loss functions for regression presented in the text, each as a function of the error ùë¶ ‚àí ùë¶.

where ùúñ is a user-chosen design parameter. This loss places a tolerance of width 2ùúñ around the observed ùë¶ and behaves like the absolute error loss outside this region. The robustness properties of the ùúñ-insensitive loss are very similar to those of the absolute error loss. The ùúñ-insensitive loss turns out to be useful for support vector regression in Chapter 8. We illustrate all these loss functions for regression in Figure 5.1.

Loss functions for binary classiÔ¨Åcation An intuitive loss function for binary classiÔ¨Åcation is provided by the misclassiÔ¨Åcation loss,

ùêø(ùë¶, ùë¶) = I{ùë¶ ‚â† ùë¶} = 0 1

if ùë¶ = ùë¶, if ùë¶ ‚â† ùë¶.

(5.10)

However, even though a small misclassiÔ¨Åcation loss might be the ultimate goal in practice, this loss function is rarely used when training models. As mentioned in Section 5.1, there are at least two reasons for this. First, using a diÔ¨Äerent loss function can result in a model that generalizes better from the training data. This can be understood by noting that the Ô¨Ånal prediction ùë¶ does not reveal all aspects of the classiÔ¨Åer. Intuitively, we may prefer to not have the decision boundary close to the training data points, even if they are correctly classiÔ¨Åed, but instead push the boundary further away to have some margin. To achieve this we can formulate the loss function, not just in terms of the hard class prediction ùë¶, but based on the predicted class probability ùëî(x) or some other continuous quantity used to compute the class prediction. The second reason for not using misclassiÔ¨Åcation loss as training objective, which is also important, is that it would result in a cost function that is piecewise constant. From a numerical optimization perspective this is a diÔ¨Écult objective since the gradient is zero everywhere, except where it is undeÔ¨Åned.
For a binary classiÔ¨Åer that predicts conditional class probabilities ùëù(ùë¶ = 1 | x) in terms of a function ùëî(x), the cross-entropy loss, as introduced in Chapter 3, is a natural choice:

ùêø(ùë¶, ùëî(x)) =

ln ùëî(x) ln(1 ‚àí ùëî(x))

if ùë¶ = 1, if ùë¶ = ‚àí1.

(5.11)

This loss was derived from a maximum likelihood perspective, but unlike regression (where we had to specify a distribution for ùúÄ) there are no user choices left in the cross-entropy loss, other than what model to use for ùëî(x). Indeed, for a binary classiÔ¨Åcation problem the model ùëî(x) provides a complete statistical description of the conditional distribution of the output given the input.
While cross entropy is commonly used in practice, but there are also other loss functions for binary classiÔ¨Åcation that are useful. To deÔ¨Åne an entire family of loss functions, let us Ô¨Årst introduce the concept of margins in binary classiÔ¨Åcation. Many binary classiÔ¨Åers ùë¶(x) can be constructed by thresholding some

84

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Loss functions and likelihood-based models

real-valued function2 ùëì (x) at 0. That is, we can write the class prediction

ùë¶(x) = sign{ ùëì (x)}.

(5.12)

Logistic regression, for example, can be brought into this form by simply using ùëì (x) = ùúΩTx, as shown in (3.39). More generally, for a nonlinear generalization of the logistic regression model where the probability
of the positive class is modeled as in (5.3), the prediction (5.12) corresponds to the most likely class. Not
all classiÔ¨Åers have a probabilistic interpretation, however, but often they can still be expressed as in (5.12) for some underlying function ùëì (x).
The decision boundary for any classiÔ¨Åer of the form (5.12) is given by the values of x for which ùëì (x) = 0. To simplify our discussion we will assume that none of the data points fall exactly on the decision boundary (which always gives rise to an ambiguity). This will imply that we can assume that ùë¶(x) as deÔ¨Åned above is always either ‚àí1 or +1. Based on the function ùëì (x), we say that

the margin of a classiÔ¨Åer for a data point (x, ùë¶) is ùë¶ ¬∑ ùëì (x).

(5.13)

It follows that if ùë¶ and ùëì (x) have the same sign, meaning that the classiÔ¨Åcation is correct, then the margin is positive. Analogously, for an incorrect classiÔ¨Åcation ùë¶ and ùëì (x) will have diÔ¨Äerent signs and the margin is negative. The margin can be viewed as a measure of certainty in a prediction, where data points with small margins in some sense (not necessarily Euclidean) are close to the decision boundary. The margin plays a similar role for binary classiÔ¨Åcation as the prediction error ùë¶ ‚àí ùë¶ does for regression.
We can now deÔ¨Åne loss functions for binary classiÔ¨Åcation in terms of the margin, by assigning a small loss to positive margins (correct classiÔ¨Åcations) and a large loss to negative margins (misclassiÔ¨Åcations). We can, for instance, re-formulate the logistic loss (3.34) in terms of the margin as

ùêø(ùë¶ ¬∑ ùëì (x)) = ln (1 + exp (‚àíùë¶ ¬∑ ùëì (x))) ,

(5.14)

where, in line with the discussion above, the linear logistic regression model corresponds to ùëì (x) = ùúÉTx. Analogously to the derivation of the logistic loss in Chapter 3, this is just another way of writing the cross entropy (or negative log-likelihood) loss (5.11), assuming that the probability of the positive class is modeled according to (5.3). However, from an alternative point of view we can consider (5.14) as a generic margin-based loss, without linking it to the probabilistic model (5.3). That is, we simply postulate a classiÔ¨Åer according to (5.12) and learn the parameters of ùëì (x) by minimizing (5.14). This is of course equivalent to logistic regression, except for the fact that we have seemingly lost the notion of a conditional class probability estimate ùëî(x) and only have a ‚Äúhard‚Äù prediction ùë¶(x). We will, however, recover the class probability estimate later when we discuss the asymptotic minimizer of the logistic loss.
We can also re-formulate the misclassiÔ¨Åcation loss in terms of the margin,

ùêø(ùë¶ ¬∑ ùëì (x)) =

1 0

if ùë¶ ¬∑ ùëì (x) < 0, otherwise.

(5.15)

More importantly, however, is that the margin-view allows us to easily come up with other loss functions with possibly favorable properties. In principle, any decreasing function is a candidate loss. However, most loss functions used in practice are also convex which is useful when optimizing the training loss numerically.
One example is the exponential loss, deÔ¨Åned as

ùêø(ùë¶ ¬∑ ùëì (x)) = exp(‚àíùë¶ ¬∑ ùëì (x)),

(5.16)

which turns out to be a useful loss function when we later derive the AdaBoost algorithm in Chapter 7. The downside of the exponential loss is that it is not particularly robust against outliers, due to its exponential
2In general the function ùëì (x) depends on the model parameters ùúΩ, but in the presentation below we will drop this dependence from the notation for brevity.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

85

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models

growth for negative margins, compared to, for example, the linear asymptotic growth of the logistic loss.3 We also have the hinge loss, which we will use later for support vector classiÔ¨Åcation in Chapter 8,

ùêø(ùë¶ ¬∑ ùëì (x)) =

1 ‚àí ùë¶ ¬∑ ùëì (x) 0

for ùë¶ ¬∑ ùëì (x) ‚â§ 1, otherwise.

(5.17)

As we will see in Chapter 8, the hinge loss has an attractive so-called support-vector property. However, a downside of the hinge loss is that it is not a strictly proper loss function, which means that it is not possible to interpret the learnt classiÔ¨Åcation model probabilistically when using this loss (we elaborate on this below). As a remedy, one may instead consider the squared hinge loss

ùêø(ùë¶ ¬∑ ùëì (x)) =

(1 ‚àí ùë¶ ¬∑ ùëì (x))2 0

for ùë¶ ¬∑ ùëì (x) ‚â§ 1, otherwise,

(5.18)

which on the other hand is less robust than the hinge loss (quadratic instead of linear growth). A more elaborate alternative is therefore the Huberized squared hinge loss

ùêø(ùë¶

¬∑

ùëì

(x))

=

Ô£±Ô£¥Ô£¥Ô£¥Ô£≤‚àí4ùë¶ Ô£¥Ô£¥Ô£¥Ô£≥0(1 ‚àí

¬∑ ùë¶

ùëì (x) ¬∑ ùëì (x))2

for ùë¶ ¬∑ ùëì (x) ‚â§ ‚àí1, for ‚àí 1 ‚â§ ùë¶ ¬∑ ùëì (x) ‚â§ 1, (squared hinge loss) otherwise,

(5.19)

whose name refers to its similarities to the Huber loss for regression, namely that the quadratic function is replaced with a linear function for margins < ‚àí1. The three loss functions presented above are all particularly interesting for support vector classiÔ¨Åcation, due to the fact that they are all exactly 0 for margins > 1.
We summarize this cascade of loss functions for binary classiÔ¨Åcation in Figure 5.2, which illustrates all these losses as a function of the margin.
When learning models for imbalanced or asymmetric problems, it is possible to modify the loss function to account for the imbalance or asymmetry. For example, to reÔ¨Çect that not predicting ùë¶ = 1 correctly is a ‚Äúùê∂ times more severe mistake‚Äù than not predicting ùë¶ = ‚àí1 correctly, the misclassiÔ¨Åcation loss can be modiÔ¨Åed into

Ô£±Ô£¥Ô£¥Ô£¥Ô£≤0 ùêø(ùë¶, ùë¶) = Ô£¥Ô£¥Ô£¥Ô£≥ùê∂1

if ùë¶ = ùë¶, if ùë¶ ‚â† ùë¶ and ùë¶ = ‚àí1, if ùë¶ ‚â† ùë¶ and ùë¶ = 1.

(5.20)

The other loss functions can be modiÔ¨Åed in a similar fashion. A similar eÔ¨Äect can also be achieved by, for this example, simply duplicating all positive training data points ùê∂ times in the training data, instead of modifying the loss function.
We have already made some claims about robustness. Let us motivate them using Figure 5.2. One characterization of an outlier is as a data point on the wrong side of and far away from the decision boundary. In a margin perspective, that is equivalent to a large negative margin. The robustness of a loss function is therefore tightly connected to the shape of the loss function for large negative margins. The steeper slope and heavier penalization of large negative margins, the more sensitive it is to outliers. We can therefore tell from Figure 5.2 that the exponential loss is expected to by sensitive to outliers, due to its exponential growth, whereas the squared hinge loss is somewhat more robust with a quadratic growth instead. However, even more robust are the Huberized squared hinge loss, the hinge loss and the logistic loss which all have an asymptotic behavior which is linear. Most robust is the misclassiÔ¨Åcation loss, but as already discussed, that loss has other disadvantages.

3For ùë¶ ùëì (x) 0 it holds that ln (1 + exp (‚àíùë¶ ùëì (x))) ‚âà ‚àíùë¶ ùëì (x).

86

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Loss functions and likelihood-based models

Logistic loss

12

Exponential loss

Hinge loss

Squared hinge loss

Huberized squared hinge loss

10

MisclassiÔ¨Åcation loss

8

Loss

6

4

2

0

‚àí3

‚àí2.5

‚àí2

‚àí1.5

‚àí1

‚àí0.5

0

0.5

1

1.5

2

2.5

Margin, ùë¶ ¬∑ ùëê(x)

Figure 5.2: Comparison of some common loss functions for classiÔ¨Åcation, plotted as a function of the margin.

Multiclass classiÔ¨Åcation

So far we have only discussed the binary classiÔ¨Åcation problem with ùëÄ = 2. The cross-entropy (equivalently, negative log-likelihood) loss is straightforward to generalize to the multiclass problem, that is ùëÄ > 2, as we did in Chapter 3 for logistic regression. This is a useful property of the likelihood-based loss, since it allows us to systematically treat both binary and multiclass classiÔ¨Åcation in the same coherent framework.

Generalizing the other loss functions discussed above requires a generalization of the margin to the multiclass problem. That is possible but we do not elaborate on it in this book. Instead we mention a pragmatic approach which is to reformulate the problem as several binary problems. This reformulation can be done using either a one-versus-rest or one-versus-one scheme.

The one-versus-rest (or one-versus-all or binary relevance) idea is to train ùëÄ binary classiÔ¨Åers. Each classiÔ¨Åer in this scheme is trained for predicting one class against all the other classes. To make a prediction for a test data point, all ùëÄ classiÔ¨Åers are used, and the class which, for example, is predicted with the largest margin is taken as the predicted class. This approach is a pragmatic solution, which may turn out to work well for some problems.

The one-versus-one idea is instead to train one classiÔ¨Åers for each pair of classes. If there are ùëÄ classes

in

total,

there

are

1 2

ùëÄ

(

ùëÄ

‚àí

1)

such

pairs.

To

make

a

prediction

each

classiÔ¨Åer

predicts

either

of

its

two

classes, and the class which overall obtains most ‚Äúvotes‚Äù is chosen as the Ô¨Ånal prediction. The predicted

margins can be used to break a tie if that happens. Compared to one-versus-rest, the one-versus-one

approach

has

the

disadvantage

of

involving

1 2

ùëÄ

(

ùëÄ

‚àí

1)

classiÔ¨Åers,

instead

of

only

ùëÄ.

On

the

other

hand

each of these classiÔ¨Åers is trained on much smaller datasets (only the data points that belong to either

of the two classes) compared to one-versus-rest which uses the entire original training dataset for all ùëÄ

classiÔ¨Åers.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

87

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models

Likelihood-based models and the maximum likelihood approach

The maximum likelihood approach is a generic way of constructing a loss function based on a statistical model of the observed data. In general, maximizing the data likelihood is equivalent to minimizing a cost function based on the negative log-likelihood loss,

ùêΩ(ùúΩ)

=

‚àí

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

ln

ùëù(ùë¶ùëñ

|

xùëñ ;

ùúΩ).

Hence, in all cases where we have a probabilistic model of the conditional distribution ùëù(ùë¶ | x) the negative log-likelihood is a plausible loss function. For classiÔ¨Åcation problems this takes a particularly simple form since ùëù(ùë¶ | x) then corresponds to a probability vector over the ùëÄ classes and the negative log-likelihood is then equivalent to the cross-entropy loss (3.44) (or (3.32) in the case of binary classiÔ¨Åcation).
Also in the regression case there is a duality between certain common loss functions and the maximum likelihood approach, as we have previously observed. For instance, in a regression model with additive noise as in (5.1), the squared error loss is equivalent to the negative log-likelihood if we assume a Gaussian noise distribution, ùúÄ ‚àº N 0, ùúéùúÄ2 . Similarly, we noted above that the absolute error loss corresponds to an implicit assumption of Laplace distributed noise, ùúÄ ‚àº L (0, ùëè ùúÄ).4 This statistical perspective is one way to understand the fact that the absolute error loss is more robust (less sensitive to outliers) than the squared error loss, since the Laplace distribution has thicker tails compared to the Gaussian distribution. The Laplace distribution therefore encodes sporadic large noise values (that is, outliers) as more probable, compared to the Gaussian distribution.
Using the maximum likelihood approach, other assumptions about the noise or insights into its distribution can be incorporated in a similar way in the regression model (5.1). For instance, if we believe that the error is non-symmetric, in the sense that the probability of observing a large positive error is larger than the probability of observing a large negative error, then this can be modeled by a skewed noise distribution. Using the negative log-likelihood loss is then a systematic way of incorporating this skewness in the training objective.
Relaxing the Gaussian assumption in (5.1) gives additional Ô¨Çexibility to the model. However, the noise is still assumed to be additive and independent of the input x. The real power of the likelihood perspective for designing a loss function comes when also these basic assumptions are dropped. For instance, in Section 3.4 we introduced generalized linear models as a way to handle output variables with speciÔ¨Åc properties, such as count data (that is, ùë¶ takes values in the set of natural numbers 0, 1, 2, . . . ). In such situations, to build a model we often start from a speciÔ¨Åc form of the likelihood ùëù(ùë¶ | x), which is chosen to capture the key properties of the data (for instance, having support only on the natural numbers). Hence, the likelihood becomes an integral part of the model and this approach therefore lends itself naturally to training by maximum likelihood.
In generalized linear models (see Section 3.4) the likelihood is parameterized in a very particular way, but when working with nonlinear parametric models this is not strictly necessary. A more direct approach to (nonlinear) likelihood-based parametric modeling is therefore to

model the conditional distribution ùëù(ùë¶ | x; ùúΩ) directly as a function parameterized by ùúΩ.

More speciÔ¨Åcally, once we have assumed a certain form for the likelihood (such as a Gaussian, a Poisson, or some other distribution) its shape will be controlled by some parameters (such as the mean and the variance of the Gaussian, or the rate of a Poisson distribution, not to be confused by ùúΩ). The idea is then the construct a parametric model fùúΩ (x), such that the output of this model is a vector of parameters controlling the shape of the distribution ùëù(ùë¶ | x; ùúΩ).
As an example, assume that we are working with unbounded real-valued outputs and want to use a Gaussian likelihood, similarly to the regression model (5.2). However, the nature of data is such that the
4This can be veriÔ¨Åed from the deÔ¨Ånition of the Laplace probability density function, which is an exponential of the negative absolute deviation from the mean.

88

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Loss functions and likelihood-based models

noise variance, that is the magnitude of the errors that we expect to see, is varying with the input x. By directly working with the likelihood formulation, we can then hypothesize a model according to

ùëù(ùë¶ | x; ùúΩ) = N ( ùëìùúΩ (x), exp(‚ÑéùúΩ (x)))

where ùëìùúΩ and ‚ÑéùúΩ are two arbitrary (linear or nonlinear) real-valued regression functions parameterized by ùúΩ (hence, following the notation above, fùúΩ (x) = ( ùëìùúΩ (x) ‚ÑéùúΩ (x))T). The exponential function is used to ensure that the variance is always positive without explicitly constraining the function ‚ÑéùúΩ (x). The two functions can be learned simultaneously by minimizing the negative log-likelihood over the training data. Note that
in this case the problem does not simplify to a squared error loss, despite the fact that the likelihood is
Gaussian, since we need to take the dependence on the variance into account. More precisely, the negative
log-likelihood loss becomes,

ùêø (ùë¶, ùúΩ) = ‚àí ln N ( ùëìùúΩ (x), exp(‚ÑéùúΩ (x)))

‚àù

‚ÑéùúΩ (x)

+

(ùë¶ ‚àí ùëìùúΩ (x))2 exp(‚ÑéùúΩ (x))

+

const.

Once the parameters have been learning, the resulting model is capable of predicting a diÔ¨Äerent mean and a diÔ¨Äerent variance for the output ùë¶, depending on the value of the input variable x.5
Other situations that can be modeled using a direct likelihood model in a similar way include
multimodality, quantization and truncated data. As long as the modeler can come up with a reasonable
likelihood, that is a distribution that could have generated the data under study, the negative log-likelihood loss can be used to learn the model in a systematic way.

Strictly proper loss functions and asymptotic minimizers

As mentioned earlier the asymptotic minimizer of a loss function is an important theoretical concept for
understanding its properties. The asymptotic minimizer is the model which minimizes the cost function when the number of training data ùëõ ‚Üí ‚àû (hence the name asymptotic). To formalize this, assume that the model is expressed in terms of a function ùëì (x). As we have seen above this captures not only regression, but also classiÔ¨Åcation through the margin concept. The asymptotic minimizer ùëì ‚àó(x) of a loss function ùêø(ùë¶, ùëì (x)) is then deÔ¨Åned as the function which minimizes the expected loss,

ùëì ‚àó(¬∑) = arg min E[ùêø (ùë¶, ùëì (x))] .
ùëì

(5.21)

There are a couple of things to note about this expression. First, we stated above that the asymptotic minimizer is obtained as the solution to the training objective (5.4) as ùëõ ‚Üí ‚àû, but this has now been replaced by an expected value. This is motivated by the law of large numbers stipulating that the cost function (5.4) will converge to the expected loss as ùëõ ‚Üí ‚àû, and the latter is more convenient to analyze mathematically. Note that the expected value is taken with respect to a ground truth data generating probability distribution ùëù(ùë¶, x), analogously to how we reasoned about a the new data error in Chapter 4. Second, when we talk about asymptotic minimizers it is typically assumed that the model class is Ô¨Çexible enough to contain any function ùëì (x). Consequently, the minimization in (5.21) is not with respect to a Ô¨Ånite dimensional model parameter ùúΩ, but rather with respect to the function ùëì (x) itself. The reason for this, rather abstract, deÔ¨Ånition is that we want to derive the asymptotic minimizer as a property of the loss function itself, not of a particular combination of loss function and model class.
The expected value above is with respect to both inputs x and outputs ùë¶. However, by the law of total expectation we can write E[ùêø(ùë¶, ùëì (x))] = E[E[ùêø (ùë¶, ùëì (x)) | x]] where the inner expectation is over ùë¶ (conditionally on x) and the outer expectation is over x. Now, since ùëì (¬∑) is free to be any function,
5This property is referred to as heteroskedasticity (in contrast to the standard regression model (5.2) which is homoskedastic, that is it has the same output variance for all possible inputs).

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

89

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models

minimizing the total expectation is equivalent to minimizing the inner expectation point-wise for each value of x. Therefore, we can replace (5.21) with

ùëì ‚àó(x) = arg min E[ùêø (ùë¶, ùëì (x)) | x]
ùëì (x)

(5.22)

where the minimization is now done independently for any Ô¨Åxed value of x. By computing the asymptotic minimizer of a loss function, we obtain information about the expected
behavior or properties of a model that is learned using this loss function. Although the asymptotic
minimizer is an idealized theoretical concept (assuming inÔ¨Ånite data and inÔ¨Ånite Ô¨Çexibility) it reveals, in
some sense, what the training algorithm strives to achieve when minimizing a particular loss.
The concept is useful for understanding both regression and classiÔ¨Åcation losses. A few notable
examples in the regression setting are the asymptotic minimizers of the squared error loss and the absolute
error loss, respectively. For the former, the asymptotic minimizer can be shown to be equal to the conditional mean, ùëì ‚àó(x) = E[ùë¶ | x]. That is, a regression model trained using squared error loss will strive to predict ùë¶ according to its true conditional mean under the data generating distribution ùëù(ùë¶, x) (although in practice this will be hampered by the limited Ô¨Çexibility of the model class and the limited amount of
training data). For the absolute error loss the asymptotic minimizer is given by the conditional median, ùëì ‚àó(x) = Median[ùë¶ | x]. This is less sensitive to the tail probability of ùëù(ùë¶ | x) than the conditional mean, providing yet another interpretation of the improved robustness of the absolute error loss.
Related to the concept of asymptotic minimizers is the notion of a strictly proper loss function. A loss function is said to be strictly proper6 if its asymptotic minimizer is (i) unique and (ii) in one-to-one correspondence with the true conditional distribution ùëù(ùë¶ | x). Put diÔ¨Äerently, for a strictly proper loss function we can express ùëù(ùë¶ | x) in terms of the asymptotic minimizer ùëì ‚àó(x). Such a loss function will thus strive to recover a complete probabilistic characterization of the true input‚Äìoutput relationship.
This requires a probabilistic interpretation of the model, in the sense that we can express ùëù(ùë¶ | x) in terms of the model ùëì (x), which is not always obvious. One case which stands out in this respect is the maximum likelihood approach. Indeed, training by maximum likelihood requires a likelihood-based model
since the corresponding loss is expressed directly in terms of the likelihood. As we have discussed above,
the negative log-likelihood loss is a very generic loss function (it is applicable to regression, classiÔ¨Åcation,
and many other types of problems). We can now complement this by the theoretical statement that

the negative-log likelihood loss is strictly proper.

Note that this applies to any type of problem where the negative log-likelihood loss can be used. As noted above, the concept of a loss function being strictly proper is related to its asymptotic minimizer, which in turn is derived under the assumption of inÔ¨Ånite Ô¨Çexibility and inÔ¨Ånite data. Hence, what our claim above says is that if the likelihood-based model is Ô¨Çexible enough to describe the true conditional distribution ùëù(ùë¶ | x), then the optimal solution to the maximum likelihood problem as ùëõ ‚Üí ‚àû is to learn this true distribution.

Time to reÔ¨Çect 5.1: To express the expected negative log-likelihood loss mathematically we need to distinguish between the likelihood according to the model, which we can denote by ùëû(ùë¶ | x) for the time being, and the likelihood with respect to the true data generating distribution ùëù(ùë¶ | x).
The expected loss becomes

Eùëù(ùë¶ | x) [‚àí ln ùëû(ùë¶ | x) | x]
which is referred to as the (conditional) cross-entropy of the distribution ùëû(ùë¶ | x) with respect to the distribution ùëù(ùë¶ | x) (which explains the alternative name cross-entropy loss commonly used in classiÔ¨Åcation).
What does our claim, that the negative log-likelihood loss is strictly proper, imply regarding the

6A loss function that is proper but not strictly proper is minimized by the true conditional distribution ùëù(ùë¶ | x), but the minimizing argument is not unique.

90

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Loss functions and likelihood-based models

cross entropy?

That negative log-likelihood is strictly proper should not come as a surprise since it is tightly linked to the statistical properties of the data. What is perhaps less obvious is that there are other loss functions that are also strictly proper, as we will see next. To make the presentation below more concrete we will focus on the case of binary classiÔ¨Åcation for the remainder of this section. In binary classiÔ¨Åcation the conditional distribution ùëù(ùë¶ | x) takes a particularly simple form, since it is completely characterized by a single number, namely the probability of the positive class, ùëù(ùë¶ = 1 | x).
Returning to the margin-based loss functions discussed above, recall that any loss function that encourages positive margins can be used to train a classiÔ¨Åer, which can then be used for making class predictions according to (5.12). However,

it is only when we use a strictly proper loss function that we can interpret the resulting classiÔ¨Åcation model ùëî(x) as an estimate of the conditional class probability ùëù(ùë¶ = 1 | x).

When choosing a loss function for classiÔ¨Åcation it is therefore instructive to consider its asymptotic

minimizer, since this will determine whether or not the loss function is strictly proper. In turn, this will

reveal if it is sensible to use the resulting model to reason about conditional class probabilities or not.

We proceed by stating the asymptotic minimizers for some of the loss functions presented above.

Deriving the asymptotic minimizer is most often a straightforward calculation, but for brevity we do not

include the derivations here. Starting with the binary cross-entropy loss (5.11), its asymptotic minimizer can be shown to be ùëî‚àó(x) = ùëù(ùë¶ = 1 | x). In other words, when ùëõ ‚Üí ‚àû the loss function (5.11) is uniquely

minimized when ùëî(x) is equal to the true conditional class probability. This is in agreement with the

discussion above, since the binary cross-entropy loss is just another name for the negative log-likelihood.

Similarly, the asymptotic minimizer invertible function of ùëù(ùë¶ = 1 | x) and

of the hence

logistic loss the logistic

(5.14) loss is

is ùëì ‚àó(x) strictly

=

ln

ùëù ( ùë¶=1 | 1‚àí ùëù ( ùë¶=1

x) | x)

.

This

proper. By inverting

is an ùëì ‚àó (x)

we obtain ùëù(ùë¶ obtained from

= ùëì‚àó

1 | x)

=

exp ùëì 1+exp

(x). With the

‚àó (x) ùëì ‚àó (x)

which

shows how conditional class probability predictions

can be

‚Äúmargin formulation‚Äù of logistic regression, we seemingly lost the class

probability predictions ùëî(x). We have now recovered it. Again, this is not surprising since the logistic loss

is a special case of negative log-likelihood when using a logistic regression model.

For

the exponential

loss (5.16),

the

asymptotic

minimizer is

ùëì ‚àó (x)

=

1 2

ln

ùëù ( ùë¶=1 | 1‚àí ùëù ( ùë¶=1

x) | x)

,

which

is

in fact

the same expression as we got for the is therefore also strictly proper, and

lùëìo‚àóg(xis)ticcalnosbseapinavret rftreodmaandcounssetdanftofracptroerdi12c.tiTngheceoxnpdoitnieonntailalclloassss

probabilities.

Turning to the hinge loss (5.17) the asymptotic minimizer is

ùëì ‚àó (x) =

1 ‚àí1

if ùëù(ùë¶ = 1 | x) > 0.5, if ùëù(ùë¶ = 1 | x) < 0.5.

This is a non-invertible transformation of ùëù(ùë¶ = 1 | x), which means that it is not possible to recover ùëù(ùë¶ = 1 | x) from the asymptotic minimizer ùëì ‚àó(x). This implies that a classiÔ¨Åer learned using hinge loss (such as support vector classiÔ¨Åcation, Section 8.5) is not able to predict conditional class probabilities.
The squared hinge loss (5.18), on the other hand, is a strictly proper loss function, since its asymptotic minimizer is ùëì ‚àó(x) = 2ùëù(ùë¶ = 1 | x) ‚àí 1. This also holds for the Huberized square hinge loss (5.19). Recalling our robustness discussion, we see that by squaring the hinge loss we make it strictly proper but at
the same time we impact its robustness. However, the ‚ÄúHuberization‚Äù (replacing the quadratic curve with a linear one for margins < ‚àí1) improves the robustness while keeping the property of being strictly proper.
We have now seen that some (but not all) of the loss functions are strictly proper, meaning they could
potentially predict conditional class probabilities correctly. However, this is only under the assumption that the model is suÔ¨Éciently Ô¨Çexible such that ùëî(x) or ùëì (x) actually can take the shape of the asymptotic minimizer. This is possibly problematic; for instance, recall that ùëì (x) is a linear function in logistic

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

91

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models
regression, whereas ùëù(ùë¶ = 1 | x) can be almost arbitrarily complicated in real world applications. It is therefore not suÔ¨Écient to use a strictly proper loss function in order to accurately predict conditional class probabilities, but our model also has to be Ô¨Çexible enough. This discussion is also only valid in the limit as ùëõ ‚Üí ‚àû. However, in practice ùëõ is always Ô¨Ånite, and we may ask how large ùëõ has to be for a Ô¨Çexible enough model to at least approximately learn the asymptotic minimizer? Unfortunately, we cannot give any general numbers, but following the same principles as the overÔ¨Åtting discussion in Chapter 4, the more Ô¨Çexible the model the larger ùëõ is required. If ùëõ is not large enough, the predicted conditional class probabilities tend to ‚ÄúoverÔ¨Åt‚Äù to the training data. In summary, using a strictly proper loss function will encourage the training procedure to learn a model that is faithful to the true statistical properties of the data, but in itself it is not enough to guarantee that these properties are well described by the model.
In many practical application, having access to reliable uncertainty estimates regarding a model‚Äôs predictions is necessary for robust and well-informed decision making. In such cases it is thus important to validate the model, not only in terms of accuracy or expected errors, but also in terms of its statistical properties. One approach is to evaluate the so-called calibration of the model, which however is beyond the scope of this book.

92

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Regularization

5.3 Regularization
We will now take a closer look at regularization, which was brieÔ¨Çy introduced in Section 3.3 as a useful tool for avoiding overÔ¨Åtting if the model was too Ô¨Çexible, such as a polynomial of high degree. We have also discussed thoroughly in Chapter 4 the need for tuning the model Ô¨Çexibility, which eÔ¨Äectively is the purpose of regularization. Finding the right level of Ô¨Çexibility, and thereby avoiding overÔ¨Åt, is very important in practice.
The idea of regularization in a parametric model is to ‚Äòkeep the parameters ùúΩ small unless the data really convinces us otherwise‚Äô, or alternatively ‚Äòif a model with small values of the parameters ùúΩ Ô¨Åts the data almost as well as a model with larger parameter values, the one with small parameter values should be preferred‚Äô. There are, however, many diÔ¨Äerent ways to implement this idea and we distinguish between explicit regularization and implicit regularization. We will Ô¨Årst discuss explicit regularization, which amounts to modifying the cost function, and in particular so-called ùêø2 and ùêø1 regularization.

ùêø2 regularization

The ùêø2 regularization (also known as Tikhonov regularization, ridge regression and weight decay) amounts

to adding an extra ùêø2 regularization,

penalty term

ùúΩ

2 2

to

the

cost

function.

as an example, amounts to solving

Linear

regression

with

squared

error

loss

and

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

XùúΩ ‚àí y

2 2

+

ùúÜ

ùúΩ

2 2

.

(5.23)

By choosing the regularization parameter ùúÜ ‚â• 0, a trade-oÔ¨Ä between the original cost function (Ô¨Åtting the training data as well as possible) and the regularization term (keeping the parameters ùúΩ close to zero) is made. In the setting ùúÜ = 0 we recover the original least squares problem (3.12), whereas ùúÜ ‚Üí ‚àû will force all parameters ùúΩ to 0. A good choice of ùúÜ is usually somewhere in between, depending on the actual problem, and can be selected using cross-validation.
It is actually possible to derive a version of the normal equations for (5.23), namely

(XTX + ùëõùúÜIùëù+1)ùúΩ = XTy,

(5.24)

where Iùëù+1 is the identity matrix of size ( ùëù + 1) √ó ( ùëù + 1). For ùúÜ > 0, the matrix XTX + ùëõùúÜIùëù+1 is always invertible, and we have the closed form solution

ùúΩ = (XTX + ùëõùúÜIùëù+1)‚àí1XTy.

(5.25)

This also reveals another reason for using regularization in linear regression, namely if XTX is not invertible. When XTX is not invertible, the ordinary normal equations (3.13) have no unique solution ùúΩ, whereas the ùêø2-regularized version always has the unique solution (5.25) if ùúÜ > 0.

ùêø1 regularization

With ùêø1 regularization (also called LASSO, an abbreviation for Least Absolute Shrinkage and Selection Operator), the penalty term ùúΩ 1 is added to the cost function. Here ùúΩ 1 is the 1-norm or ‚Äòtaxicab norm‚Äô
ùúΩ 1 = |ùúÉ0| + |ùúÉ1| + ¬∑ ¬∑ ¬∑ + |ùúÉ ùëù |. The ùêø1 regularized cost function for linear regression (with squared error loss) then becomes

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

XùúΩ ‚àí y

2 2

+

ùúÜ

ùúΩ

1.

(5.26)

Contrary to linear regression with ùêø2 regularization (3.48), there is no closed-form solution available for (5.26). However, as we will see in Section 5.4, it is possible to design an eÔ¨Écient numerical optimization algorithm for solving (5.26).

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

93

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models

As for ùêø2 regularization, the regularization parameter ùúÜ has to be chosen by the user, and has a similar meaning: ùúÜ = 0 gives the ordinary least squares solution and ùúÜ ‚Üí ‚àû gives ùúΩ = 0. Between these extremes, however, ùêø1 and ùêø2 tend to give diÔ¨Äerent solutions. Whereas ùêø2 regularization pushes all parameters towards small values (but not necessarily exactly zero), ùêø1 tends to favor so-called sparse solutions where only a few of the parameters are non-zero, and the rest are exactly zero. Thus, ùêø1 regularization can
eÔ¨Äectively ‚Äòswitch oÔ¨Ä‚Äô some inputs (by setting the corresponding parameter ùúÉùëò to zero) and it can therefore
be used as an input (or feature) selection method.

Example 5.2: Regularization for car stopping distance
Consider again Example 2.2 with the car stopping distance regression problem. We use the 10th order polynomial that was considered meaningless in Example 3.5 and apply ùêø2 and ùêø1 regularization to it, respectively. With manually chosen ùúÜ, we obtain the following models

ùêø2 regularization
150

ùêø1 regularization
150

Distance (feet)

100

100

50

50

0

Fig.

0

10

20

30

40

5.3

Speed (mph)

0

0

10

20

30

40

Speed (mph)

Both models suÔ¨Äer less from overÔ¨Åtting than the non-regularized 10th order polynomial in Example 3.5.
The two models here are, however, not identical. Whereas all parameters are relatively small but non-zero in the ùêø2-regularized model (left panel), only 4 (out of 11) parameters are non-zero in the ùêø1-regularized model (right panel). It is typical for ùêø1 regularization to give sparse models, where some parameters are set
exactly to zero.

General explicit regularization
The ùêø1 and ùêø2 regularization are two common examples of what we refer to as explicit regularization since they are both formulated as modiÔ¨Åcations of the cost function. They suggest a general pattern on which explicit regularization can be formulated,

ùúΩ = arg min ùêΩ (ùúΩ; X, y) + ùúÜ ùëÖ(ùúΩ) .
ùúΩ

(i)

(iii) (ii)

(5.27)

This expression contains three important elements:

(i) the cost function, which encourages a good Ô¨Åt to training data, (ii) the regularization term, which encourages small parameter values, and (iii) the regularization parameter ùúÜ, which determines the trade-oÔ¨Ä between (i) and (ii).

In this view, it is clear that explicit regularization modiÔ¨Åes the problem of Ô¨Åtting to the training data
(minimizing ùê∏train) into something else, which hopefully rather minimizes ùê∏new. The actual design of the regularization term ùëÖ(ùúΩ) can be done in many ways. As a combination of the ùêø1 and ùêø2 terms, one option is ùëÖ(ùúΩ) = ùúΩ 1 + ùúΩ 22, which often is referred to as elastic net regularization. Regardless of the exact expression of the regularization term, its purpose is to encourage small parameter values and thereby
decrease the Ô¨Çexibility of the model, which might improve the performance and lower ùê∏new.

94

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Regularization
Implicit regularization
Any supervised machine learning method that is trained by minimizing a cost function can be regularized as (5.27). There are, however, alternative ways to achieve a similar eÔ¨Äect without explicitly modifying the cost function. One such example of implicit regularization is early stopping. Early stopping is applicable to any method that is trained using iterative numerical optimization, which is the topic of the next section. It amounts to aborting the optimization before it has reached the minimum of the cost function. Although it may appear counter-intuitive to prematurely abort an optimization procedure, it has proven useful in practice and early stopping has shown to be a of practical importance to avoid overÔ¨Åtting for some models, most notably deep learning (Chapter 6). Early stopping can be implemented by setting aside some hold-out validation data and computing ùê∏hold-out as in (4.6) for ùúΩ (ùë°) after each iteration ùë° of the numerical optimization7. It is typically observed that ùê∏hold-out decreases initially but reaches eventually a minimum and thereafter starts to increase, even though the cost function (by design of the optimization algorithm) decreases monotonically. The optimization is then aborted at the point when ùê∏hold-out reached its minimum, as we later will illustrate in Example 5.7.
Early stopping is a commonly used implicit regularization technique, but not the only one. Another technique with a regularizing eÔ¨Äect is dropout for neural networks, which we discuss in Chapter 6, and data augmentation, which we discuss in Chapter 11. For decision trees the splitting criteria can be seen as a type of implicit regularization. It has also been argued that the randomness of the stochastic gradient optimization algorithm in itself also has the eÔ¨Äect of implicit regularization.

7More practically, to reduce the computational overhead of early stopping, we can compute the validation error at regular intervals, for instance after each epoch.

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

95

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models
5.4 Parameter optimization
Many supervised machine learning methods, linear and logistic regression included, involves one (or more) optimization problems, such as (3.12), (3.35) or (5.26). A machine learning engineer therefore needs to be familiar with the main strategies for how to solve optimization problems fast. Starting in the optimization problems from linear and logistic regression, we will introduce the ideas behind some of the optimization methods commonly used in supervised machine learning. This section only gives a brief introduction to optimization theory and, for example, we will only discuss unconstrained optimization problems.
Optimization is about Ô¨Ånding the minimum or maximum of an objective function. Since the maximization problem can be formulated as minimization of the negative objective function, we can limit ourselves to minimization without any loss of generality.
There are primarily two ways in which optimization is used in machine learning:
1. For training a model by minimizing the cost function with respect to the model parameters ùúΩ. In this case the objective function corresponds to the cost function ùêΩ (ùúΩ), and the optimization variables correspond to the model parameters.
2. For tuning hyperparameters, such as the regularization parameter ùúÜ. For instance, by using a held-out validation dataset (see Chapter 4) we can select ùúÜ to minimize the hold-out validation error ùê∏hold-out. In this case, the objective function is the validation error, and the optimization variables correspond to the hyperparameters.
In the presentation below we will use ùúΩ to denote a general optimization variable, but keep in mind that optimization can also be used for selecting hyperparameters.
An important class of objective functions are convex functions. Optimization is often easier to carry out for convex objective functions, and it is a general advise to spend some extra eÔ¨Äort to consider whether a non-convex optimization problem can be re-formulated into a convex problem (which sometimes, but not always, is possible). The most important property of a convex function, for this discussion, is that a convex function has a unique minimum8, and no other local minima. Examples of convex functions are the cost functions for logistic regression, linear regression and ùêø1-regularized linear regression. An example of non-convex functions is the cost function for a deep neural network. We illustrate by Example 5.3.
Example 5.3: Example of objective functions
Figure 5.4 contains examples of what an objective function can look like.

Objective function Objective function

2
1 1

Fig. 5.4

0 ‚àí5
0
ùúÉ1

‚àí4 ‚àí2

0

2
ùúÉ2

4

‚àí5 0
ùúÉ1

‚àí4 ‚àí2

0

2
ùúÉ2

4

Both examples are functions of a two-dimensional parameter vector ùúΩ = [ùúÉ1 ùúÉ2]T. The left is convex and has a Ô¨Ånite unique global minimum, whereas the right is non-convex and has three local minima (of which only one is the global minimum). We will in the following examples illustrate these objective functions using contour plots instead, as shown in Figure 5.5.

8The minimum does, however, not have to be Ô¨Ånite. The exponential function, for example, is convex but attains its minimum in ‚àí‚àû. Convexity is a relatively strong property, and also non-convex functions may have only one minimum.

96

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Parameter optimization

5

5

ùúÉ2 ùúÉ2

0

0

‚àí5

Fig.

‚àí5

0

5

5.5

ùúÉ1

‚àí5

‚àí5

0

5

ùúÉ1

Time to reÔ¨Çect 5.2: After reading the rest of this book, return here and try to Ô¨Åll out this table, summarizing how optimization is used by the diÔ¨Äerent methods.

Method
ùëò -NN Trees Linear regression Linear regression with L2-regularization Linear regression with L1-regularization Logistic regression Deep learning Random forests AdaBoost Gradient boosting Gaussian processes *including coordinate descent

What is optimization used for?

Training

Hyperparameters

Nothing

Closedform*

What type of optimization?

Grid search

Gradientbased

Stochastic gradient descent

Optimization using closed-form expressions

For linear regression with squared error loss, training the model amounts to solving the optimization problem (3.12)

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

XùúΩ ‚àí y

22 .

As we have discussed, and also proved in Appendix 3.A, the solution (3.14) to this problem can (under the assumption that XTX is invertible) be derived analytically. If we only spend some time to eÔ¨Éciently implement (3.14) once, for example using the Cholesky or QR factorization, we can use that every time
we want to train a linear regression model with squared error loss. Each time we use it we know that we
have found the optimal solution in a computationally eÔ¨Écient way. If we instead want to learn the ùêø1-regularized version, we have to solve (5.26)

ùúΩ

=

arg

min
ùúΩ

1 ùëõ

XùúΩ ‚àí y

2 2

+

ùúÜ

ùúΩ

1.

This problem can, unfortunately, not be solved analytically. Instead we have to use computer power to solve it, by constructing an iterative procedure for seeking the solution. With a certain choice of such an optimization algorithm, we can make use of some analytical expressions along the way, which turns out to give an eÔ¨Écient way of solving it. Remember that ùúΩ is a vector containing ùëù + 1 parameters we want to learn from the training data. As it turns out, if we seek the minimum for only one of these parameters, say ùúÉ ùëó, while keeping the other parameters Ô¨Åxed, we can Ô¨Ånd the optimum as

arg

min
ùúÉùëó

1 ùëõ

XùúΩ ‚àí y

2 2

+

ùúÜ

ùúΩ

‚àëùëõÔ∏Å

‚àëÔ∏Å

1 = sign(ùë°) (|ùë° | ‚àí ùúÜ), where ùë° = ùë•ùëñ ùëó (ùë¶ùëñ ‚àí ùë•ùëñùëò ùúÉùëò ).

ùëñ=1

ùëò‚â† ùëó

(5.28)

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

97

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models
It turns out that making repeated ‚Äúsweeps‚Äù through the vector ùúΩ and updating one parameter at a time according to (5.28) is a good way to solve (5.26). This type of algorithm, where we update one parameter at a time, is referred to as coordinate descent, and we illustrate it in Example 5.4.
It can be shown that the cost function in (5.26) is convex. Only convexity is not suÔ¨Écient to guarantee that coordinate descent will Ô¨Ånd its (global) minimum, but for the ùêø1-regularized cost function (5.26) it can be shown that coordinate descent actually Ô¨Ånds the (global) minimum. In practice we know that we have found the global minimum when no parameters have changed during a full ‚Äúsweep‚Äù of the parameter vector.
It turns out that coordinate descent is a very eÔ¨Écient method for ùêø1-regularized linear regression (5.26). The keys are (i) that (5.28) exists and is cheap to compute, and (ii) many updates will simply set ùúÉ ùëó = 0 due to the sparsity of the optimal ùúΩ. This makes the algorithm fast. For most machine learning optimization problems it can, however, not be said that coordinate descent is the preferred method. We will now have a look at some more general families of optimization methods that are widely used in machine learning.
Example 5.4: Coordinate descent
We apply coordinate descent to the objective functions from Example 5.3 and show the result in Figure 5.6. For coordinate descent to be an eÔ¨Écient alternative in practice, closed-form solutions for updating one parameter at a time, similar to (5.28), have to be available.

5

5

ùúÉ2 ùúÉ2

0

0

‚àí5

Fig.

‚àí5

0

5

5.6

ùúÉ1

‚àí5

‚àí5

0

5

ùúÉ1

Figure 5.6 shows how the parameters are updated in the coordinate descent algorithm, for two diÔ¨Äerent initial parameter vectors (blue and green trajectory, respectively). It is clear from the Ô¨Ågure that only one parameter is updated each time, which gives the trajectory a characteristic shape. The obtained minimum is marked with a yellow dot. Note how the diÔ¨Äerent initializations lead to diÔ¨Äerent (local) minima in the non-convex case (right panel).

Gradient descent

In many situation we can not do closed-form manipulations, but we do have access to the value of the objective function as well as its derivative (or rather, the gradient). Sometimes we even have access to the second derivative (or rather, the Hessian). In those situations, it is often a good idea to use a gradient descent method, which we will introduce now, or even Newton‚Äôs method that we will discuss later.
Gradient descent can be used for learning parameter vectors ùúΩ of high dimension when the objective function ùêΩ (ùúΩ) is simple enough such that its gradient is possible to compute. Let us therefore consider the parameter learning problem

ùúΩ = arg min ùêΩ (ùúΩ)
ùúΩ

(5.29)

(even though gradient descent possibly can be used for hyperparameters as well). We will assume9 that

9This assumption is primarily made for the theoretical discussion. In practice, there are successful examples of gradient descent being applied to objective functions not diÔ¨Äerentiable everywhere, such as neural networks with ReLu activation functions (Chapter 6).

98

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

. Parameter optimization

the gradient of the cost function ‚àáùúΩ ùêΩ (ùúΩ) exists for all ùúΩ. As an example, the gradient of the cost function for logistic regression (3.34) is

‚àáùúΩ ùêΩ (ùúΩ)

=

‚àí

1 ùëõ

‚àëùëõÔ∏Å
ùëñ=1

1 1 + ùëíùë¶ùëñùúΩTxùëñ

ùë¶ùëñxùëñ .

(5.30)

Note that ‚àáùúΩ ùêΩ (ùúΩ) is a vector of the same dimension as ùúΩ, which describes the direction in which ùêΩ (ùúΩ) increases. Consequently, and more useful for us, ‚àí‚àáùúΩ ùêΩ (ùúΩ) describes the direction in which ùêΩ (ùúΩ) decreases. That is, if we take a small step in the direction of the negative gradient, this will reduce the value of the cost function,

ùêΩ ùúΩ ‚àí ùõæ‚àáùúΩ ùêΩ (ùúΩ) ‚â§ ùêΩ ùúΩ

(5.31)

for some (possibly very small) ùõæ > 0. If ùêΩ (ùúΩ) is convex, the inequality in (5.31) is strict except at the minimum (where ‚àáùúΩ ùêΩ (ùúΩ) is zero). This suggests that if we have ùúΩ (ùë°) and want to select ùúΩ (ùë°+1) such that ùêΩ (ùúΩ (ùë°+1) ) ‚â§ ùêΩ (ùúΩ (ùë°) ), we should

update ùúΩ (ùë°+1) = ùúΩ (ùë°) ‚àí ùõæ‚àáùúΩ ùêΩ (ùúΩ (ùë°) ) with some positive ùõæ > 0. Repeating (5.32) gives the gradient descent Algorithm 5.1.

(5.32)

Algorithm 5.1: Gradient descent
Input: Objective function ùêΩ (ùúΩ), initial ùúΩ (0) , learning rate ùõæ Result: ùúΩ 1 Set ùë° ‚Üê 0 2 while ùúΩ (ùë°) ‚àí ùúΩ (ùë°‚àí1) not small enough do 3 Update ùúΩ (ùë°+1) ‚Üê ùúΩ (ùë°) ‚àí ùõæ‚àáùúΩ ùêΩ (ùúΩ (ùë°) ) 4 Update ùë° ‚Üê ùë° + 1
5 end 6 return ùúΩ ‚Üê ùúΩ (ùë°‚àí1)

This material will be published by Cambridge University Press. This pre-publication version is free to view

and download for personal use only. Not for re-distribution, re-sale or use in derivative works.

99

¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

Learning parametric models

ùêΩ(ùúÉ)

ùúÉ (a) Low learning rate ùõæ = 0.05

ùúÉ (b) High learning rate ùõæ = 1.2

ùúÉ (c) Good learning rate ùõæ = 0.3

Figure 5.7: Optimization using gradient descent of a cost function ùêΩ (ùúÉ) where ùúÉ is a scalar parameter. In the
diÔ¨Äerent subÔ¨Ågures we use a too low learning rate (a), a too high learning rate (b), and a good learning rate (c). Remember that a good value of ùõæ is very much related to the shape of the cost function; ùõæ = 0.3 might be too small (or high) for a diÔ¨Äerent ùêΩ (ùúÉ).

In practice we do not know ùõæ, which determines how big the ùúΩ-step is at each iteration. It is possible to formulate the selection of ùõæ as an internal optimization problem that is solved at each iteration, a so-called line-search problem. This will result in a possibly diÔ¨Äerent value for ùõæ at each iteration of the algorithm. Here we will consider the simpler solution where we leave the choice of ùõæ to the user, or more speciÔ¨Åcally view it as a hyperparameter10. In such cases, ùõæ is often referred to as the learning rate or step-size. Note that the gradient ‚àáùúΩ ùêΩ (ùúΩ) will typically decrease and eventually attain 0 at a stationary point (possibly, but not necessarily, a minimum), so Algorithm 5.1 may converge if ùõæ is kept constant. This is in contrast to what we later will discuss when we introduce the stochastic gradient algorithm.
The choice of learning rate ùõæ is important. Some typical situations with too small, too high and a good choice of learning rate are shown in Figure 5.7. With the intuition from these Ô¨Ågures, we advise to monitor ùêΩ (ùúΩ (ùë°) ) during the optimization, and
‚Ä¢ decrease the learning rate ùõæ if the cost function values ùêΩ (ùúÉ (ùë°) ) are getting worse or oscillates widely (as in Figure 5.7b),
‚Ä¢ increase the learning rate ùõæ if the cost function values ùêΩ (ùúÉ (ùë°) ) are fairly constant and only slowly decreasing (as in Figure 5.7a).
No general convergence guarantees can be given for gradient descent, basically because a bad learning rate ùõæ may break the method. However, with the ‚Äúright‚Äù choice of ùõæ, the value of ùêΩ (ùúΩ) will decrease for each iteration (as suggested by (5.31)) until a point with zero gradient is found, that is, a stationary point. A stationary point is, however, not necessarily a minimum, but can also be a maximum or a saddle-point of the objective function. In practice one typically monitor the value of ùêΩ (ùúΩ) and terminate the algorithm when it seems not to decrease anymore, and hope it has arrived at a minimum.
In non-convex problems with multiple local minima, we can not expect gradient descent to always Ô¨Ånd the global minimum. The initialization is usually critical for determining which minimum (or stationary point) that is found, as illustrated by Example 5.5. It can therefore be a good practice (if time and computational resources permit) to run the optimization multiple times with diÔ¨Äerent initializations. For computationally heavy non-convex problems such as training a deep neural network (Chapter 6) when we cannot aÔ¨Äord to re-run the training, we usually employ method-speciÔ¨Åc heuristics and tricks to Ô¨Ånd a good initialization point.
For convex problems there is only one stationary point, which also is the global minimum. Hence, the initialization for convex problem can be done arbitrarily. However, by warm-starting the optimization
10When viewed as a hyperparmeter we can also optimize ùõæ, for instance by using cross-validation, as discussed above. However, this is an ‚Äúexternal‚Äù optimization problem, contrary to line-search which is an ‚Äúinternal‚Äù optimization problem.

100

Draft April ,

of Machine Learning ‚Äì A First Course for Engineers and Scientists. http://smlbook.org ¬© Andreas Lindholm, Niklas Wahlstr√∂m, Fredrik Lindsten, and Thomas B. Sch√∂n .

