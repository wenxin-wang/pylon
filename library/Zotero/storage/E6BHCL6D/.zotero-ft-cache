Department of Computer Science and Engineering
Flexible Operating System Internals: The Design and Implementation of the Anykernel and Rump Kernels
Antti Kantee
DOCTORAL DISSERTATIONS

"BMUP6OJWFSTJUZQVCMJDBUJPOTFSJFT %0$503"-%*44&35"5*0/4
&2#&ŗ*,.#(!ŗ3-.'ŗ (.,(&-Ćŗ "ŗ-#!(ŗ(ŗ '*&'(..#)(ŗ)ŗ."ŗ (3%,(&ŗ(ŗ/'*ŗ,(&-ŗ
"OUUJ,BOUFF
"EPDUPSBMEJTTFSUBUJPODPNQMFUFEGPSUIFEFHSFFPG%PDUPSPG 4DJFODFJO5FDIOPMPHZUPCFEFGFOEFEXJUIUIFQFSNJTTJPOPGUIF "BMUP6OJWFSTJUZ4DIPPMPG4DJFODFBUBQVCMJDFYBNJOBUJPOIFMEBU UIFMFDUVSFIBMM5PGUIFTDIPPMPO%FDFNCFSBUOPPO "BMUP6OJWFSTJUZ 4DIPPMPG4DJFODF %FQBSUNFOUPG$PNQVUFS4DJFODFBOE&OHJOFFSJOH

4VQFSWJTJOHQSPGFTTPS 1SPGFTTPS)FJLLJ4BJLLPOFO 1SFMJNJOBSZFYBNJOFST %S.BSTIBMM,JSL.D,VTJDL64" 1SPGFTTPS3FO[P%BWPMJ6OJWFSTJUZPG#PMPHOB*UBMZ 0QQPOFOU %S1FUFS5SÍHFS)BTTP1MBUUOFS*OTUJUVUF(FSNBOZ
"BMUP6OJWFSTJUZQVCMJDBUJPOTFSJFT %0$503"-%*44&35"5*0/4 "OUUJ,BOUFFQPPLB!JLJGJ  1FSNJTTJPOUPVTFDPQZBOEPSEJTUSJCVUFUIJTEPDVNFOUXJUIPS XJUIPVUGFFJTIFSFCZHSBOUFEQSPWJEFEUIBUUIFBCPWFDPQZSJHIU OPUJDFBOEUIJTQFSNJTTJPOOPUJDFBQQFBSJOBMMDPQJFT%JTUSJCVUJOH NPEJGJFEDPQJFTJTQSPIJCJUFE *4#/ QSJOUFE
 *4#/ QEG
 *44/- *44/ QSJOUFE
 *44/ QEG
 IUUQVSOGJ63/*4#/ 6OJHSBGJB0Z )FMTJOLJ 'JOMBOE

"CTUSBDU
"BMUP6OJWFSTJUZ10#PY'*"BMUPXXXBBMUPGJ

"VUIPS (..#ŗ(.ŗ
/BNFPGUIFEPDUPSBMEJTTFSUBUJPO &2#&ŗ*,.#(!ŗ3-.'ŗ (.,(&-Ćŗ "ŗ-#!(ŗ(ŗ '*&'(..#)(ŗ)ŗ."ŗ(3%,(&ŗ(ŗ/'*ŗ,(&-ŗ

1VCMJTIFS "))&ŗ)ŗ#(ŗ

6OJU *,.'(.ŗ)ŗ)'*/.,ŗ#(ŗ(ŗ(!#(,#(!ŗ

4FSJFT &.)ŗ(#0,-#.3ŗ*/&#.#)(ŗ-,#-ŗŗŗûāûĞüúûüŗ

'JFMEPGSFTFBSDI ).1,ŗ3-.'-ŗ

.BOVTDSJQUTVCNJUUFE üûŗ/!/-.ŗüúûüŗ

%BUFPGUIFEFGFODF āŗ',ŗüúûüŗ

1FSNJTTJPOUPQVCMJTIHSBOUFE EBUF
 ăŗ.),ŗüúûüŗ

-BOHVBHF (!&#-"ŗ

.POPHSBQI

"SUJDMFEJTTFSUBUJPO TVNNBSZPSJHJOBMBSUJDMFT


"CTUSBDU
"ŗ')()&#."#ŗ%,(&ŗ,"#../,ŗ#-ŗ-#!(#ð(.ŗ#(ŗ."ŗ,&ŗ1),&ŗ/ŗ.)ŗ."ŗ&,!ŗ')/(.ŗ )ŗ1),%#(!ŗ(ŗ*,)0(ŗ)Ąŗ)10,ąŗ."ŗ,"#../,ŗ#-ŗ().ŗ1#.")/.ŗ*,)&'-Ćŗ.-.#(!ŗ(ŗ 0&)*'(.ŗ#-ŗ# ð/&.ąŗ0#,./&#4#(!ŗ%,(&ŗ-,0#-ŗ(ŗŗ)(ŗ)(&3ŗ3ŗ/*&#.#(!ŗ."ŗ(.#,ŗ %,(&ąŗ(ŗ-/,#.3ŗ#-ŗ1%ŗ/ŗ.)ŗŗ-#(!&ŗ)'#(ŗ1",ŗ&&ŗ)ŗ"-ŗ#,.ŗ--ŗ.)ŗ 0,3."#(!Ąŗ&.,(.ŗ%,(&ŗ,"#../,-ŗ-/"ŗ-ŗ."ŗ'#,)%,(&ŗ(ŗ2)%,(&ŗ"0ŗ(ŗ *,)*)-ŗ.)ŗ,.# 3ŗ."-ŗ*,)&'-ŗ1#."ŗ')()&#."#ŗ%,(&-Ąŗ)10,ąŗ&.,(.ŗ-3-.'ŗ -.,/./,-ŗ)ŗ().ŗ,--ŗ."ŗ)'')(ŗ-#,ŗ)ŗ/-#(!ŗŗ')()&#."#ŗ%,(&ŗ1"(ŗ."ŗ )0'(.#)(ŗ*,)&'-ŗ)ŗ().ŗ**&3Ąŗ ŗ ŗ*,)*)-ŗŗó2#&ŗ(3%,(&ŗ,"#../,ŗ1"#"ŗ(&-ŗ,/((#(!ŗ%,(&ŗ,#0,-ŗ#(ŗŗ0,#.3ŗ )ŗ)(ð!/,.#)(-ąŗ2'*&-ŗ)ŗ1"#"ŗ#(&/ŗ'#,)%,(&Ě-.3&ŗ-,0,-ŗ(ŗ**&#.#)(ŗ &#,,#-Ąŗŗ')()&#."#ŗ%,(&ŗ#-ŗ-")1(ŗ.)ŗŗ)(0,.#&ŗ#(.)ŗ(ŗ(3%,(&ŗ1#."ŗŗ,-)(&ŗ ')/(.ŗ)ŗ ),.ŗ(ŗ1#.")/.ŗ#(.,)/#(!ŗ*, ),'(Ě"#(,#(!ŗ#(#,.#)(ŗ&3,-Ąŗ"ŗ ),#!#(&ŗ')()&#."#ŗ')ŗ)ŗ)*,.#)(ŗ#-ŗ*,-,0ŗ .,ŗ."ŗ(3%,(&ŗ$/-.'(.-ąŗ(ŗ &.,(.ŗ')-ŗ)ŗ)*,.#)(ŗ ),ŗ,#0,-ŗ,ŗ0#&&ŗ-ŗŗ,/(.#'ŗ")#Ąŗ),ŗ()(Ě')()&#."#ŗ ')-ŗ)ŗ)*,.#)(ąŗ."ŗ,/'*ŗ%,(&ŗ#-ŗ#(.,)/ŗ-ŗŗ&#!".1#!".ŗ)(.#(,ŗ ),ŗ,#0,-Ąŗŗ ,/'*ŗ%,(&ŗ,/(-ŗ)(ŗ.)*ŗ)ŗŗ"3*,0#-),ŗ1"#"ŗ) ,-ŗ"#!"Ě&0&ŗ*,#'#.#0-ŗ-/"ŗ-ŗ.",ŗ -"/&#(!ŗ(ŗ0#,./&ŗ''),3Ąŗŗ*,)/.#)(ŗ+/&#.3ŗ#'*&'(..#)(ŗ ),ŗ."ŗ.ŗ)*(ŗ -)/,ŗŗ"-ŗ(ŗ)(Ąŗ"ŗ(3%,(&ŗ,"#../,ŗ(ŗ,/'*ŗ%,(&-ŗ,ŗ0&/.ŗ)."ŗ !#(-.ŗ )/,ŗ3,-ŗ)ŗ,&Ě1),&ŗ2*,#(ŗ ,)'ŗ#&3ŗ.ŗ0&)*'(.ŗ-ŗ1&&ŗ-ŗ!#(-.ŗ -3(.".#ŗ("',%-Ąŗ

,FZXPSET )*,.#(!ŗ-3-.'ŗ%,(&ŗ,"#../,ąŗ#'*&'(..#)(ąŗ)*(ŗ-)/,ŗ

*4#/ QSJOUFE
 ăāĂĚăÿüĚĀúĚþăûĀĚĂŗ

*4#/ QEG
 ăāĂĚăÿüĚĀúĚþăûāĚÿŗ

*44/- ûāăăĚþăýþŗ

*44/ QSJOUFE
 ûāăăĚþăýþŗ

*44/ QEG
 ûāăăĚþăþüŗ

-PDBUJPOPGQVCMJTIFS -*))ŗ

-PDBUJPOPGQSJOUJOH &-#(%#ŗ

:FBS üúûüŗ

1BHFT ýÿĂŗ

VSO "..*ĆĞĞ/,(ĄðĞĆĆăāĂĚăÿüĚĀúĚþăûāĚÿŗ

5JJWJTUFMN»
"BMUPZMJPQJTUP1-"BMUPXXXBBMUPGJ

5FLJK» (..#ŗ(.ŗ

7»JUÍTLJSKBOOJNJ 
)/-.0.ŗ%<3..$<,$-.&'<.Ćŗ
)%3.#'(ŗ$ŗ3(%<3.#'#(ŗ-//((#..&/ŗ$ŗ.)././-ŗ

+VMLBJTJKB ,/-.#.#(ŗ%),%%)/&/ŗ

:LTJLLÍ #.).%(##%(ŗ&#.)-ŗ

4BSKB &.)ŗ(#0,-#.3ŗ*/&#.#)(ŗ-,#-ŗŗŗûāûĞüúûüŗ

5VULJNVTBMB "$&'#-.)$<,$-.&'<.ŗ

,»TJLJSKPJUVLTFOQWN üûĄúĂĄüúûüŗ

7»JUÍTQ»JW» úāĄûüĄüúûüŗ

+VMLBJTVMVWBONZÍOU»NJTQ»JW» úăĄûúĄüúûüŗ ,JFMJ (!&(.#ŗ

.POPHSBGJB

:IEJTUFMN»W»JUÍTLJSKB ZIUFFOWFUPPTBFSJMMJTBSUJLLFMJU


5JJWJTUFMN»
)()&##..#-(ŗ3.#'(ŗ%<3..$<,$-.&'<.ŗ)0.ŗ',%#..<0#<ąŗ%)-%ŗ(ŗ-#-<&.<0<.ŗ-//,(ŗ'<<,<(ŗ )&'--)&0#ŗ$ŗ.-...$ŗ$/,#.Ąŗ)()&##..#((ŗ,%%#.".//,#ŗ#ŗ%/#.(%(ŗ)&ŗ)(!&'.)(Ćŗ 0#,./&#-)#(.#ŗ)((#-.//ŗ0#(ŗ0#,./&#-)#'&&ŗ3#(ŗ%)%)(#-//.(ąŗ3.#'(ŗ.-./-ŗ)(ŗ0#%ŗ $ŗ.#.)./,0ŗ)(ŗ"#%%)ŗ$)"./(ŗ-##.<ąŗ..<ŗ3.#'--<ŗ-/),#...0&&ŗ)"$&'#-.)&&ŗ)(ŗ-/),ŗ*<<-3ŗ %<3..$<,$-.&'<(ŗ%#%%##(ŗ)-##(Ąŗ#".)".)#-#ŗ3#(,%%#.".//,#.ąŗ%/.(ŗ'#%,)3#(ŗ$ŗ %-)3#(ąŗ)(ŗ").../ŗ%),$'(ŗ')()&##..#-(ŗ,%%#.".//,#(ŗ)(!&'#Ąŗ#".)".)#-#--ŗ '&&#--ŗ)(ŗ%/#.(%#(ŗ)(!&'(ŗ-ąŗ..#0<.ŗ(ŗ./ŗ3&#-.<ŗ"&/ŗ%<3..<<ŗ')()&##..#-.ŗ3#(.<ąŗ %/(ŗ3&&<'#(#./.ŗ)(!&'.ŗ#0<.ŗ)&ŗ,&0(..$Ąŗ ŗ <--<ŗ.3 --<ŗ&/)(ŗ$)/-.0ŗ$)%3#(Ąŗŗ'")&&#-.ŗ$/,#(ŗ-/),#..'#-(ŗ')(#--ŗ %)(ð!/,.#)#--ąŗ-#',%%$<ŗ$)#-.ŗ)0.ŗ'#%,)3#(Ě.33&#-.ŗ*&0&#'.ŗ$ŗ -)0&&/-%#,$-.).Ąŗ)()&##..#-(ŗ3.#'(ŗ'//(.'#-(ŗ$)%3.#'%-#ŗ(<3..<<(ŗ)((#-./0(ŗ -<<3&&#-&&<ŗ0#0&&ŗ#&'(ŗ-/),#./-%3%3<ŗ"#%(.<0#<ŗ-.,%.#)#.Ąŗ")&&#-//-ŗ-/),#..ŗ $/,#.ŗ')()&##..#---<ŗ3.#'--<ŗ-<#&33ŗ$ŗ'//.ŗ-/),#./-%)(ð!/,.#).ŗ.,$).(ŗ$)(#%#-(ŗ 0&#(.(Ąŗ/#.ŗ%/#(ŗ')()&##..#-#-.ŗ-/),#./-%)(ð!/,.#).ŗ0,.(ŗ'<<,#.&&<<(ŗ.3(%<3#(ąŗ $)%ŗ)(ŗ$)3'*<,#-.ŗ$/,#&&Ąŗ3(%<3.#'(ŗ-/),#./-ŗ.*".//ŗ"3*,%#".#'(ŗ *<<&&<Ąŗ3*,%#"#(ŗ.,$)ŗ.3(%<3.#'&&ŗ%),%(ŗ.-)(ŗ*&0&/#.ŗ%/.(ŗ-<#-%/&)#((#(ŗ $ŗ0#,./&#'/#-.#(Ąŗ)././-ŗ)(ŗ.".3ŗ./).(.).-)#-(ŗ.Ě(#'#-&&ŗ0)#'(ŗ &<"%))#(ŗ%<3..$<,$-.&'<&&Ąŗ
)%3#(,%%#.".//,#ŗ$ŗ.3(%<3.#'.ŗ,0#)#(ŗ-%<ŗ(&$<(ŗ 0/)(ŗ,&#%)%'/%-(ŗ..<ŗ-3(...#-.(ŗ%)%#(ŗ*,/-.&&Ąŗ ŗ

"WBJOTBOBU %<3..$<,$-.&'<3#(,%%#.".//,#ąŗ.)././-ąŗ0)#(ŗ&<"%))#ŗ

*4#/ QBJOFUUV
 ăāĂĚăÿüĚĀúĚþăûĀĚĂŗ

*4#/ QEG
 ăāĂĚăÿüĚĀúĚþăûāĚÿŗ

*44/- ûāăăĚþăýþŗ

*44/ QBJOFUUV
 ûāăăĚþăýþŗ

*44/ QEG
 ûāăăĚþăþüŗ

+VMLBJTVQBJLLB -*))ŗ

1BJOPQBJLLB &-#(%#ŗ

7VPTJ üúûüŗ

4JWVN»»S» ýÿĂŗ

VSO "..*ĆĞĞ/,(ĄðĞĆĆăāĂĚăÿüĚĀúĚþăûāĚÿŗ

7
To the memory of my grandfather, Lauri Kantee

8

9
Preface
Meet the new boss Same as the old boss – The Who
The document you are reading is an extension of the production quality implementation I used to verify the statements that I put forth in this dissertation. The implementation and this document mutually support each other, and cross-referencing one while studying the other will allow for a more thorough understanding of this work. As usually happens, many people contributed to both, and I will do my best below to account for the ones who made major contributions.
In the professor department, my supervisor Heikki Saikkonen has realized that ﬁguring things out properly takes a lot of time, so he did not pressure me for status updates when there was nothing new worth reporting. When he challanged me on my writing, his comments were to the point. He also saw the connecting points of my work and put me in touch with several of my later key contacts.
At the beginning of my journey, I had discussions with Juha Tuominen, the professor I worked for during my undergraduate years. Our discussions about doctoral work were on a very high level, but I found myself constantly going back to them, and I still remember them now, seven years later.
I thank my preliminary examiners, Kirk McKusick and Renzo Davoli, for the extra eﬀort they made to fully understand my work before giving their verdicts. They not only read the manuscript, but also familiarized themselves with the implementation and manual pages.

10
My friend and I daresay mentor Johannes Helander inﬂuenced this work probably more than any other person. The previous statement is somewhat hard to explain, since we almost never talked speciﬁcally about my work. Yet, I am certain that the views and philosophies of Johannes run deep throughout my work.
Andre Dolenc was the ﬁrst person to see the potential for using this technology in a product. He also read some of my earlier texts and tried to guide me to a “storytelling” style of writing. I think I may ﬁnally have understood his advice.
The ﬁrst person to read a complete draft of the manuscript was Thomas Klausner. Those who know Thomas or his reputation will no doubt understand why I was glad when he oﬀered to proofread and comment. I was rewarded with what seemed like an endless stream of comments, ranging from pointing out typos and grammatical mistakes to asking for clariﬁcations and to highlighting where my discussion was ﬂawed.
The vigilance of the NetBSD open source community kept my work honest and of high quality. I could not cut corners in the changes I made, since that would have been called out immediately. At the risk of doing injustice to some by forgetting to name them, I will list a few people from the NetBSD community who helped me throughout the years. Arnaud Ysmal was the ﬁrst person to use my work for implementing a large-scale application suite. Nicolas Joly contributed numerous patches. I enjoyed the many insightful discussions with Valeriy Ushakov, and I am especially thankful to him for nudging me into the direction of a simple solution for namespace protection. Finally, the many people involved in the NetBSD test eﬀort helped me to evaluate one of the main use cases for this work.
Since a doctoral dissertation should not extensively repeat previously published material, on the subject of contributions from my family, please refer to the preface of [Kantee 2004].

11
Then there is my dear Xi, who, simply put, made sure I survived. When I was hungry, I got food. When I was sleepy, I got coﬀee, perfectly brewed. Sometimes I got tea, but that usually happened when I had requested it instead of coﬀee. My output at times was monosyllabic at best (“hack”, “bug”, “ﬁx”, ...), but that did not bother her, perhaps because I occasionally ventured into the extended monosyllabic spectrum (“debug”, “coﬀee”, “pasta”, ...). On top of basic caretaking, I received assistance when ﬁnalizing the manuscript, since I had somehow managed to put myself into a situation characterized by a distinct lack of time. For example, she proofread multiple chapters and checked all the references ﬁxing several of my omissions and mistakes. The list goes on and on and Xi quite literally goes to 11.
This work was partially funded by the following organizations: Finnish Cultural Foundation, Research Foundation of the Helsinki University of Technology, Nokia Foundation, Ulla Tuominen Foundation and the Helsinki University of Technology. I thank these organizations for funding which allowed me to work full-time pursuing my vision. Since the time it took for me to realize the work was much longer than the time limit for receiving doctoral funding, I also see ﬁt to extend my gratitude to organizations which have hired me over the years and thus indirectly funded this work. I am lucky to work in ﬁeld where this is possible and even to some point synergetic with the research.
Lastly, I thank everyone who has ever submitted a bug report to an open source project.
Antti Kantee November 2012

12

13
Contents

Preface

9

Contents

13

List of Abbreviations

19

List of Figures

23

List of Tables

27

1 Introduction

29

1.1 Challenges with the Monolithic Kernel . . . . . . . . . . . . . . . . 30

1.2 Researching Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . 31

1.3 Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

1.4 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

1.5 Dissertation Outline . . . . . . . . . . . . . . . . . . . . . . . . . . 35

1.6 Further Material . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

1.6.1 Source Code . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

1.6.2 Manual Pages . . . . . . . . . . . . . . . . . . . . . . . . . . 38

1.6.3 Tutorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

2 The Anykernel and Rump Kernels

39

2.1 An Ultralightweight Virtual Kernel for Drivers . . . . . . . . . . . . 41

2.1.1 Partial Virtualization and Relegation . . . . . . . . . . . . . 42

2.1.2 Base, Orthogonal Factions, Drivers . . . . . . . . . . . . . . 44

2.1.3 Hosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

2.2 Rump Kernel Clients . . . . . . . . . . . . . . . . . . . . . . . . . . 47

2.3 Threads and Schedulers . . . . . . . . . . . . . . . . . . . . . . . . 53

2.3.1 Kernel threads . . . . . . . . . . . . . . . . . . . . . . . . . 57

2.3.2 A CPU for a Thread . . . . . . . . . . . . . . . . . . . . . . 57

14

2.3.3 Interrupts and Preemption . . . . . . . . . . . . . . . . . . . 60 2.3.4 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 2.4 Virtual Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 2.5 Distributed Services with Remote Clients . . . . . . . . . . . . . . . 64 2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

3 Implementation

67

3.1 Kernel Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

3.1.1 Extracting and Implementing . . . . . . . . . . . . . . . . . 70

3.1.2 Providing Components . . . . . . . . . . . . . . . . . . . . . 72

3.2 Running the Kernel in an Hosted Environment . . . . . . . . . . . . 73

3.2.1 C Symbol Namespaces . . . . . . . . . . . . . . . . . . . . . 74

3.2.2 Privileged Instructions . . . . . . . . . . . . . . . . . . . . . 76

3.2.3 The Hypercall Interface . . . . . . . . . . . . . . . . . . . . 77

3.3 Rump Kernel Entry and Exit . . . . . . . . . . . . . . . . . . . . . 81

3.3.1 CPU Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . 84

3.3.2 Interrupts and Soft Interrupts . . . . . . . . . . . . . . . . . 91

3.4 Virtual Memory Subsystem . . . . . . . . . . . . . . . . . . . . . . 93

3.4.1 Page Remapping . . . . . . . . . . . . . . . . . . . . . . . . 95

3.4.2 Memory Allocators . . . . . . . . . . . . . . . . . . . . . . . 97

3.4.3 Pagedaemon . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

3.5 Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

3.5.1 Passive Serialization Techniques . . . . . . . . . . . . . . . . 105

3.5.2 Spinlocks on a Uniprocessor Rump Kernel . . . . . . . . . . 109

3.6 Application Interfaces to the Rump Kernel . . . . . . . . . . . . . . 112

3.6.1 System Calls . . . . . . . . . . . . . . . . . . . . . . . . . . 113

3.6.2 vnode Interface . . . . . . . . . . . . . . . . . . . . . . . . . 118

3.6.3 Interfaces Speciﬁc to Rump Kernels . . . . . . . . . . . . . . 120

3.7 Rump Kernel Root File System . . . . . . . . . . . . . . . . . . . . 121

3.7.1 Extra-Terrestrial File System . . . . . . . . . . . . . . . . . 122

15

3.8 Attaching Components . . . . . . . . . . . . . . . . . . . . . . . . . 124 3.8.1 Kernel Modules . . . . . . . . . . . . . . . . . . . . . . . . . 124 3.8.2 Modules: Loading and Linking . . . . . . . . . . . . . . . . 127 3.8.3 Modules: Supporting Standard Binaries . . . . . . . . . . . 131 3.8.4 Rump Component Init Routines . . . . . . . . . . . . . . . . 136
3.9 I/O Backends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 3.9.1 Networking . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 3.9.2 Disk Driver . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
3.10 Hardware Device Drivers: A Case of USB . . . . . . . . . . . . . . 149 3.10.1 Structure of USB . . . . . . . . . . . . . . . . . . . . . . . . 149 3.10.2 Deﬁning Device Relations with Conﬁg . . . . . . . . . . . . 150 3.10.3 DMA and USB . . . . . . . . . . . . . . . . . . . . . . . . . 153 3.10.4 USB Hubs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
3.11 Microkernel Servers: Case Study with File Servers . . . . . . . . . . 158 3.11.1 Mount Utilities and File Servers . . . . . . . . . . . . . . . . 159 3.11.2 Requests: The p2k Library . . . . . . . . . . . . . . . . . . . 160 3.11.3 Unmounting . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
3.12 Remote Clients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162 3.12.1 Client-Kernel Locators . . . . . . . . . . . . . . . . . . . . . 164 3.12.2 The Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 3.12.3 The Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 3.12.4 Communication Protocol . . . . . . . . . . . . . . . . . . . . 167 3.12.5 Of Processes and Inheritance . . . . . . . . . . . . . . . . . 168 3.12.6 System Call Hijacking . . . . . . . . . . . . . . . . . . . . . 172 3.12.7 A Tale of Two Syscalls: fork() and execve() . . . . . . . . 177 3.12.8 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
3.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182

4 Evaluation

185

4.1 Feasibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185

16

4.1.1 Implementation Eﬀort . . . . . . . . . . . . . . . . . . . . . 185 4.1.2 Maintenance Eﬀort . . . . . . . . . . . . . . . . . . . . . . . 188 4.2 Use of Rump Kernels in Applications . . . . . . . . . . . . . . . . . 190 4.2.1 fs-utils . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 4.2.2 makefs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 4.3 On Portability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194 4.3.1 Non-native Hosting . . . . . . . . . . . . . . . . . . . . . . . 195 4.3.2 Other Codebases . . . . . . . . . . . . . . . . . . . . . . . . 200 4.4 Security: A Case Study with File System Drivers . . . . . . . . . . 201 4.5 Testing and Developing Kernel Code . . . . . . . . . . . . . . . . . 202 4.5.1 The Approaches to Kernel Development . . . . . . . . . . . 203 4.5.2 Test Suites . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206 4.5.3 Testing: Case Studies . . . . . . . . . . . . . . . . . . . . . . 208 4.5.4 Regressions Caught . . . . . . . . . . . . . . . . . . . . . . . 215 4.5.5 Development Experiences . . . . . . . . . . . . . . . . . . . 217 4.6 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 4.6.1 Memory Overhead . . . . . . . . . . . . . . . . . . . . . . . 219 4.6.2 Bootstrap Time . . . . . . . . . . . . . . . . . . . . . . . . . 220 4.6.3 System Call Speed . . . . . . . . . . . . . . . . . . . . . . . 224 4.6.4 Networking Latency . . . . . . . . . . . . . . . . . . . . . . 226 4.6.5 Backend: Disk File Systems . . . . . . . . . . . . . . . . . . 226 4.6.6 Backend: Networking . . . . . . . . . . . . . . . . . . . . . . 231 4.6.7 Web Servers . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 4.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234

5 Related Work

237

5.1 Running Kernel Code in Userspace . . . . . . . . . . . . . . . . . . 237

5.2 Microkernel Operating Systems . . . . . . . . . . . . . . . . . . . . 238

5.3 Partitioned Operating Systems . . . . . . . . . . . . . . . . . . . . 239

5.4 Plan 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240

17

5.5 Namespace Virtualization . . . . . . . . . . . . . . . . . . . . . . . 241 5.6 Lib OS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242 5.7 Inter-OS Kernel Code . . . . . . . . . . . . . . . . . . . . . . . . . . 243 5.8 Safe Virtualized Drivers . . . . . . . . . . . . . . . . . . . . . . . . 246 5.9 Testing and Development . . . . . . . . . . . . . . . . . . . . . . . . 247 5.10 Single Address Space OS . . . . . . . . . . . . . . . . . . . . . . . . 247

6 Conclusions

249

6.1 Future Directions and Challenges . . . . . . . . . . . . . . . . . . . 252

References

255

Appendix A Manual Pages

Appendix B Tutorial on Distributed Kernel Services B.1 Important concepts and a warmup exercise B.1.1 Service location speciﬁers B.1.2 Servers B.1.3 Clients B.1.4 Client credentials and access control B.1.5 Your First Server B.2 Userspace cgd encryption B.3 Networking B.3.1 Conﬁguring the TCP/IP stack B.3.2 Running applications B.3.3 Transparent TCP/IP stack restarts B.4 Emulating makefs B.5 Master class: NFS server B.5.1 NFS Server B.5.2 NFS Client B.5.3 Using it B.6 Further ideas

18 Appendix C Patches to the 5.99.48 source tree

19
List of Abbreviations

ABI
ASIC CAS CPU
DMA DSL DSO ELF
FFS
FS GPL i386 IPC

Application Binary Interface: The interface between two binaries. ABI-compatible binaries can interface with each other.
Application-Speciﬁc Integrate Circuit
Compare-And-Swap; atomic operation
Central Processing Unit; in this document the term is context-dependant. It is used to denote both the physical hardware unit or a virtual concept of a CPU.
Direct Memory Access
Domain Speciﬁc Language
Dynamic Shared Object
Executable and Linking Format; the binary format used by NetBSD and some other modern Unix-style operating systems.
Berkeley Fast File System; in most contexts, this can generally be understood to mean the same as UFS (Unix File System).
File System
General Public License; a software license
Intel 32-bit ISA (a.k.a. IA-32)
Inter-Process Communication

ISA LGPL LRU LWP
MD MI MMU
NIC OS PIC
PR RTT RUMP

20
Instruction Set Architecture
Lesser GPL; a less restrictive variant of GPL
Least Recently Used
Light Weight Process; the kernel’s idea of a thread. This acronym is usually written in lowercase (lwp) to mimic the kernel structure name (struct lwp).
Machine Dependent [code]; [code] speciﬁc to the platform
Machine Independent [code]; [code] usable on all platforms
Memory Management Unit: hardware unit which handles memory access and does virtual-to-physical translation for memory addresses
Network Interface Controller
Operating System
Position Independent Code; code which uses relative addressing and can be loaded at any location. It is typically used in shared libraries, where the load address of the code can vary from one process to another.
Problem Report
Round Trip Time
Deprecated “backronym” denoting a rump kernel and its local client application. This backronym should not appear in any material written since mid-2010.

SLIP
TLS TLS UML USB VAS VM
VMM

21
Serial Line IP: protocol for framing IP datagrams over a serial line. Thread-Local Storage; private per-thread data Transport Layer Security User Mode Linux Universal Serial Bus Virtual Address Space Virtual Memory; the abbreviation is context dependent and can mean both virtual memory and the kernel’s virtual memory subsystem Virtual Machine Monitor

22

23
List of Figures
2.1 Rump kernel hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . 45 2.2 BPF access via ﬁle system . . . . . . . . . . . . . . . . . . . . . . . 48 2.3 BPF access without a ﬁle system . . . . . . . . . . . . . . . . . . . 49 2.4 Client types illustrated . . . . . . . . . . . . . . . . . . . . . . . . . 51 2.5 Use of curcpu() in the pool allocator . . . . . . . . . . . . . . . . . 59 2.6 Providing memory mapping support on top of a rump kernel . . . . 63
3.1 Performance of position independent code (PIC) . . . . . . . . . . . 73 3.2 C namespace protection . . . . . . . . . . . . . . . . . . . . . . . . 75 3.3 Hypercall locking interfaces . . . . . . . . . . . . . . . . . . . . . . 79 3.4 rump kernel entry/exit pseudocode . . . . . . . . . . . . . . . . . . 83 3.5 System call performance using the trivial CPU scheduler . . . . . . 85 3.6 CPU scheduling algorithm in pseudocode . . . . . . . . . . . . . . . 87 3.7 CPU release algorithm in pseudocode . . . . . . . . . . . . . . . . . 89 3.8 System call performance using the improved CPU scheduler . . . . 90 3.9 Performance of page remapping vs. copying . . . . . . . . . . . . . 98 3.10 Using CPU cross calls when checking for syscall users . . . . . . . . 108 3.11 Cost of atomic memory bus locks on a twin core host . . . . . . . . 111 3.12 Call stub for rump_sys_lseek() . . . . . . . . . . . . . . . . . . . 115 3.13 Compile-time optimized sizeof() check . . . . . . . . . . . . . . . 117 3.14 Implementation of RUMP_VOP_READ() . . . . . . . . . . . . . . . . . 119 3.15 Application interface implementation of lwproc rfork() . . . . . . 120 3.16 Loading kernel modules with dlopen() . . . . . . . . . . . . . . . . 129 3.17 Adjusting the system call vector during rump kernel bootstrap . . . 130 3.18 Comparison of pmap_is_modified deﬁnitions . . . . . . . . . . . . 133 3.19 Comparison of curlwp deﬁnitions . . . . . . . . . . . . . . . . . . . 134 3.20 Example: selected contents of component.c for netinet . . . . . . . 138 3.21 Networking options for rump kernels . . . . . . . . . . . . . . . . . 141

24
3.22 Bridging a tap interface to the host’s re0 . . . . . . . . . . . . . . . 142 3.23 sockin attachment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 3.24 dmesg of USB pass-through rump kernel with mass media attached 151 3.25 USB mass storage conﬁguration . . . . . . . . . . . . . . . . . . . . 154 3.26 SCSI device conﬁguration . . . . . . . . . . . . . . . . . . . . . . . 154 3.27 Attaching USB Hubs . . . . . . . . . . . . . . . . . . . . . . . . . . 156 3.28 USB device probe without host HUBs . . . . . . . . . . . . . . . . . 156 3.29 USB device probe with host HUBs . . . . . . . . . . . . . . . . . . 157 3.30 File system server . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 3.31 Use of -o rump in /etc/fstab . . . . . . . . . . . . . . . . . . . . 160 3.32 Implementation of p2k_node_read() . . . . . . . . . . . . . . . . . 161 3.33 Remote client architecture . . . . . . . . . . . . . . . . . . . . . . . 163 3.34 Example invocations lines of rump_server . . . . . . . . . . . . . . 166 3.35 System call hijacking . . . . . . . . . . . . . . . . . . . . . . . . . . 173 3.36 Implementation of fork() on the client side . . . . . . . . . . . . . 179 3.37 Local vs. Remote system call overhead . . . . . . . . . . . . . . . . 181
4.1 Source Lines Of Code in rump kernel and selected drivers . . . . . . 186 4.2 Lines of code for platform support . . . . . . . . . . . . . . . . . . . 187 4.3 Duration for various i386 target builds . . . . . . . . . . . . . . . . 190 4.4 Simple compatibility type generation . . . . . . . . . . . . . . . . . 199 4.5 Generated compatibility types . . . . . . . . . . . . . . . . . . . . . 199 4.6 Mounting a corrupt FAT FS with the kernel driver in a rump kernel 202 4.7 Valgrind reporting a kernel memory leak . . . . . . . . . . . . . . . 205 4.8 Flagging an error in the scsitest driver . . . . . . . . . . . . . . . . 210 4.9 Automated stack trace listing . . . . . . . . . . . . . . . . . . . . . 215 4.10 Memory usage of rump kernels per idle instance . . . . . . . . . . . 218 4.11 Time required to bootstrap one rump kernel . . . . . . . . . . . . . 221 4.12 Script for starting, conﬁguring and testing a network cluster . . . . 223 4.13 Time required to start, conﬁgure and send an initial packet . . . . . 224

25
4.14 Time to execute 5M system calls per thread in 2 parallel threads . . 225 4.15 UDP packet RTT . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 4.16 Performance of FFS with the ﬁle system a regular ﬁle . . . . . . . . 230 4.17 Performance of FFS on a HD partition (raw device) . . . . . . . . . 230 4.18 Performance of a journaled FFS with the ﬁle system on a regular ﬁle 231 4.19 RTT of ping with various virtualization technologies . . . . . . . . . 233 4.20 Speed of 10,000 HTTP GET requests over LAN . . . . . . . . . . . 233
(with 9 illustrations)

26

27
List of Tables
2.1 Comparison of client types . . . . . . . . . . . . . . . . . . . . . . . 50
3.1 Symbol renaming illustrated . . . . . . . . . . . . . . . . . . . . . . 75 3.2 File system I/O performance vs. available memory . . . . . . . . . 101 3.3 Kernel module classiﬁcation . . . . . . . . . . . . . . . . . . . . . . 125 3.4 Rump component classes . . . . . . . . . . . . . . . . . . . . . . . . 137 3.5 Requests from the client to the kernel . . . . . . . . . . . . . . . . . 168 3.6 Requests from the kernel to the client . . . . . . . . . . . . . . . . . 169 3.7 Step-by-step comparison of host and rump kernel syscalls, part 1/2 170 3.8 Step-by-step comparison of host and rump kernel syscalls, part 2/2 171
4.1 Commit log analysis for sys/rump Aug 2007 - Dec 2008 . . . . . . . 188 4.2 makefs implementation eﬀort comparison . . . . . . . . . . . . . . 193 4.3 Minimum memory required to boot the standard installation . . . . 218 4.4 Bootstrap times for standard NetBSD installations . . . . . . . . . 221

28

29
1 Introduction
In its classic role, an operating system is a computer program which abstracts the platform it runs on and provides services to application software. Applications in turn provide functionality that the user of the computer system is interested in. For the user to be satisﬁed, the operating system must therefore support both the platform and application software.
An operating system is understood to consist of the kernel, userspace libraries and utilities, although the exact division between these parts is not deﬁnitive in every operating system. The kernel, as the name says, contains the most fundamental routines of the operating system. In addition to low-level platform support and critical functionality such as thread scheduling and IPC, the kernel oﬀers drivers, which abstract an underlying entity. Throughout this dissertation we will use the term driver in an extended sense which encompasses not only hardware device drivers, but additionally for example ﬁle system drivers and the TCP/IP network driver.
Major contemporary operating systems follow the monolithic kernel model. Of popular general purpose operating systems, for example Linux, Windows and Mac OS X are regarded as monolithic kernel operating systems. A monolithic kernel means that the entire kernel is executed in a single privileged domain, as opposed to being spread out to multiple independent domains which communicate via message passing. The single privileged domain in turn means that all code in the kernel has full capability to directly control anything running on that particular system. Furthermore, the monolithic kernel does not inherently impose any technical restrictions for the structure of the kernel: a routine may call any other routine in the kernel and access all memory directly. However, like in most disciplines, a well-designed architecture is desirable. Therefore, even a monolithic kernel tends towards structure.

30
1.1 Challenges with the Monolithic Kernel
Despite its widespread popularity, we identiﬁed a number of suboptimal characteristics in monolithic kernels which we consider as the motivating problems for this dissertation:
1. Weak security and robustness. Since all kernel code runs in the same privileged domain, a single mistake can bring the whole system down. The fragile nature of the monolithic kernel is a long-standing problem to which all monolithic kernel operating systems are vulnerable. Bugs are an obvious manifestation of the problem, but there are more subtle issues to consider. For instance, widely used ﬁle system drivers are vulnerable against untrusted disk images [115]. This vulnerability is acknowledged in the manual page of the mount command for example on Linux: “It is possible for a corrupted ﬁle system to cause a crash”. The commonplace act of accessing untrusted removable media such as a USB stick or DVD disk with an in-kernel ﬁle system driver opens the entire system to a security vulnerability.
2. Limited possibilities for code reuse. The code in a monolithic kernel is viewed to be an all-or-nothing deal due to a belief that everything is intertwined with everything else. This belief implies that features cannot be cherry-picked and put into use in other contexts and that the kernel drivers have value only when they are a part of the monolithic kernel. Examples include ﬁle systems [115] and networking [82]. One manifestation of this belief is the reimplementation of kernel drivers for userspace. These reimplementations include TCP/IP stacks [27, 96] and ﬁle system drivers [2, 89, 105] 1 for the purposes of research, testing, teaching
1 We emphasize that with ﬁle systems we do not mean FUSE (Filesystem in Userspace) [106]. FUSE provides a mechanism for attaching a ﬁle system driver as a microkernel style server, but does not provide the driver itself. The driver attached by FUSE may be an existing kernel driver which was reimplemented in userspace [2, 3].

31
and application-level drivers. The common approaches for reimplementation are starting from scratch or taking a kernel driver and adjusting the code until the speciﬁc driver can run in userspace. If cherry-picking unmodiﬁed drivers were possible, kernel drivers could be directly used at application level. Code reuse would not only save the initial implementation eﬀort, but more importantly it would save from having to maintain the second implementation.
3. Development and testing is convoluted. This is a corollary of the previous point: in the general case testing involves booting up the whole operating system for each iteration. Not only is the bootup slow in itself, but when it is combined with the fact that an error may bring the entire system down, development cycles become long and batch mode regression testing is diﬃcult. The diﬃculty of testing aﬀects how much testing and quality assurance the ﬁnal software product receives. Users are indirectly impacted: better testing produces a better system. Due to the complexity and slowness of in-kernel development, a common approach is to implement a prototype in userspace before porting the code to the kernel. For example FFS in BSD [70] and ZFS in Solaris [16] were implemented this way. This approach may bring additional work when the code is being moved into the kernel, as the support shim in userspace may not have fully emulated all kernel interfaces [70].
1.2 Researching Solutions
One option for addressing problems in monolithic kernels is designing a better model and starting from scratch. Some examples of alternative kernel models include the microkernel [9, 43, 45, 64] Exokernel [33] and a partitioned kernel [12, 112]. The problem with starting from scratch is getting to the point of having enough

32
support for external protocols to be a viable alternative for evaluation with real world applications. These external protocols include anything serviced by a driver and range from a networking stack to a POSIX interface. As the complexity of the operating environment and external restrictions grow, it is more and more diﬃcult to start working on an operating system from scratch [92]. For a ﬁgure on the amount of code in a modern OS, we look at two subsystems in the Linux 3.3 kernel from March 2012. There are 1,001,218 physical lines of code for ﬁle system drivers in the fs subdirectory and 711,150 physical lines of code for networking drivers in the net subdirectory (the latter ﬁgure does not include NIC drivers, which are kept elsewhere in the source tree). For the sake of discussion, let us assume that a person who can write 100 lines of bugfree code each day writes all of those drivers. In that case, it will take over 46 years to produce the drivers in those subdirectories.
Even if there are resources to write a set of drivers from scratch, the drivers have not been tested in production in the real world when they are ﬁrst put out. Studies show that new code contains the most faults [18, 91]. The faults do not exist because the code would have been poorly tested before release, but rather because it is not possible to anticipate every real world condition in a laboratory environment. We argue that real world use is the property that makes an existing driver base valuable, not just the fact that it exists.
1.3 Thesis
We claim that it is possible to construct a ﬂexible kernel architecture which solves the challenges listed in Section 1.1, and yet retain the monolithic kernel. Furthermore, it is possible to implement the ﬂexible kernel architecture solely by good programming principles and without introducing levels of indirection which hinder the monolithic kernel’s performance characteristics. We show our claim to be true by an implementation for a BSD-derived open source monolithic kernel OS, NetBSD [87].

33
We deﬁne an anykernel to be an organization of kernel code which allows the kernel’s unmodiﬁed drivers to be run in various conﬁgurations such as application libraries and microkernel style servers, and also as part of a monolithic kernel. This approach leaves the conﬁguration the driver is used in to be decided at runtime. For example, if maximal performance is required, the driver can be included in a monolithic kernel, but where there is reason to suspect stability or security, the driver can still be used as an isolated, non-privileged server where problems cannot compromised the entire system.
An anykernel can be instantiated into units which virtualize the bare minimum support functionality for kernel drivers. We call these virtualized kernel instances rump kernels since they retain only a part of the original kernel. This minimalistic approach makes rump kernels fast to bootstrap (˜10ms) and introduces only a small memory overhead (˜1MB per instance). The implementation we present hosts rump kernels in unprivileged user processes on a POSIX host. The platform that the rump kernel is hosted on is called the host platform or host.
At runtime, a rump kernel can assume the role of an application library or that of a server. Programs requesting services from rump kernels are called rump kernel clients. Throughout this dissertation we use the shorthand client to denote rump kernel clients. We deﬁne three client types.
1. Local: the rump kernel is used in a library capacity. Like with any library, using the rump kernel as a library requires that the application is written to use APIs provided by a rump kernel. The main API for a local client is a system call API with the same call signatures as on a regular NetBSD system. For example, it is possible to use a kernel ﬁle system driver as a library in an application which interprets a ﬁle system image.

34
2. Microkernel: the host routes client requests from regular processes to drivers running in isolated servers. Unmodiﬁed application binaries can be used. For example, it is possible to run a block device driver as a microkernel style server, with the kernel driver outside the privileged domain.
3. Remote: the client and rump kernel are running in diﬀerent containers (processes) with the client deciding which services to request from the rump kernel and which to request from the host kernel. For example, the client can use the TCP/IP networking services provided by a rump kernel. The kernel and client can exist either on the same host or on diﬀerent hosts. In this model, both speciﬁcally written applications and unmodiﬁed applications can use services provided by a rump kernel. The API for speciﬁcally written applications is the same as for local clients. For example, it is possible to use an unmodiﬁed Firefox web browser with the TCP/IP code running in a rump kernel server.
Each conﬁguration contributes to solving our motivating problems:
1. Security and robustness. When necessary, the use of a rump kernel will allow unmodiﬁed kernel drivers to be run as isolated microkernel servers while preserving the user experience. At other times the same driver code can be run in the original fashion as part of the monolithic kernel.
2. Code reuse. A rump kernel may be used by an application in the same fashion as any other userlevel library. A local client can call any routine inside the rump kernel.
3. Development and testing. The lightweight nature and safety properties of a rump kernel allow for safe testing of kernel code with iteration times in the millisecond range. The remote client model enables the creation of tests using familiar tools.

35 Our implementation supports rump kernels for ﬁle systems [55], networking [54] and device drivers [56]. Both synthetic benchmarks and real world data gathered from a period between 2007 and 2011 are used for the evaluation. We focus our eﬀorts at drivers which do not depend on a physical backend being present. Out of hardware device drivers, support for USB drivers has been implemented and veriﬁed. We expect it is possible to support generic unmodiﬁed hardware device drivers in rump kernels by using previously published methods [62].
1.4 Contributions
The original contributions of this dissertation are as follows:
1. The deﬁnition of an anykernel and a rump kernel. 2. Showing that it is possible to implement the above in production quality code
and maintain them in a real world monolithic kernel OS. 3. Analysis indicating that the theory is generic and can be extended to other
operating systems.
1.5 Dissertation Outline
Chapter 2 deﬁnes the concept of an anykernel and explains rump kernels. Chapter 3 discusses the implementation and provides microbenchmarks as supporting evidence for implementation decisions. Chapter 4 evaluates the solution. Chapter 5 looks at related work. Chapter 6 provides concluding remarks.

36
1.6 Further Material
1.6.1 Source Code
The implementation discussed in this dissertation can be found in source code form from the NetBSD CVS tree as of March 31st 2011 23:59UTC.
NetBSD is an evolving open source project with hundreds of volunteers and continuous change. Any statement we make about NetBSD reﬂects solely the above timestamp and no other. It is most likely that statements will apply over a wide period of time, but it is up to the interested reader to verify if they apply to earlier or later dates.
It is possible to retrieve the source tree with the following command:
cvs -d anoncvs@anoncvs.netbsd.org:/cvsroot co -D’20110331 2359UTC’ src
Whenever we refer to source ﬁle, we implicitly assume the src directory to be a part of the path, i.e. sys/kern/init_main.c means src/sys/kern/init_main.c.
For simplicity, the above command checks out the entire NetBSD operating system source tree instead of attempting to cherry-pick only the relevant code. The checkout will require approximately 1.1GB of disk space. Most of the code relevant to this document resides under sys/rump, but relevant code can be found under other paths as well, such as tests and lib.
Diﬀs in Appendix C detail where the above source tree diﬀers from the discussion in this dissertation.

37 The project was done in small increments in the NetBSD source with almost daily changes. The commits are available for study from repository provided by the NetBSD project, e.g. via the web interface at cvsweb.NetBSD.org.
NetBSD Release Model
We use NetBSD release cycle terminology throughout this dissertation. The following contains an explanation of the NetBSD release model. It is a synopsis of the information located at http://www.NetBSD.org/releases/release-map.html.
The main development branch or HEAD of NetBSD is known as NetBSD-current or, if NetBSD is implied, simply -current. The source code used in this dissertation is therefore -current from the aforementioned date.
Release branches are created from -current and are known by their major number, for example NetBSD 5. Release branches get bug ﬁxes and minor features, and releases are cut from them at suitable dates. Releases always contain one or more minor numbers, e.g. NetBSD 5.0.1. The ﬁrst major branch after March 31st is NetBSD 6 and therefore the ﬁrst release to potentially contain this work is NetBSD 6.0.
A -current snapshot contains a kernel API/ABI version. The version is incremented only when an interface changes. The kernel version corresponding to March 31st is 5.99.48. While this version number stands for any -current snapshot between March 9th and April 11th 2011, whenever 5.99.48 is used in this dissertation, it stands for -current at 20110331.

38 Code examples
This dissertation includes code examples from the NetBSD source tree. All such examples are copyright of their respective owners and are not public domain. If pertinent, please check the full source for further information about the licensing and copyright of each such example.
1.6.2 Manual Pages
Unix-style manual pages for interfaces described in this dissertation are available in Appendix A. The manual pages are taken verbatim from the NetBSD 5.99.48 distribution.
1.6.3 Tutorial
Appendix B contains a hands-on tutorial. It walks through various use cases where drivers are virtualized, such as encrypting a ﬁle system image using the kernel crypto driver and running applications against virtual userspace TCP/IP stacks. The tutorial uses standard applications and does not require writing code or compiling special binaries.

39
2 The Anykernel and Rump Kernels
As a primer for the technical discussion in this document, we consider the elements that make up a modern Unix-style operating system kernel. The following is not the only way to make a classiﬁcation, but it is the most relevant one for our coming discussion.
The CPU speciﬁc code is on the bottom layer of the OS. This code takes care of low level bootstrap and provides an abstract interface to the hardware. In most, if not all, modern general purpose operating systems the CPU architecture is abstracted away from the bulk of the kernel and only the lowest layers have knowledge of it. To put the previous statement into terms which are used in our later discussions, the interfaces provided by the CPU speciﬁc code are the hypercall interfaces that the OS runs on. In the NetBSD kernel these functions are usually preﬁxed with “cpu”.
The virtual memory subsystem manages the virtual address space of the kernel and processes. Virtual memory management includes deﬁning what happens when a memory address is accessed. Examples include normal read/write access to the memory, ﬂagging a segmentation violation, or a ﬁle being read from the ﬁle system.
The process execution subsystem understands the formats that executable binaries use and knows how to create a new process when an executable is run.
The scheduling code includes a method and policy to deﬁne what code a CPU is executing. The currently executing thread can be switched either when the scheduler decides it has run too long, or when the thread itself makes a system call which requires waiting for a condition to become true before execution can be resumed. When a thread is switched, the scheduler calls the CPU speciﬁc code to save the machine context of the current thread and load the context of the new thread. In

40
NetBSD, both user processes and the kernel are preemptively scheduled, meaning the scheduler can decide to unschedule the currently executing thread and schedule a new one.
Atomic operations enable modifying memory atomically and avoid race conditions in for example a read-modify-write cycle. For uniprocessor architectures, kernel atomic operations are a matter of disabling interrupts and preemption for the duration of the operation. Multiprocessor architectures provide machine instructions for atomic operations. The operating system’s role with atomic operations is mapping function interfaces to the way atomic operations are implemented on that particular machine architecture.
Synchronization routines such as mutexes and condition variables build upon atomic operations and interface with the scheduler. For example, if locking a mutex is attempted, the condition for it being free is atomically tested and set. If a sleep mutex was already locked, the currently executing thread interfaces with the scheduling code to arrange for itself to be put to sleep until the mutex is released.
Various support interfaces such CPU cross-call, time-related routines, kernel linkers, etc. provide a basis on which to build drivers.
Resource management includes general purpose memory allocation, a pool and slab [15] allocator, ﬁle descriptors, PID namespace, vmem/extent resource allocators etc. Notably, in addition to generic resources such as memory, there are more speciﬁc resources to manage. Examples of more speciﬁc resources include vnodes [58] for ﬁle systems and mbufs [114] for the TCP/IP stack.
Drivers interact with external objects such as ﬁle system images, hardware, the network, etc. After a fashion, it can be said they accomplish all the useful work an operating system does. It needs to be remembered, though, that they operate by

41
building on top of the entities mentioned earlier in this section. Drivers are what we are ultimately interested in utilizing, but to make them available we must deal with everything they depend on. Being able to reuse drivers means we have to provide semantically equivalent implementations of the support routines that the drivers use. The straightforward way is to run the entire kernel, but it not always the optimal approach, as we will demonstrate throughout this dissertation.
2.1 An Ultralightweight Virtual Kernel for Drivers
Virtualization of the entire operating system can done either by modifying the operating system kernel by means of paravirtualization (e.g. User-Mode Linux [26] or Xen [11]) or by virtualizing at the hardware layer so than an unmodiﬁed operating system can be run (by using e.g. QEMU [13]). From the perspective of the host, virtualization provides both multiplicity and isolation of the monolithic kernel, and can be seen as a possible solution for our security and testing challenges from Section 1.1. Using a fully virtualized OS as an application library is less straightforward, but can be done by bootstrapping a guest instance of an operating system and communicating with the guest’s kernel through an application running on the guest. For example, libguestfs [4] uses this approach to access ﬁle system images safely.
Full OS virtualization is a heavyweight operation. For instance, several seconds of bootstrap delay for a fully virtualized OS [48] is too long if we wish to use virtualized kernel instances as application libraries — humans perceive delays of over 100ms [78]. While it may be possible to amortize the bootstrap delay over several invocations, managing cached instances adds complexity to the applications, especially if multiple diﬀerent users want to use ones for multiple diﬀerent purposes. Furthermore, a full OS consumes more machine resources, such as memory and storage and CPU, than is necessary for kernel driver virtualization. The increased resource requirement

42
is because a full OS provides the entire application environment, which from our perspective is overhead.
Virtualization via containers [52] provides better performance than paravirtualization [104, 110]. However, containers do not address our motivating problems. With containers, the host kernel is directly used for the purposes of all guests. In other words, kernel drivers are run in a single domain within the host. There is no isolation between kernel drivers for diﬀerent guests and a single error in one of them can bring all of the guests and the host down. The lack of isolation is due to the use case that containers are targeted at: they provide a virtual application environment instead of a virtual kernel environment.
We wish to investigate a lightweight solution to maximize the performance and simplicity in our use cases. In other words, we believe in an approach which requires only the essential functionality necessary for solving a problem [59].
2.1.1 Partial Virtualization and Relegation
A key observation in our lightweight approach is that part of the supporting functionality required by drivers is readily provided by the system hosting our virtualized driver environment. For example, drivers need a memory address space to execute in; we use the one that the host provides instead of simulating a second one on top of it. Likewise, we directly use the host’s threading and scheduling facilities in our virtual kernel instead of having the host schedule a virtual kernel with its own layer of scheduling. Relegating support functionality to the host avoids adding a layer of indirection and overhead. It is also the reason why we call our virtualized kernel instance a rump kernel : it virtualizes only a part of the original. In terms of taxonomy, we classify a rump kernel as partial paravirtualization.

43
Drivers in a rump kernel remain unmodiﬁed over the original ones. A large part of the support routines remain unmodiﬁed as well. Only in places where support is relegated to the host, do we require speciﬁcally written glue code. We use the term anykernel to describe a kernel code base with the property of being able use unmodiﬁed drivers and the relevant support routines in rump kernels. It should be noted that unlike for example the term microkernel, the term anykernel does not convey information about how the drivers are organized at runtime, but rather that it is possible to organize them in a number of ways. We will examine the implementation details of an anykernel more closely in Chapter 3 where we turn a monolithic kernel into an anykernel.
While rump kernels (i.e. guests) use features provided by the host, the diﬀerence to containers is that rump kernels themselves are not a part of the host. Instead, the host provides the necessary facilities for starting multiple rump kernel instances. These instances are not only isolated from each other, but also from the host. In POSIX terms, this means that a rump kernel has the same access rights as any other process running with the same credentials.
An example of a practical beneﬁt resulting from relegating relates to program execution. When a fully virtualized operating system executes a program, it searches for the program from its ﬁle system namespace and runs it while the host remains oblivious to the fact that the guest ran a program. In contrast, the rump kernel and its clients are run from the host’s ﬁle system namespace by the host. Since process execution is handled by the host, there is no need to conﬁgure a root ﬁle system for a rump kernel, and rump kernels can be used as long as the necessary binaries are present on the host. This also means that there is no extra maintenance burden resulting from keeping virtual machine images up-to-date. As long the host is kept up-to-date, the binaries used with rump kernels will not be out of date and potentially contain dormant security vulnerabilities [38].

44
Another example of a practical beneﬁt that a rump kernel provides is core dump size. Since a rump kernel has a small memory footprint, the core dumps produced as the result of a kernel panic are small. Small core dumps signiﬁcantly reduce disk use and restart time without having to disable core dumps completely and risk losing valuable debugging information.
A negative implication of selective virtualization is that not all parts of the kernel can be tested and isolated using this scheme. However, since a vast majority of kernel bugs are in drivers [18], our focus is on improving the state-of-the-art for them.
2.1.2 Base, Orthogonal Factions, Drivers
A monolithic kernel, as the name implies, is one single entity. The runtime footprint of a monolithic kernel contains support functionality for all subsystems, such as sockets for networking, vnodes for ﬁle systems and device autoconﬁguration for drivers. All of these facilities cost resources, especially memory, even if they are not used.
We have divided a rump kernel, and therefore the underlying NetBSD kernel codebase, into three layers which are illustrated in Figure 2.1: the base, factions and drivers. The base contains basic support such as memory allocation and locking. The dev, net and vfs factions, which denote devices, networking and [virtual] ﬁle systems, respectively, provide subsystem level support. To minimize runtime resource consumption, we require that factions are orthogonal. By orthogonal we mean that the code in one faction must be able to operate irrespective if any other faction is present in the rump kernel conﬁguration or not. Also, the base may not depend on any faction, as that would mean the inclusion of a faction in a rump kernel is mandatory instead of optional.

45




 

   

   

   

 



    


  

Figure 2.1: Rump kernel hierarchy. The desired drivers dictate the required components. The factions are orthogonal and depend only on the rump kernel base. The rump kernel base depends purely on the hypercall layer.
We use the term component to describe a functional unit for a rump kernel. For example, a ﬁle system driver is a component. A rump kernel is constructed by linking together the desired set of components, either at compile-time or at run-time. A loose similarity exists between kernel modules and the rump kernel approach: code is compiled once per target architecture, and a linker is used to determine runtime features. For a given driver to function properly, the rump kernel must be linked with the right set of dependencies. For example, the NFS component requires both the ﬁle system and networking factions, but in contrast the tmpfs component requires only the ﬁle system faction.
User interfaces are used by applications to request services from rump kernels. Any dependencies induced by user interfaces are optional, as we will illustrate next. Consider Unix-style device driver access. Access is most commonly done through ﬁle system nodes in /dev, with the relevant user interfaces being open and read/write

46
(some exceptions to the ﬁle system rule exist, such as Bluetooth and Ethernet interfaces which are accessed via sockets on NetBSD). To access a /dev ﬁle system node in a rump kernel, ﬁle systems must be supported. Despite ﬁle system access being the standard way to access a device, it is possible to architect an application where the device interfaces are called directly without going through ﬁle system code. Doing so means skipping the permission checks oﬀered by ﬁle systems, calling private kernel interfaces and generally having to write more fragile code. Therefore, it is not recommended as the default approach, but if need be due to resource limitations, it is a possibility. For example, let us assume we have a rump kernel running a TCP/IP stack and we wish to use the BSD Packet Filter (BPF) [67]. Access through /dev is presented in Figure 2.2, while direct BPF access which does not use ﬁle system user interfaces is presented in Figure 2.3. You will notice the ﬁrst example is similar to a regular application, while the latter is more complex. We will continue to refer to these examples in this chapter when we go over other concepts related to rump kernels.
The faction divisions allow cutting down several hundred kilobytes of memory overhead and milliseconds in startup time per instance. While the saving per instance is not dramatic, the overall savings are sizeable in applications such as network testing [44] which require thousands of virtual instances. For example, as we will later measure in Chapter 4, a virtual TCP/IP stack without ﬁle system support is 40% smaller (400kB) than one which contains ﬁle system support.
2.1.3 Hosting
A rump kernel accesses host resources through the rumpuser hypercall interface. The hypercall layer is currently implemented for POSIX hosts, but there is no reason why it could not be adapted to suit alternative hosting as well, such as microkernels. We analyze the requirements for the hypercall interface in more detail

47
in Section 3.2.3. Whenever discussing hosting and the hypercall interface we attempt to keep the discussion generic and fall back to POSIX terminology only when necessary.
The host controls what resources the guest has access to. It is unnecessary to run rump kernels as root on POSIX systems — security-conscious people will regard it as unwise as well unless there is a very good reason for it. Accessing resources such as the host ﬁle system will be limited to whatever credentials the rump kernel runs with on the host. Other resources such as memory consumption may be limited by the host as well.
2.2 Rump Kernel Clients
We deﬁne a rump kernel client to be an application that requests services from a rump kernel. Examples of rump kernel clients are an application that accesses the network through a TCP/IP stack provided by a rump kernel, or an application that reads ﬁles via a ﬁle system driver running in a rump kernel. Likewise, a test program that is used to test kernel code by means of running it in a rump kernel is a rump kernel client.
The relationship between a rump kernel and a rump kernel client is an almost direct analogy to an application process executing on an operating system and requesting services from the host kernel. The diﬀerence is that a rump kernel client must explicitly request all services from the rump kernel, while a process receives some services such as scheduling and memory protection implicitly from the host.
As mentioned in Chapter 1 there are several possible relationship types the client and rump kernel can have. Each of them have diﬀerent implications on the client and kernel. The possibilities are: local, remote and microkernel. The conﬁgurations

48
int main(int argc, char *argv[]) {
struct ifreq ifr; int fd; /* bootstrap rump kernel */ rump_init(); /* open bpf device, fd is in implicit process */ if ((fd = rump_sys_open(_PATH_BPF, O_RDWR, 0)) == -1)
err(1, "bpf open"); /* create virt0 in the rump kernel the easy way and set bpf to use it */ rump_pub_virtif_create(0); strlcpy(ifr.ifr_name, "virt0", sizeof(ifr.ifr_name)); if (rump_sys_ioctl(fd, BIOCSETIF, &ifr) == -1)
err(1, "set if"); /* rest of the application */ [....] }
Figure 2.2: BPF access via the ﬁle system. This ﬁgure demonstrates the system call style programming interface of a rump kernel.

49
int rumpns_bpfopen(dev_t, int, int, struct lwp *);
int main(int argc, char *argv[]) {
struct ifreq ifr; struct lwp *mylwp; int fd, error;
/* bootstrap rump kernel */ rump_init();
/* create an explicit rump kernel process context */ rump_pub_lwproc_rfork(RUMP_RFCFDG); mylwp = rump_pub_lwproc_curlwp();
/* schedule rump kernel CPU */ rump_schedule();
/* open bpf device */ error = rumpns_bpfopen(0, FREAD|FWRITE, 0, mylwp); if (mylwp->l_dupfd < 0) {
rump_unschedule(); errx(1, "open failed"); }
/* need to jump through a hoop due to bpf being a "cloning" device */ error = rumpns_fd_dupopen(mylwp->l_dupfd, &fd, 0, error); rump_unschedule(); if (error)
errx(1, "dup failed");
/* create virt0 in the rump kernel the easy way and set bpf to use it */ rump_pub_virtif_create(0); strlcpy(ifr.ifr_name, "virt0", sizeof(ifr.ifr_name)); if (rump_sys_ioctl(fd, BIOCSETIF, &ifr) == -1)
err(1, "set if");
/* rest of the application */ [....] }
Figure 2.3: BPF access without a ﬁle system. This ﬁgure demonstrates the ability to directly call arbitrary kernel routines from a user program. For comparison, it implements the same functionality as Figure 2.2. This ability is most useful for writing kernel unit tests when the calls to the unit under test cannot be directly invoked by using the standard system call interfaces.

50

Type

Request Policy Access Available Interface

local

client

full all

remote

client

limited system call

microkernel host kernel

limited depends on service

Table 2.1: Comparison of client types. Local clients get full access to a rump kernel, but require explicit calls in the program code. Remote clients have standard system call access with security control and can use unmodiﬁed binaries. In microkernel mode, the rump kernel is run as a microkernel style system server with requests routed by the host kernel.

are also depicted in Figure 2.4. The implications of each are available in summarized form in Table 2.1. Next, we will discuss the conﬁgurations and explain the table.

• Local clients exist in the same application process as the rump kernel itself. They have full access to the rump kernel’s address space, and make requests via function calls directly into the rump kernel. Typically requests are done via established interfaces such as the rump kernel syscall interface, but there is nothing preventing the client from jumping to any routine inside the rump kernel. The VFS-bypassing example in Figure 2.3 is a local client which manipulates the kernel directly, while the local client in Figure 2.2 uses established interfaces.
The beneﬁts of local clients include speed and compactness. Speed is due to a rump kernel request being essentially a function call. A null rump kernel system call is twice as fast as a native system call. Compactness results from the fact that there is only a single program and can make managing the whole easier. The drawback is that the single program must conﬁgure the kernel to a suitable state before the application can act. Examples of conﬁguration tasks include adding routing tables (the route utility) and mounting ﬁle systems (the mount utility). Since existing conﬁguration tools are built around the





 
 







51









 
  

 


 
 








 


 
 



 
 







Figure 2.4: Client types illustrated. For local clients the client and rump kernel reside in a single process, while remote and microkernel clients reside in separate processes and therefore do not have direct memory access into the rump kernel.
concept of executing diﬀerent conﬁguration steps as multiple invocations of the tool, adaptation of the conﬁguration code may not always be simple. On a POSIX system, local clients do not have meaningful semantics for a host fork() call. This lack of semantics is because the rump kernel state would be duplicated and could result in for example two kernels accessing the same ﬁle system or having the same IP address. A typical example of a local client is an application which uses the rump kernel as a programming library e.g. to access a ﬁle system.
• Remote clients use a rump kernel which resides elsewhere, either on the local host or a remote one. The request routing policy is up to the client. The policy locus is an implementation decision, not a design decision, and alternative implementations can be considered [37] if it is important to have the request routing policy outside of the client. Since the client and kernel are separated, kernel side access control is fully enforced — if the client and rump kernel are on the same host, we assume that the host enforces separation between the respective processes. This separation means that a remote client will not be able to access resources

52
except where the rump kernel lets it, and neither will it be able to dictate the thread and process context in which requests are executed. The client not being able to access arbitrary kernel resources in turn means that real security models are possible, and that diﬀerent clients may have varying levels of privileges. We have implemented support for remote clients which communicate with the server using local domain sockets or TCP sockets. Using sockets is not the only option, and for example the ptrace() facility can also be used to implement remote clients [26, 37]. Remote clients are not as performant as local clients due to IPC overhead. However, since multiple remote clients can run against a single rump kernel, they lead to more straightforward use of existing code and even that of unmodiﬁed binaries. Remote clients, unlike local clients, have meaningful semantics for fork() since both the host kernel context and rump kernel contexts can be correctly preserved: the host fork() duplicates only the client and not the rump kernel.
• Microkernel clients requests are routed by the host kernel to a separate server which handles the requests using a driver in a rump kernel. While microkernel clients can be seen to be remote clients, the key diﬀerence to remote clients is that the request routing policy is in the host kernel instead of in the client. Furthermore, the interface used to access the rump kernel is below the system call layer. We implemented microkernel callbacks for ﬁle systems (puﬀs [53]) and character /block device drivers (pud [84]). They use the NetBSD kernel VFS/vnode and cdev /bdev interfaces to access the rump kernel, respectively.
It needs to be noted that rump kernels accepting multiple diﬀerent types of clients are possible. For example, remote clients can be used to conﬁgure a rump kernel,

53
while the application logic still remains in the local client. The ability to use multiple types of clients on a single rump kernel makes it possible to reuse existing tools for the conﬁguration job and still reap the speed beneﬁt of a local client.
Rump kernels used by remote or microkernel clients always include a local client as part of the process the rump kernel is hosted in. This local client is responsible for forwarding incoming requests to the rump kernel, and sending the results back after the request has been processed.
2.3 Threads and Schedulers
Next, we will discuss the theory and concepts related to processes, threads, CPUs, scheduling and interrupts in a rump kernel. An example scenario is presented after the theory in Section 2.3.4. This subject is revisited in Section 3.3 where we discuss it from a more concrete perspective along with the implementation.
As stated earlier, a rump kernel uses the host’s process, thread and scheduling facilities. To understand why we still need to discuss this topic, let us ﬁrst consider what a thread represents to an operating system. First, a thread represents machine execution context, such as the program counter, other registers and the virtual memory address space. We call this machine context the hard context. It determines how machine instructions will be executed when a thread is running on a CPU and what their eﬀects will be. The hard context is determined by the platform that the thread runs on. Second, a thread represents all auxiliary data required by the operating system. We call this auxiliary data the soft context. It comprises for example of information determining which process a thread belongs to, and e.g. therefore what credentials and ﬁle descriptors it has. The soft context is determined by the operating system.

54
To further illustrate, we go over a simpliﬁed version of what happens in NetBSD when an application process creates a thread:
1. The application calls pthread_create() and passes in the necessary parameters, including the address of the new thread’s start routine.
2. The pthread library does the necessary initialization, including stack allocation. It creates a hard context by calling _lwp_makecontext() and passing the start routine’s address as an argument. The pthread library then invokes the _lwp_create() system call.
3. The host kernel creates the kernel soft context for the new thread and the thread is put into the run queue.
4. The newly created thread will be scheduled and begin execution at some point in the future.
A rump kernel uses host threads for the hard context. Local client threads which call a rump kernel are created as described above. Since host thread creation does not involve the rump kernel, a host thread does not get an associated rump kernel thread soft context upon creation.
Nonetheless, a unique rump kernel soft context must exist for each thread executing within the rump kernel because the code we wish to run relies on it. For example, code dealing with ﬁle descriptors accesses the relevant data structure by dereferencing curlwp->l_fd 2. The soft context determines the value of curlwp.
2 curlwp is not variable in the C language sense. It is a platform-speciﬁc macro which produces a pointer to the currently executing thread’s kernel soft context. Furthermore, since ﬁle descriptors are a process concept instead of a thread concept, it would be more logical to access them via curlwp->l_proc->p_fd. The pointer is cached directly in the thread structure to avoid extra indirection.

55
We must solve the lack of a rump kernel soft context resulting from the use of host threads. Whenever a host thread makes a function call into the rump kernel, an entry point wrapper must be called. Conversely, when the rump kernel routine returns to the client, an exit point wrapper is called. These calls are done automatically for oﬃcial interfaces, and must be done manually in other cases — compare Figure 2.2 and Figure 2.3 and see that the latter includes calls to rump_schedule() and rump_unschedule(). The wrappers check the host’s thread local storage (TLS) to see if there is a rump kernel soft context associated with the host thread. The soft context may either be set or not set. We discuss both cases in the following paragraphs.
1. implicit threads: the soft context is not set in TLS. A soft context will be created dynamically and is called an implicit thread. Conversely, the implicit thread will be released at the exit point. Implicit threads are always attached to the same rump kernel process context, so callers performing multiple calls, e.g. opening a ﬁle and reading from the resulting ﬁle descriptor, will see expected results. The rump kernel thread context will be diﬀerent as the previous one no longer exists when the next call is made. A diﬀerent context does not matter, as the kernel thread context is not exposed to userspace through any portable interfaces — that would not make sense for systems which implement a threading model where userspace threads are multiplexed on top of kernel provided threads [10].
2. bound threads: the soft context is set in TLS. The rump kernel soft context in the host thread’s TLS can be set, changed and disbanded using interfaces further described in the manual page rump lwproc.3 at A–23. We call a thread with the rump kernel soft context set a bound thread. All calls to the rump kernel made from a host thread with a bound thread will be executed with the same rump kernel soft context.

56 The soft context is always set by a local client. Microkernel and remote clients are not able to directly inﬂuence their rump kernel thread and process context. Their rump kernel context is set by the local client which receives the request and makes the local call into the rump kernel.
Discussion
There are alternative approaches to implicit threads. It would be possible to require all local host threads to register with the rump kernel before making calls. The registration would create essentially a bound thread. There are two reasons why this approach was not chosen. First, it increases the inconvenience factor for casual users, as they now need a separate call per host thread. Second, some mechanism like implicit threads must be implemented anyway: allocating a rump kernel thread context requires a rump kernel context for example to be able to allocate memory for the data structures. Our implicit thread implementation doubles as a bootstrap context.
Implicit contexts are created dynamically because because any preconﬁgured reasonable amount of contexts risks application deadlock. For example, n implicit threads can be waiting inside the rump kernel for an event which is supposed to be delivered by the n + 1’th implicit thread, but only n implicit threads were precreated. Creating an amount which will never be reached (e.g. 10,000) may avoid deadlock, but is wasteful. Additionally, we assume all users aiming for high performance will use bound threads.

57
2.3.1 Kernel threads
Up until now, we have discussed the rump kernel context of threads which are created by the client, typically by calling pthread_create(). In addition, kernel threads exist. The creation of a kernel thread is initiated by the kernel and the entry point lies within the kernel. Therefore, a kernel thread always executes within the kernel except when it makes a hypercall. Kernel threads are associated with process 0 (struct proc0). An example of a kernel thread is the workqueue worker thread, which the workqueue kernel subsystem uses to schedule and execute asynchronous work units.
On a regular system, both an application process thread and a kernel thread have their hard context created by the kernel. As we mentioned before, a rump kernel cannot create a hard context. Therefore, whenever kernel thread creation is requested, the rump kernel creates the soft context and uses a hypercall to request the hard context from the host. The entry point given to the hypercall is a bouncer routine inside the rump kernel. The bouncer ﬁrst associates the kernel thread’s soft context with the newly created host thread and then proceeds to call the thread’s actual entry point.
2.3.2 A CPU for a Thread
First, let us use broad terms to describe how scheduling works in regular virtualized setup. The hypervisor has an idle CPU it wants to schedule work onto and it schedules a guest system. While the guest system is running, the guest system decides which guest threads to run and when to run them using the guest system’s scheduler. This means that there are two layers of schedulers involved in scheduling a guest thread.

58
We also point out that a guest CPU can be a purely virtual entity, e.g. the guest may support multiplexing a number of virtual CPUs on top of one host CPU. Similarly, the rump kernel may be conﬁgured to provide any number of CPUs that the guest OS supports regardless of the number of CPUs present on the host. The default for a rump kernel is to provide the same number of virtual CPUs as the number of physical CPUs on the host. Then, a rump kernel can fully utilize all the host’s CPUs, but will not waste resources on virtual CPUs where the host cannot schedule threads for them in parallel.
As a second primer for the coming discussion, we will review CPU-local algorithms. CPU-local algorithms are used avoid slow cross-CPU locking and hardware cache invalidation. Consider a pool-style resource allocator (e.g. memory): accessing a global pool is avoided as far as possible because of the aforementioned reasons of locking and cache. Instead, a CPU-local allocation cache for the pools is kept. Since the local cache is tied to the CPU, and since there can be only one thread executing on one CPU at a time, there is no need for locking other than disabling thread preemption in the kernel while the local cache is being accessed. Figure 2.5 gives an illustrative example.
The host thread doubles as the guest thread in a rump kernel and the host schedules guest threads. The guest CPU is left out of the relationship. The one-to-one relationship between the guest CPU and the guest thread must exist because CPUlocal algorithms rely on that invariant. If we remove the restriction of each rump kernel CPU running at most one thread at a time, code written against CPU-local algorithms will cause data structure corruption and fail. Therefore, it is necessary to uphold the invariant that a CPU has at most one thread executing on it at a time.
Since selection of the guest thread is handled by the host, we select the guest CPU instead. The rump kernel virtual CPU is assigned for the thread that was selected

59
void * pool_cache_get_paddr(pool_cache_t pc) {
pool_cache_cpu_t *cc;
cc = pc->pc_cpus[curcpu()->ci_index]; pcg = cc->cc_current; if (__predict_true(pcg->pcg_avail > 0)) {
/* fastpath */ object = pcg->pcg_objects[--pcg->pcg_avail].pcgo_va; return object; } else { return pool_cache_get_slow(); } }
Figure 2.5: Use of curcpu() in the pool allocator simpliﬁed as pseudocode from sys/kern/subr_pool.c. An array of CPU-local caches is indexed by the current CPU’s number to obtain a pointer to the CPU-local data structure. Lockless allocation from this cache is attempted before reaching into the global pool.
by the host, or more precisely that thread’s rump kernel soft context. Simpliﬁed, scheduling in a rump kernel can be considered picking a CPU data structure oﬀ of a freelist when a thread enters the rump kernel and returning the CPU to the freelist once a thread exits the rump kernel. A performant implementation is more delicate due to multiprocessor eﬃciency concerns. One is discussed in more detail along with the rest of the implementation in Section 3.3.1.
Scheduling a CPU and releasing it are handled at the rump kernel entrypoint and exitpoint, respectively. The BPF example with VFS (Figure 2.2) relies on rump kernel interfaces handling scheduling automatically for the clients. The BPF example which calls kernel interfaces directly (Figure 2.3) schedules a CPU before it calls a routine inside the rump kernel.

60
2.3.3 Interrupts and Preemption
An interrupt is an asynchronously occurring event which preempts the current thread and proceeds to execute a compact handler for the event before returning control back to the original thread. The interrupt mechanism allows the OS to quickly acknowledge especially hardware events and schedule the required actions for a suitable time (which may be immediately). Taking an interrupt is tied to the concept of being able to temporarily replace the currently executing thread with the interrupt handler. Kernel thread preemption is a related concept in that code currently executing in the kernel can be removed from the CPU and a higher priority thread selected instead.
The rump kernel uses a cooperative scheduling model where the currently executing thread runs to completion. There is no virtual CPU preemption, neither by interrupts nor by the scheduler. A thread holds on to the rump kernel virtual CPU until it either makes a blocking hypercall or returns from the request handler. A host thread executing inside the rump kernel may be preempted by the host. Preemption will leave the virtual CPU busy until the host reschedules the preempted thread and the thread runs to completion in the rump kernel.
What would be delivered by a preempting interrupt in the monolithic kernel is always delivered via a schedulable thread in a rump kernel. In the event that later use cases present a strong desire for fast interrupt delivery and preemption, the author’s suggestion is to create dedicated virtual rump CPUs for interrupts and real-time threads and map them to high-priority host threads. Doing so avoids interaction with the host threads via signal handlers (or similar mechanisms on other non-POSIX host architectures). It is also in compliance with the paradigm that the host handles all scheduling in a rump kernel.

61
2.3.4 An Example
We present an example to clarify the content of this subsection. Let us assume two host threads, A and B, which both act as local clients. The host schedules thread A ﬁrst. It makes a call into the rump kernel requesting a bound thread. First, the soft context for an implicit thread is created and a CPU is scheduled. The implicit thread soft context is used to create the soft context of the bound thread. The bound thread soft context is assigned to thread A and the call returns after free’ing the implicit thread and releasing the CPU. Now, thread A calls the rump kernel to access a driver. Since it has a bound thread context, only CPU scheduling is done. Thread A is running in the rump kernel and it locks mutex M. Now, the host scheduler decides to schedule thread B on the host CPU instead. There are two possible scenarios:
1. The rump kernel is a uniprocessor kernel and thread B will be blocked. This is because thread A is still scheduled onto the only rump kernel CPU. Since there is no preemption for the rump kernel context, B will be blocked until A runs and releases the rump kernel CPU. Notably, it makes no diﬀerence if thread B is an interrupt thread or not — the CPU will not be available until thread A releases it.
2. The rump kernel is a multiprocessor kernel and there is a chance that other rump kernel CPUs may be available for thread B to be scheduled on. In this case B can run.
We assume that B can run immediately. Thread B uses implicit threads, and therefore upon entering the rump kernel an implicit thread soft context gets created and assigned to thread B, along with a rump kernel CPU.

62
After having received a rump kernel CPU and thread context, thread B wants to lock mutex M. M is held, and thread B will have to block and await M’s release. Thread B will release the rump kernel CPU and sleep until A unlocks the mutex. After the mutex is unlocked, the host marks thread B as runnable and after B wakes up, it will attempt to schedule a rump kernel CPU and after that attempt to lock mutex M and continue execution. When B is done with the rump kernel call, it will return back to the application. Before doing so, the CPU will be released and the implicit thread context will be free’d.
Note that for thread A and thread B to run in parallel, both the host and the rump kernel must have multiprocessor capability. If the host is uniprocessor but the rump kernel is conﬁgured with multiple virtual CPUs, the threads can execute inside the rump kernel concurrently. In case the rump kernel is conﬁgured with only one CPU, the threads will execute within the rump kernel sequentially irrespective of if the host has one or more CPUs available for the rump kernel.
2.4 Virtual Memory
Virtual memory address space management in a rump kernel is relegated to the host because support in a rump kernel would not add value in terms of the intended use cases. The only case where full virtual memory support would be helpful would be for testing the virtual memory subsystem. Emulating page faults and memory protection in a usermode OS exhibits over tenfold performance penalty and can be signiﬁcant in other, though not all, hypervisors [11]. Therefore, supporting a corner use case was not seen worth the performance penalty in other use cases.
The implication of a rump kernel not implementing full memory protection is that it does not support accessing resources via page faults. There is no support in a rump kernel for memory mapping a ﬁle to a client. Supporting page faults inside a

63

   

 
 
 




 

 







 

Figure 2.6: Providing memory mapping support on top of a rump kernel. The ﬁle is mapped into the client’s address space by the host kernel. When non-resident pages in the mapped range are accessed by the client, a page fault is generated and the rump kernel is invoked via the host kernel’s ﬁle system code to supply the desired data.
rump kernel would not work for remote clients anyway, since the page faults need to be trapped on the client machine.
However, it is possible to provide memory mapping on top of rump kernels. In fact, when running ﬁle systems as microkernel servers, the puﬀs [53] userspace ﬁle system framework and the host kernel provide memory mapping for the microkernel client. The page fault is resolved in the host kernel, and the I/O request for paging in the necessary data sent to the rump kernel. After the rump kernel has satisﬁed the request and responded via puﬀs, the host kernel unblocks the process that caused the page fault (Figure 2.6). If a desirable use case is found, distributed shared memory [80] can be investigated for memory mapping support in remote clients.
Another implication of the lack of memory protection is that a local client can freely access the memory in a rump kernel. Consider the BPF example which accesses the kernel directly (Figure 2.3). Not only does the local client call kernel routines, it also examines the contents of a kernel data structure.

64
2.5 Distributed Services with Remote Clients
As mentioned in our client taxonomy in Section 2.2, remote clients use services from a rump kernel hosted either on the same host in another process or on a remote host. We describe the general concept here and provide implementation details later in Section 3.12.
It is known to be possible to build a Unix system call emulation library on top of a distributed system [81]. We go further: while we provide the Unix interface to applications, we also use existing Unix kernel code at the server side.
Running a client and the rump kernel on separate hosts is possible because on a fundamental level Unix already works like a distributed system: the kernel and user processes live in diﬀerent address spaces and information is explicitly moved across this boundary by the kernel. Copying data across the boundary simpliﬁes the kernel, since data handled by the kernel can always be assumed to be resident and non-changing. Explicit copy requests in the kernel code make it possible to support remote clients by implementing only a request transport layer. System calls become RPC requests from the client to the kernel and routines which copy data between arbitrary address spaces become RPC requests from the kernel to the client.
When a remote client connects to a rump kernel, it gets assigned a rump kernel process context with appropriate credentials. After the handshake is complete, the remote client can issue service requests via the standard system call interface. First, the client calls a local stub routine, which marshalls the request. The stub then sends the request to the server and blocks the caller. After the rump kernel server has processed the request and responded, the response is decoded and the client is unblocked. When the connection between a rump kernel and a remote client is severed, the rump kernel treats the client process as terminated.

65
The straightforward use of existing data structures has its limitations: the system the client is hosted on must share the same ABI with the system hosting the rump kernel. Extending support for systems which are not ABI-compatible is beyond the scope of our work. However, working remote client support shows that it is possible to build distributed systems out of a Unix codebase without the need for a new design and codebase such as Plan 9 [93].
2.6 Summary
A rump kernel is a partial virtualization of an operating system kernel with the virtualization target being the drivers. To be as lightweight as possible, a rump kernel relies on two features: relegating support functionality to the host where possible and an anykernel codebase where diﬀerent units of the kernel (e.g. networking and ﬁle systems) are disjoint enough to be usable in conﬁgurations where all parties are not present.
Rump kernels support three types of clients: local, microkernel and remote. Each client type has its unique properties and varies for example in access rights to a rump kernel, the mechanism for making requests, and performance characteristics. Remote clients are able to access a rump kernel over the Internet.
For drivers to function, a rump kernel must possess runtime context information. This information consists of the process/thread context and a unique rump kernel CPU that each thread is associated with. A rump kernel does not assume virtual memory, and does not provide support for page faults or memory protection. Virtual memory protection and page faults, where necessary, are always left to be performed by the host of the rump kernel client.

66

67
3 Implementation
The previous chapter discussed the concept of an anykernel and rump kernels. This chapter describes the code level modiﬁcations that were necessary for a production quality implementation on NetBSD. The terminology used in this chapter is mainly that of NetBSD, but the concepts are believed to apply to other similar operating systems.
3.1 Kernel Partitioning
As mentioned in Section 2.1, to maximize the lightweight nature of rump kernels, the kernel code was several logical layers: a base, three factions (dev, net and vfs) and drivers. The factions are orthogonal, meaning they do not depend on each other. Furthermore, the base does not depend on any other part of the kernel. The modiﬁcations we made to reach this goal of independence are described in this section.
As background, it is necessary to recall how the NetBSD kernel is linked. In C linkage, symbols which are unresolved at compile-time must be satisﬁed at executable link-time. For example, if a routine in file1.c wants to call myfunc() and myfunc() is not present in any of the object ﬁles or libraries being linked into an executable, the linker ﬂags an error. A monolithic kernel works in a similar fashion: all symbols must be resolved when the kernel is linked. For example, if an object ﬁle with an unresolved symbol to the kernel’s pathname lookup routine namei() is included, then either the symbol namei must be provided by another object ﬁle being linked, or the calling source module must be adjusted to avoid the call. Both approaches are useful for us and the choice depends on the context.

68
We identiﬁed three obstacles for having a partitioned kernel:
1. Compile-time deﬁnitions (#ifdef) indicating which features are present in the kernel. Compile-time deﬁnitions are ﬁne within a component, but do not work between components if linkage dependencies are created (for example a cross-component call which is conditionally included in the compilation).
2. Direct references between components where we do not allow them. An example is a reference from the base to a faction.
3. Multiclass source modules contain code which logically belongs in several components. For example, if the same ﬁle contains routines related to both ﬁle systems and networking, it belongs in this problem category.
Since our goal is to change the original monolithic kernel and its characteristics as little as possible, we wanted to avoid heavy approaches in addressing the above problems. These approaches include but are not limited to converting interfaces to be called only via pointer indirection. Instead, we observed that indirect interfaces were already used on most boundaries (e.g. struct fileops, struct protosw, etc.) and we could concentrate on the exceptions. Code was divided into functionality groups using source modules as boundaries.
The three techniques we used to address problems are as follows:
1. code moving. This solved cases where a source module belonged to several classes. Part of the code was moved to another module. This technique had to be used sparingly since it is very intrusive toward other developers who have outstanding changes in their local trees. However, we preferred moving

69
over splitting a ﬁle into several portions using #ifdef, as the ﬁnal result is clearer to anyone looking at the source tree. In some cases code, moving had positive eﬀects beyond rump kernels. One such example was splitting up sys/kern/init_sysctl.c, which had evolved to include sysctl handlers for many diﬀerent pieces of functionality. For example, it contained the routines necessary to retrieve a process listing. Moving the process listing routines to the source ﬁle dealing with process management (sys/kern/kern_proc.c) not only solved problems with references to factions, but also grouped related code and made it easier to locate.
2. function pointers. Converting direct references to calls via function pointers removes link-time restrictions. A function pointer gets a default value at compile time. Usually this value is a stub indicating the requested feature is not present. At runtime the pointer may be adjusted to point to an actual implementation of the feature if it is present.
3. weak symbols. A weak symbol is one which is used in case no other definition is available. If a non-weak symbol is linked in the same set, it is used and the weak symbol is silently ignored by the linker. Essentially, weak symbols can be used to provide a default stub implementation. However, as opposed to function pointers, weak symbols are a linker concept instead of a program concept. It is not possible to deduce behavior by looking at the code at the callsite. Unlike a function pointer, using a weak symbol also prevents a program from reverting from a non-weak deﬁnition back to a weak deﬁnition. Therefore, the weak symbol technique cannot be used anywhere where code can be unloaded at runtime. We used stub implementations deﬁned by weak symbols very sparingly because of the above reasons and preferred other approaches.
Finally, to illustrate the problems and techniques, we discuss the modiﬁcations to the ﬁle sys/kern/kern_module.c. The source module in question provides support

70
for loadable kernel modules (discussed further in Section 3.8.1). Originally, the ﬁle contained routines both for loading kernel modules from the ﬁle system and for keeping track of them. Having both in one module was a valid possibility before the anykernel faction model. In the anykernel model, loading modules from a ﬁle system is VFS functionality, while keeping track of the modules is base functionality.
To make the code comply with the anykernel model, we used the code moving technique to move all code related to ﬁle system access to its own source ﬁle in kern_module_vfs.c. Since loading from a ﬁle system must still be initiated by the kernel module management routines, we introduced a function pointer interface. By default, it is initialized to a stub:
int (*module_load_vfs_vec)(const char *, int, bool, module_t *, prop_dictionary_t *) = (void *)eopnotsupp;
If VFS is present, the routine module_load_vfs_init() is called during VFS subsystem init after the vfs_mountroot() routine has successfully completed to set the value of the function pointer to module_load_vfs(). In addition to avoiding a direct reference from the base to a faction in rump kernels, this pointer has another beneﬁt: during bootstrap it protects the kernel from accidentally trying to load kernel modules from the ﬁle system before a ﬁle system has been mounted 3.
3.1.1 Extracting and Implementing
We have two methods for providing functionality in the rump kernel: we can extract it out of the kernel sources, meaning we use the source ﬁle as such, or we can implement it, meaning that we do an implementation suitable for use in a rump
3sys/kern/vfs_subr.c rev 1.401

71
kernel. We work on a source ﬁle granularity level, which means that either all of an existing source ﬁle is extracted, or the necessary routines from it (which may be all of them) are implemented. Implemented source ﬁles are placed under /sys/rump, while extracted ones are picked up by Makeﬁles from other subdirectories under /sys.
The goal is to extract as much as possible for the features we desire. Broadly speaking, there are three cases where extraction is not possible.
1. code that does not exist in the regular kernel: this means drivers speciﬁc to rump kernels. Examples include anything using rump hypercalls, such as the virtual block device driver.
2. code dealing with concepts not supported in rump kernels. An example is the virtual memory fault handler: when it is necessary to call a routine which in a regular kernel is invoked from the fault hander, it must be done from implemented code. It should be noted, though, that not all VM code should automatically be disqualiﬁed from extraction. For instance, VM readahead code is an algorithm which does not have anything per se to do with VM, and we have extracted it.
3. bypassed layers such as scheduling. They need completely diﬀerent handling.
In some cases a source module contained code which was desirable to be extracted, but it was not possible to use the whole source module because others parts were not suitable for extraction. Here we applied the code moving technique. As an example, we once again look at the code dealing with processes (kern_proc.c). The source module contained mostly process data structure management routines,

72
e.g. the routine for mapping a process ID number (pid_t) to the structure describing it (struct proc *). We were interested in being able to extract this code. However, the same ﬁle also contained the deﬁnition of the lwp0 variable. Since that deﬁnition included references to the scheduler (“concept not supported in a rump kernel”), we could not extract the ﬁle as such. However, after moving the deﬁnition of lwp0 to kern_lwp.c, where it arguably belongs, kern_proc.c could be extracted.
3.1.2 Providing Components
On a POSIX system, the natural way to provide components to be linked together is with libraries. Rump kernel components are compiled as part of the regular system build and installed as libraries into /usr/lib. The kernel base library is called librump and the hypervisor library is called librumpuser. The factions are installed with the names librumpdev, librumpnet and librumpvfs for dev, net and vfs, respectively. The driver components are named with the pattern librump<faction>_driver, e.g. librumpfs_nfs (NFS client driver). The faction part of the name is an indication of what type of driver is in question, but it does not convey deﬁnitive information on what the driver’s dependencies are. For example, consider the NFS client: while it is a ﬁle system driver, it also depends on networking.
By default, NetBSD installs two production variants of each library: a static library and a shared library. The system default is to produce dynamically linked executables which use shared libraries, and this approach is also used when linking rump kernels. In custom built rump kernels the choice is up to the user. Shared libraries allow the host to load the components into physical memory only once irrespective of how many rump kernel instances are started, but shared libraries have worse performance due to indirection [39]. Figure 3.1 illustrates the speed penalty inherent to the position independent code in shared libraries by measuring the time it takes

73

runtime (s)

2

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

none

hv

base base+hv

Figure 3.1: Performance of position independent code (PIC). A regular kernel is compiled as non-PIC code. This compilation mode is eﬀectively the same as “none” in the graph. If the hypervisor and rump kernel base use PIC code, the execution time increases as is expected. In other words, rump kernels allow to make a decision on the tradeoﬀ between execution speed and memory use.

to create and disband 300k threads in a rump kernel. As can be deduced from the combinations, shared and static libraries can be mixed in a single rump kernel instance so as to further optimize the behavior with the memory/CPU tradeoﬀ.

3.2 Running the Kernel in an Hosted Environment
Software always runs on top of an entity which provides the interfaces necessary for the software to run. A typical operating system kernel runs on top of hardware and uses the hardware’s “interfaces” to fulﬁll its needs. When running on top a hardware emulator the emulator provides the same hardware interfaces. In a paravirtualized setup the hypervisor provides the necessary interfaces. In a usermode OS setup, the application environment of the hosting OS makes up the hypervisor. In this section we discuss hosting a rump kernel in a process on a POSIX host.

74
3.2.1 C Symbol Namespaces
In the regular case, the kernel and process C namespaces are disjoint. Both the kernel and application can contain the same symbol name, for example printf, without a collision occurring. When we run the kernel in a process container, we must take care to preserve this property. Calls to printf made by the client still need to go to libc, while calls to printf made by the kernel need to be handled by the in-kernel implementation.
Single address space operating systems provide a solution [23], but require a different calling convention. On the other hand, C preprocessor macros were used by OSKit [34] to rename conﬂicting symbols and allow multiple diﬀerent namespaces to be linked together. UML [26] uses a variant of the same technique and renames colliding symbols using a set of preprocessor macros in the kernel build Makeﬁle, e.g. -Dsigprocmask=kernel_sigprocmask. This manual renaming approach is inadequate for a rump kernel; unlike a usermode OS kernel which is an executable application, a rump kernel is a library which is compiled, shipped, and may be linked with any other libraries afterwards. This set of libraries is not available at compile time and therefore we cannot know which symbols will cause conﬂicts at link time. Therefore, the only option is to assume that any symbol may cause a conﬂict.
We address the issue by protecting all symbols within the rump kernel. The objcopy utility’s rename functionality is used ensure that all symbols within the rump kernel have a preﬁx starting with “rump” or “RUMP”. Symbol names which do not begin with “rump” or “RUMP” are renamed to contain the preﬁx “rumpns ”. After renaming, the kernel printf symbol will be seen as rumpns_printf by the linker. Preﬁxes are illustrated in Figure 3.2: callers outside of a rump kernel must include the preﬁx explicitly, while the preﬁx for routines inside a rump kernel is implicit since it is automatically added by objcopy. Table 3.1 illustrates further by providing examples of the outcome of renaming.

75

namespace: rump

implicit

rump_sys()

rump_func()

explicit

application_func() namespace: std

Figure 3.2: C namespace protection. When referencing a rump kernel symbol from outside of the rump kernel, the preﬁx must be explicitly included in the code. All references from inside the rump kernel implicitly contain the preﬁx due to bulk symbol renaming. Corollary: it is not possible to access a symbol outside the rump kernel namespace from inside the rump kernel.

rump kernel object original symbol name symbol after renaming

yes

rump_sys_call

rump_sys_call

yes

printf

rumpns_printf

no

rump_sys_call

rump_sys_call

no

printf

printf

Table 3.1: Symbol renaming illustrated. Objects belonging to a rump kernel have their exported symbols and symbol dereferences renamed, if necessary, so that they are inside the rump kernel namespace. Objects which do not belong to a rump kernel are not aﬀected.

However, renaming all symbols also creates a problem. Not all symbols in a kernel object ﬁle come from kernel source code. Some symbols are a property of the toolchain. An example is _GLOBAL_OFFSET_TABLE_, which is used by position independent code to store the oﬀsets. Renaming toolchain-generated symbols causes failures, since the toolchain expects to ﬁnd symbols where it left them.
We observed that almost all of the GNU toolchain’s symbols are in the doubleunderscore namespace “ ”, whereas the NetBSD kernel exported under 10 symbols in that namespace. The decision was to rename existing kernel symbols in

76
the double underscore namespace to a single underscore namespace and exclude the double underscore namespace from the rename. There were two exceptions to the double underscore rule which had to be excluded from the rename as well: _GLOBAL_OFFSET_TABLE_ and architecture speciﬁc ones. We handle the architecture speciﬁc ones with a quirk table. There is one quirk each for PA-RISC, MIPS, and PowerPC64. For example, the MIPS toolchain generates the symbol _gp_disp, which needs to be excluded from the renaming. Experience of over 2.5 years shows that once support for an architecture is added, no maintenance is required.
We conclude mass renaming symbols is a practical and feasible solution for the symbol collision problem which, unlike manual renaming, does not require knowledge of the set of symbols that the application namespace exports.
3.2.2 Privileged Instructions
Kernel code dealing with for example the MMU may execute CPU instructions which are available only in privileged mode. Executing privileged instructions while in non-privileged mode should cause a trap and the host OS or VMM to take control. Typically, this trap will result in process termination.
Virtualization and CPU emulation technologies solve the problem by not executing privileged instructions on the host CPU. For example, Xen [11] uses hypercalls, User Mode Linux [26] does not use privileged instructions in the usermode machine dependent code, and QEMU [13] handles such instructions in the machine emulator.
In practice kernel drivers do not use privileged instructions because they are found only in the architecture speciﬁc parts of the kernel. Therefore, we can solve the problem by deﬁning that it does not exist in our model — if there are any it is a failure in modifying the OS to support rump kernels.

77
3.2.3 The Hypercall Interface
The hypercall library implements the hypercall interface for the particular host. On a POSIX host, the library is called librumpuser and the source code can be found from lib/librumpuser. For historical reasons, the interface itself is also called rumpuser, although for example rumphyper would be a more descriptive name.
As an example of a hypercall implementation we consider allocating “physical” memory from the host. This hypercall is implemented by the hypercall library with a call to posix_memalign()4. Conversely, the hypercall for releasing memory back to the system is implemented with a call to free().
Theoretically, a rump kernel can accomplish this by calling the host directly. However, there are a number of reasons a separate hypercall interface is better.
1. The user namespace is not available in the kernel. To for example make the posix_memalign() call, the rump kernel has to deﬁne the prototype for the function itself. If the prototype matches what libc expects, the call is correctly resolved at link time. The downside is that duplicated information can always go out-of-date. Furthermore, calling out of the kernel is not directly possible due to the symbol renaming we presented in Section 3.2.1.
2. A separate interface helps when hosting a rump kernel on a platform which is not native to the rump kernel version. For example, while all POSIX platforms provide stat(const char *path, struct stat *sb), there is no standard for the binary representation of struct stat. Therefore, a reference to the structure cannot be passed over the interface as binary data,
4 posix_memalign() is essentially malloc(), but it takes an alignment parameter in addition to the size parameter. Kernel code assumes that allocating a page of memory will return it from a page-aligned oﬀset, and using posix_memalign() instead of malloc() allows to guarantee that memory allocated by the hypercall will be page-aligned.

78
since the representation might not be the same in the kernel and the hypercall library. Even diﬀerent versions of NetBSD have diﬀerent representations, for example due to increasing time_t from 32bit to 64bit. Therefore, to have a well-deﬁned interface when running on a non-native host, the hypercall interface should only use data types which are the same regardless of ABI. For example, the C99 constant width type uint64_t is preferred over time_t. Since the hypercall executes in the same process as the rump kernel, byte order is not an issue, and the native one can be used. 3. A separate interface helps future investigation of running rump kernels on non-POSIX platforms, such as microkernels. This investigation, however, will require adjustments to the hypercall interface. While the need for adjustments is foreseeable, the exact forms of the adjustments are not, and that is the reason they are not already in place.
The header ﬁle sys/rump/include/rump/rumpuser.h deﬁnes the hypercall interfaces. All hypercalls by convention begin with the string “rumpuser”. This prevents hypercall interface references in the rump kernel from falling under the jurisdiction of symbol renaming.
We divide the hypercall interfaces into mandatory and optional ones. The routines that must be implemented by the rumpuser library are:
• memory management: allocate aligned memory, free • thread management: create and join threads, TLS access • synchronization routines: mutex, read/write lock, condition variable.
This class is illustrated in Figure 3.3. • exit: terminate the host container. In a POSIX environment depending on
the parameters termination is done by calling either abort() or exit().

79
void rumpuser_mutex_init(struct rumpuser_mtx **); void rumpuser_mutex_init_kmutex(struct rumpuser_mtx **); void rumpuser_mutex_enter(struct rumpuser_mtx *); int rumpuser_mutex_tryenter(struct rumpuser_mtx *); void rumpuser_mutex_exit(struct rumpuser_mtx *); void rumpuser_mutex_destroy(struct rumpuser_mtx *); struct lwp *rumpuser_mutex_owner(struct rumpuser_mtx *);
void rumpuser_rw_init(struct rumpuser_rw **); void rumpuser_rw_enter(struct rumpuser_rw *, int); int rumpuser_rw_tryenter(struct rumpuser_rw *, int); void rumpuser_rw_exit(struct rumpuser_rw *); void rumpuser_rw_destroy(struct rumpuser_rw *); int rumpuser_rw_held(struct rumpuser_rw *); int rumpuser_rw_rdheld(struct rumpuser_rw *); int rumpuser_rw_wrheld(struct rumpuser_rw *);
void rumpuser_cv_init(struct rumpuser_cv **); void rumpuser_cv_destroy(struct rumpuser_cv *); void rumpuser_cv_wait(struct rumpuser_cv *, struct rumpuser_mtx *); int rumpuser_cv_timedwait(struct rumpuser_cv *, struct rumpuser_mtx *,
int64_t, int64_t); void rumpuser_cv_signal(struct rumpuser_cv *); void rumpuser_cv_broadcast(struct rumpuser_cv *); int rumpuser_cv_has_waiters(struct rumpuser_cv *);
Figure 3.3: Hypercall locking interfaces. These interfaces use opaque lock data types to map NetBSD kernel locking operations to hypervisor operations.
Strictly speaking, the necessity of thread support and locking depends on the drivers being executed. We oﬀer the following anecdote and discussion. At one point when working on rump kernel support, support for gdb in NetBSD was broken so that threaded programs could not be single-stepped (the problem has since been ﬁxed). As a way to work around the problem, the variable RUMP_THREADS was created. If it is set to 0, the rump kernel silently ignores kernel thread creation requests. Despite the lack of threads, for example ﬁle system drivers still function, because they do not directly depend on worker threads. The ability to run without threads allowed attaching the broken debugger to rump kernels and single-stepping them.

80
The following rumpuser interfaces called from a rump kernel can provide added functionality. Their implementation is optional.
• host ﬁle access and I/O: open, read/write. Host I/O is necessary if any host resources are to be accessed via ﬁles. Examples include accessing a ﬁle containing a ﬁle system image or accessing a host device via /dev.
• I/O multiplexing: strictly speaking, multiplexing operations such as poll() can be handled with threads and synchronous I/O operations. However, it is often more convenient to multiplex, and additionally it has been useful in working around at least one host system bug5.
• scatter-gather I/O: can be used for optimizing the virtual network interface to transmit data directly from mbufs[114].
• symbol management and external linking: this is akin to the task of a bootloader/ﬁrmware in a regular system, and is required only if dynamically extending the rump kernel is desired (see Section 3.8.1).
• host networking stack access: this is necessary for the sockin protocol module (Section 3.9.1).
• errno handling: If system calls are to be made, the hypervisor must be able to set a host thread-speciﬁc errno so that the client can read it. Note: errno handling is unnecessary if the clients do not use the rump system call API.
• putchar: output character onto console. Being able to print console output is helpful for debugging purposes.
• printf : a printf-like call. see discussion below.
5 rev 1.11 of sys/rump/net/lib/libvirtif/if_virt.c.

81
The Beneﬁt of a printf-like Hypercall
The rumpuser_dprintf() call has the same calling convention as the NetBSD kernel printf() routine. It is used to write debug output onto the console, or elsewhere if the implementation so chooses. While the kernel printf() routine can be used to produce debug output via rumpuser_putchar(), the kernel printf routine inkernel locks to synchronize with other in-kernel consumers of the same interface. These locking operations may cause the rump kernel virtual CPU context to be relinquished, which in turn may cause inaccuracies in debug prints especially when hunting racy bugs. Since the hypercall runs outside of the kernel, and will not unschedule the current rump kernel virtual CPU, we found that debugging information produced by it is much more accurate. Additionally, a hypercall can be executed without a rump kernel context. This property was invaluable when working on the low levels of the rump kernel itself, such as thread management and CPU scheduling.
3.3 Rump Kernel Entry and Exit
As we discussed in Chapter 2, a client must possess an execution context before it can successfully operate in a rump kernel. These resources consist of a rump kernel process/thread context and a virtual CPU context. The act of ensuring that these resources have been created and selected is presented as pseudocode in Figure 3.4 and available as real code in sys/rump/librump/rumpkern/scheduler.c. We will discuss obtaining the thread context ﬁrst.
Recall from Section 2.3 that there are two types of thread contexts: an implicit one which is dynamically created when a rump kernel is entered and a bound one which the client thread has statically set. We assume that all clients which are critical about their performance use bound threads.

82
The entry point rump_schedule()6 starts by checking if the host thread has a bound rump kernel thread context. This check maps to consulting the host’s thread local storage with a hypercall. If a value is set, it is used and the entrypoint can move to scheduling a CPU.
In case an implicit thread is required, it is necessary to create one. We use the system thread lwp0 as the bootstrap context for creating the implicit thread. Since there is only one instance of this resource, it may be used only by a single consumer at a time, and must be locked before use. After a lock on lwp0 has been obtained, a CPU is scheduled for it. Next, the implicit thread is created and it is given the same CPU we obtained for lwp0. Finally, lwp0 is unlocked and servicing the rump kernel request can begin.
The exit point is the converse: in case we were using a bound thread, just releasing the CPU is enough. In case an implicit thread was used it must be released. Again, we need a thread context to do the work and again we use lwp0. A critical detail is noting the resource acquiry order: the CPU must be unscheduled before lwp0 can be locked. Next, a CPU must be scheduled for lwp0 via the normal path. Attempting to obtain lwp0 while holding on to the CPU may lead to a deadlock.
Instead of allocating and free’ing an implicit context at every entry and exit point, respectively, a possibility is to cache them. Since we assume that all performanceconscious clients use bound threads, caching would add unwarranted complexity to the code.
6 rump_schedule() / rump_unschedule() are slight misnomers and for example rump_enter() / rump_exit() would be more descriptive. The interfaces are exposed to clients, so changing the names is not worth the eﬀort anymore.

83
void rump_schedule() {
struct lwp *lwp;
if (__predict_true(lwp = get_curlwp()) != NULL) { rump_schedule_cpu(lwp);
} else { lwp0busy();
/* allocate & use implicit thread. uses lwp0’s cpu */ rump_schedule_cpu(&lwp0); lwp = rump_lwproc_allocateimplicit(); set_curlwp(lwp);
lwp0rele(); } }
void rump_unschedule() {
struct lwp *lwp = get_curlwp();
rump_unschedule_cpu(lwp); if (__predict_false(is_implicit(lwp))) {
lwp0busy();
rump_schedule_cpu(&lwp0); rump_lwproc_releaseimplicit(lwp);
lwp0rele(); set_curlwp(NULL); } }
Figure 3.4: rump kernel entry/exit pseudocode. The entrypoint and exitpoint are rump_schedule() and rump_unschedule(), respectively. The assignment of a CPU and implicit thread context are handled here.

84
3.3.1 CPU Scheduling
Recall from Section 2.3.2 that the purpose of the rump kernel CPU scheduler is to map the currently executing thread to a unique rump CPU context. In addition to doing this mapping at the entry and exit points as described above, it must also be done around potentially blocking hypercalls as well. One reason for releasing the CPU around hypercalls is because the wakeup condition for the hypercall may depend on another thread being able to run. Holding on to the CPU could lead to zero available CPUs for performing a wakeup, and the system would deadlock.
The straightforward solution is to maintain a list of free virtual CPUs: allocation is done by taking an entry oﬀ the list and releasing is done by putting it back on the list. A list works well for uniprocessor hosts. However, on a multiprocessor system with multiple threads, a global list causes cache contention and lock contention. The eﬀects of cache contention can be seen from Figure 3.5 which compares the wall time for executing 5 million getpid() calls per thread per CPU. This run was done 10 times, and the standard deviation is included in the graph (if it is not visible, it is practically nonexistent). The multiprocessor run took approximately three times as long as the uniprocessor one — doubling the number of CPUs made the normalized workload slower. To optimize the multiprocessor case, we developed an improved CPU scheduling algorithm.
Improved algorithm
The purpose of a rump CPU scheduling algorithm is twofold: ﬁrst, it ensures that at most one thread is using the CPU at any point in time. Second, it ensures that cache coherency is upheld. We dissect the latter point further. On a physical system, when thread A relinquishes a CPU and thread B is scheduled onto the same CPU, both threads will run on the same physical CPU, and therefore all data they see in the

85

seconds

10 8 6 4 2 0
native

1CPU rump

2CPU

Figure 3.5: System call performance using the trivial CPU scheduler. While a system call into the rump kernel is faster in a single-threaded process, it is both jittery and slow for a multithreaded process. This deﬁciency is something we address with the advanced rump kernel CPU scheduler presented later.

CPU-local cache will trivially be coherent. In a rump kernel, when host thread A relinquishes the rump kernel virtual CPU, host thread B may acquire the same rump kernel virtual CPU on a diﬀerent physical CPU. Unless the physical CPU caches are properly updated, thread B may see incorrect data. The simple way to handle cache coherency is to do a full cache update at every scheduling point. However, a full update is wasteful in the case where a host thread is continuously scheduled onto the same rump kernel virtual CPU.
The improved algorithm for CPU scheduling is presented as pseudocode in Figure 3.6. It is available as code in sys/rump/librump/rumpkern/scheduler.c. The scheduler is optimized for the case where the number of active worker threads is smaller than the number of conﬁgured virtual CPUs. This assumption is reasonable for rump kernels, since the amount of virtual CPUs can be conﬁgured based on each individual application scenario.

86
The fastpath is taken in cases where the same thread schedules the rump kernel consecutively without any other thread running on the virtual CPU in between. The fastpath not only applies to the entry point, but also to relinquishing and rescheduling a CPU during a blocking hypercall. The implementation uses atomic operations to minimize the need for memory barriers which are required by full locks.
Next, we oﬀer a verbose explanation of the scheduling algorithm.
1. Use atomic compare-and-swap (CAS) to check if we were the previous thread to be associated with the CPU. If that is the case, we have locked the CPU and the scheduling fastpath was successful.
2. The slow path does a full mutex lock to synchronize against another thread releasing the CPU. In addition to enabling a race-free sleeping wait, using a lock makes sure the cache of the physical CPU the thread is running on is up-to-date.
3. Mark the CPU as wanted with an atomic swap. We examine the return value and if we notice the CPU was no longer busy at that point, try to mark it busy with atomic CAS. If the CAS succeeds, we have successfully scheduled the CPU. We proceed to release the lock we took in step 2. If the CAS did not succeed, check if we want to migrate the lwp to another CPU.
4. In case the target CPU was busy and we did not choose to migrate to another CPU, wait for the CPU to be released. After we have woken up, loop and recheck if the CPU is available now. We must do a full check to prevent races against a third thread which also wanted to use the CPU.

87
void schedule_cpu() {
struct lwp *lwp = curlwp;
/* 1: fastpath */ cpu = lwp->prevcpu; if (atomic_cas(cpu->prevlwp, lwp, CPU_BUSY) == lwp)
return;
/* 2: slowpath */ mutex_enter(cpu->mutex); for (;;) {
/* 3: signal we want the CPU */ old = atomic_swap(cpu->prevlwp, CPU_WANTED); if (old != CPU_BUSY && old != CPU_WANTED) {
membar(); if (atomic_cas(cpu->prevlwp, CPU_WANTED, CPU_BUSY) == CPU_WANTED) {
break; } } newcpu = migrate(lwp, cpu); if (newcpu != cpu) { continue; }
/* 4: wait for CPU */ cpu->wanted++; cv_wait(cpu->cv, cpu->mutex); cpu->wanted--; } mutex_exit(cpu->mutex); return; }
Figure 3.6: CPU scheduling algorithm in pseudocode. See the text for a detailed description.

88
Releasing a CPU requires the following steps. The pseudocode is presented in Figure 3.7. The fastpath is taken if no other thread wanted to take the CPU while the current thread was using it.
1. Issue a memory barrier: even if the CPU is currently not wanted, we must perform this step. In more detail, the problematic case is as follows. Immediately after we release the rump CPU, the same rump CPU may be acquired by another hardware thread running on another physical CPU. Although the scheduling operation must go through the slowpath, unless we issue the memory barrier before releasing the CPU, the releasing CPU may have cached data which has not reached global visibility.
2. Release the CPU with an atomic swap. The return value of the swap is used to determine if any other thread is waiting for the CPU. If there are no waiters for the CPU, the fastpath is complete.
3. If there are waiters, take the CPU lock and perform a wakeup. The lock necessary to avoid race conditions with the slow path of schedule_cpu().

89
void unschedule_cpu() {
struct lwp *lwp = curlwp; /* 1: membar */ membar(); /* 2: release cpu */ old = atomic_swap(cpu->prevlwp, lwp); if (old == CPU_BUSY) {
return; } /* 3: wake up waiters */ mutex_enter(cpu->mutex); if (cpu->wanted)
cv_broadcast(cpu->cv); mutex_exit(cpu->mutex); return; }
Figure 3.7: CPU release algorithm in pseudocode. See the text for a detailed description.

90

seconds

10 8 6 4 2 0
native

1CPU rump old

2CPU rump new

Figure 3.8: System call performance using the improved CPU scheduler. The advanced rump kernel CPU scheduler is lockless and cache conscious. With it, simultaneous system calls from multiple threads are over twice as fast as against the host kernel and over four times as fast as with the old scheduler.

Performance
The impact of the improved CPU scheduling algorithm is shown in Figure 3.8. The new algorithm performs four times as good as the freelist algorithm in the dual CPU multithreaded case. It also performs twice as fast as a host kernel system call. Also, there is scalability: the dual CPU case is within 1% of the performance of the single CPU case — native performance is 20% weaker with two CPUs. Finally, the jitter we set out to eliminate has been eliminated.

CPU-bound lwps
A CPU-bound lwp will execute only on a speciﬁc CPU. This functionality is required for example for delivering a clock interrupt on every virtual CPU. Any lwp which

91
is bound to a certain rump kernel virtual CPU simply has migration disabled. This way, the scheduler will always try to acquire the same CPU for the thread.
Scheduler Priorities
The assumption is that a rump kernel is conﬁgured with a number of virtual CPUs which is equal or greater to the number of frequently executing threads. Despite this conﬁguration, a rump kernel may run into a situation where there will be competition for virtual CPUs. There are two ways to approach the issue of deciding in which order threads should be given a rump CPU context: build priority support into the rump CPU scheduler or rely on host thread priorities.
To examine the merits of having priority support in the rump CPU scheduler, we consider the following scenario. Thread A has higher priority than thread B in the rump kernel. Both are waiting for the same rump kernel virtual CPU. Even if the rump CPU scheduler denies thread B entry because the higher priority thread A is waiting for access, there is no guarantee that the host schedules thread A before thread B could theoretically run to completion in the rump kernel. By this logic, it is better to let host priorities dictate, and hand out rump kernel CPUs on a ﬁrst-come-ﬁrst-serve basis. Therefore, we do not support thread priorities in the rump CPU scheduler. It is the client’s task to call pthread_setschedparam() or equivalent if it wants to set a thread’s priority.
3.3.2 Interrupts and Soft Interrupts
As mentioned in Section 2.3.3, a rump kernel CPU cannot be preempted. The mechanism of how an interrupt gets delivered requires preemption, so we must examine that we meet the requirements of both hardware interrupts and soft interrupts.

92
Hardware interrupt handlers are typically structured to only do a minimal amount of work for acknowledging the hardware. They then schedule the bulk work to be done in a soft interrupt (softint) handler at a time when the OS deems suitable.
As mentioned in Section 2.3.3, we implement hardware interrupts as host threads which schedule a rump kernel CPU like other consumers, run the handler, and release the CPU. The only diﬀerence to a regular system is that interrupts are scheduled instead of preempting the CPU.
Softints in NetBSD are almost like regular threads. However, they have a number of special properties to keep scheduling and running them cheap:
1. Softints are run by level (e.g. networking and clock). Only one softint per level per CPU may be running, i.e. softints will run to ﬁnish before the next one may be started. Multiple outstanding softints will be queued until the currently running one has ﬁnished.
2. Softints may block brieﬂy to acquire a short-term lock, but should not sleep. This property is a corollary from the previous property.
3. Softint handlers must run on the same CPU they were scheduled on. This property relaxes cross-CPU cache eﬀects and locking requirements.
4. A softint may run only after the hardware interrupt has completed execution. That is to say, the softint handler may not run immediately after it is scheduled, only when the hardware interrupt handler that scheduled it has completed execution.
Although in a rump kernel even “hardware” interrupts are essentially software interrupts due to them being scheduled, a fair amount of code in NetBSD assumes that softints are supported. For example, the callout framework [19] schedules soft

93
interrupts from hardware clock interrupts to run periodic tasks (used e.g. by TCP timers).
The users of the kernel softint facility expect them to operate exactly according to the principles we listed. Initially, for simplicity, softints were implemented as regular threads. The use of regular threads resulted in a number of problems. For example, when the Ethernet code schedules a soft interrupt to do IP level processing for a received frame, code ﬁrst schedules the softint and only later adds the frame to the processing queue. When softints were implemented as regular threads, the host could run the softint thread before the Ethernet interrupt handler had put the frame on the processing queue. If the softint ran before the packet was queued, the packet would not be delivered until the next incoming packet was handled.
Soft interrupts are implemented in sys/rump/librump/rumpkern/intr.c according to the principles we listed earlier. The standard NetBSD implementation was not usable in a rump kernel since that implementation is based on direct interaction with the NetBSD scheduler.
3.4 Virtual Memory Subsystem
The main purpose of the NetBSD virtual memory subsystem is to manage memory address spaces and the mappings to the backing content [20]. While the memory address spaces of a rump kernel and its clients are managed by their respective hosts, the virtual memory subsystem is conceptually exposed throughout the kernel. For example, ﬁle systems are tightly built around being able to use virtual memory subsystem data structures to cache ﬁle data. To illustrate, the standard way the kernel reads data from a ﬁle system is to memory map the ﬁle, access the mapped range, and possibly fault in missing data [101].

94
Due to the design choice that a rump kernel does not use (nor require) a hardware MMU, the virtual memory subsystem implementation is diﬀerent from the regular NetBSD VM. As already explained in Section 2.4, the most fundamental diﬀerence is that there is no concept of page protection or a page fault inside the rump kernel.
The details of the rump kernel VM implementation along with their implications are described in the following subsections. The VM is implemented in the source module sys/rump/librump/rumpkern/vm.c. Additionally, routines used purely by the ﬁle system faction are in sys/rump/librump/rumpvfs/vm_vfs.c.
Pages
When running on hardware, the pages described by the struct vmpage data structure correspond with hardware pages7. Since the rump kernel does not interface with memory management hardware, the size of the memory page is merely a programmatical construct: the kernel hands out physical memory in multiples of the page size. In a rump kernel this memory is allocated from the host and since there is no memory protection or faults, the page size can in practice be any power of two within a sensible size range. However, so far there has been no reason to use anything diﬀerent than the page size for the machine architecture the rump kernel is running on.
The VM tracks status of when a page was last used. It does this tracking either by asking the MMU on CPU architectures where that is supported, e.g. i386, or by using memory protection and updating the information during page faults on architectures where it is not, e.g. alpha. This information is used by the page daemon during memory shortages to decide which pages are best suited to be paged
7 This correspondence is not a strict rule. For example the NetBSD VAX port uses clusters of 512 byte contiguous hardware pages to create logical 4kB pages to minimize management overhead.

95
to secondary storage so that memory can be reclaimed for other purposes. Instead of requiring a MMU to keep track of page usage, we observe that since memory pages allocated from a rump kernel cannot be mapped into a client’s address space, the pages are used only in kernel code. Every time kernel code wants to access a page, it does a lookup for it using uvm_pagelookup(), uses it, and releases the reference. Therefore, we hook usage information tracking to the lookup routine: whenever a lookup is done, the page is deemed as accessed.
3.4.1 Page Remapping
A regular NetBSD kernel has the ability to interact with the MMU and map physical pages to virtual addresses 8. Since a rump kernel allocates only anonymous memory from the host, it cannot ask the host to remap any of its allocations at least on a POSIX system — there is no interface for mapping anonymous memory in multiple places. Usermode operating systems typically use a memory mapped ﬁle to represent the physical memory of the virtual kernel [26, 31]. The ﬁle acts as a handle and can be mapped to the location(s) desired by the usermode kernel using the mmap() system call. The DragonFly usermode vkernel uses special host system calls to make the host kernel execute low level mappings [31].
Using a ﬁle-backed solution is in conﬂict with the lightweight fundamentals of rump kernels. First, the ﬁle must be created at a path speciﬁed either by a conﬁguration option or by a system guess. Furthermore, it disrupts the dynamic memory principle we have laid out. While it is relatively simple to extend the ﬁle to create more memory on demand, it is diﬃcult to truncate the ﬁle in case the memory is no longer needed because free pages will not automatically reside in a contiguous range at the end of the ﬁle. Finally, the approach requires virtual memory mapping support from the host and limits the environments a rump kernel can be theoretically hosted
8 NetBSD itself does not run on hardware without a MMU.

96
in. Therefore, we should critically examine if memory mapping capability is really needed in a rump kernel.
In practice, the kernel does not map physical pages in driver code. However, there is one exception we are interested in: the ﬁle system independent vnode pager. We will explain the situation in detail. The pages associated with a vnode object are cached in memory at arbitrary memory locations [101]. Consider a ﬁle which is the size of three memory pages. The content for ﬁle oﬀset 0x0000-0x0FFF might be in page X, 0x1000-0x1FFF in page X-1 and 0x2000-0x2FFF in page X+1. In other words, reading and writing a ﬁle is a scatter-gather operation. When the standard vnode pager (sys/miscfs/genfs/genfs_io.c) writes contents from memory to backing storage, it ﬁrst maps all the pages belonging to the appropriate oﬀsets in a continuous memory address by calling uvm_pagermapin(). This routine in turn uses the pmap interface to request the MMU to map the physical pages to the speciﬁed virtual memory range in the kernel’s address space. After this step, the vnode pager performs I/O on this pager window. When I/O is complete, the pager window is unmapped. Reading works essentially the same way: pages are allocated, mapped into a contiguous window, I/O is performed, and the pager window is unmapped.
To support the standard NetBSD vnode pager with its remapping feature, there are three options for dealing with uvm_pagermapin():
1. Create the window by allocating a new block of contiguous anonymous memory and use memory copy to move the contents. This approach works because pages are unavailable to other consumers during I/O; otherwise e.g. write() at an inopportune time might cause a cache ﬂush to write half old half new contents and cause a semantic break.
2. Modify the vnode pager to issue multiple I/O requests in case the backing pages for a vnode object are not at consecutive addresses.

97
3. Accept that memory remapping support is necessary in a rump kernel and handle the vnode pager using mmap() and munmap().
When comparing the ﬁrst and the second option, the principle used is that memory I/O is several orders of magnitude faster than device I/O. Therefore, anything which aﬀects device I/O should be avoided, especially if it might cause extra I/O operations and thus option 1 is preferable over option 2.
To evaluate the ﬁrst option against third option, we simulated pager conditions and measured the amount of time it takes to construct a contiguous 64kB memory window out of non-contiguous 4kB pages and to write the window out to a ﬁle backed by a memory ﬁle system. The result for 1000 loops as a function of noncontiguous pages is presented in Figure 3.9. We conclude that not only is copying the technologically preferred option since it avoids the need for memory mapping support on the host, it is also several times faster than mmap()/munmap() pairs.
It should be noted that a fourth option is to implement a separate vnode pager which does not rely on mapping pages. This option was our initial approach. While the effort produced a superﬁcially working result, we could not get all corner cases to function exactly the same as with the regular kernel — for example, the VOP_GETPAGES() interface implemented by the vnode pager takes 8 diﬀerent parameters and 14 different ﬂags. The lesson learnt from this attempt with the vnode pager reﬂects our premise for the entire work: it is easy to write superﬁcially working code, but getting all corner cases right for complicated drivers is extremely diﬃcult.
3.4.2 Memory Allocators
Although memory allocators are not strictly speaking part of the virtual memory subsystem, they are related to memory so we describe them here.

Pager window create+access time (s)

98
0.12 mmap + munmap alloc + copy + free
0.1
0.08
0.06
0.04
0.02
0 2 4 6 8 10 12 14 16 Disjoint regions
Figure 3.9: Performance of page remapping vs. copying. Allocating a pager window from anonymous memory and copying ﬁle pages to it for the purpose of pageout by the vnode pager is faster than remapping memory backed by a ﬁle. Additionally, the cost of copying is practically independent of the amount of noncontiguous pages. With remapping, each disjoint region requires a separate call to mmap().
The lowest level memory allocator in NetBSD is the UVM kernel memory allocator (uvm_km). It is used to allocate memory on a pagelevel granularity. The standard implementation in sys/uvm/uvm_km.c allocates a virtual memory address range and, if requested, allocates physical memory for the range and maps it in. Since mapping is incompatible with a rump kernel, we did a straightforward implementation which allocates a page or contiguous page range with a hypercall.
The kmem, pool and pool cache allocators are general purpose allocators meant to be used by drivers. Fundamentally, they operate by requesting pages from uvm_km and handing memory out in requested size chunks. The ﬂow of memory between UVM and the allocators is dynamic, meaning if an allocator runs out of memory, it will request more from UVM, and if there is a global memory shortage, the system will attempt to reclaim cached memory from the allocators. We have extracted the

