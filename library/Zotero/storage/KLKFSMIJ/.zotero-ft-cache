Computer Vision: Algorithms and Applications
2nd Edition Richard Szeliski
July 2, 2021 © 2021 Springer
This electronic draft is for non-commercial personal use only, and may not be posted or re-distributed in any form.
Please refer interested readers to the book’s Web site at https://szeliski.org/Book, where you can also post suggestions and feedback.

This book is dedicated to my parents, Zdzisław and Jadwiga, and my family, Lyn, Anne, and Stephen.

1 Introduction

1

What is computer vision? • A brief history • Book overview • Sample syllabus • Notation

n^

2 Image formation

33

Geometric primitives and transformations • Photometric image formation • The digital camera

3 Image processing

107

Point operators • Linear ﬁltering • Non-linear ﬁltering • Fourier transforms • Pyramids and wavelets • Geometric transformations

4 Model ﬁtting and optimization

191

Scattered data interpolation • Variational methods and regularization •
Markov random ﬁelds

5 Deep learning

235

Supervised learning • Unsupervised learning • Deep neural networks • Convolutional networks •
More complex models

6 Recognition

343

Instance recognition • Image classiﬁcation • Object detection • Semantic segmentation •
Video understanding • Vision and language

7 Feature detection and matching

417

Points and patches • Edges and contours • Contour tracking • Lines and vanishing points •
Segmentation

v

8 Image alignment and stitching

503

Pairwise alignment • Image stitching • Global alignment • Compositing

9 Motion estimation

557

Translational alignment • Parametric motion •

Optical ﬂow • Layered motion

10 Computational photography

609

Photometric calibration • High dynamic range imaging • Super-resolution, denoising, and blur removal • Image matting and compositing • Texture analysis and synthesis

11 Structure from motion and SLAM

685

Geometric intrinsic calibration • Pose estimation • Two-frame structure from motion • Multi-frame structure from motion •
Simultaneous localization and mapping (SLAM)

12 Depth estimation

753

Epipolar geometry • Sparse correspondence • Dense correspondence • Local methods •
Global optimization • Deep neural networks • Multi-view stereo • Monocular depth estimation

13 3D reconstruction

809

Shape from X • 3D scanning • Surface representations • Point-based representations • Volumetric representations • Model-based reconstruction •
Recovering texture maps and albedos

14 Image-based rendering

865

View interpolation • Layered depth images • Light ﬁelds and Lumigraphs • Environment mattes •
Video-based rendering • Neural rendering

Preface
The seeds for this book were ﬁrst planted in 2001 when Steve Seitz at the University of Washington invited me to co-teach a course called “Computer Vision for Computer Graphics”. At that time, computer vision techniques were increasingly being used in computer graphics to create image-based models of real-world objects, to create visual effects, and to merge realworld imagery using computational photography techniques. Our decision to focus on the applications of computer vision to fun problems such as image stitching and photo-based 3D modeling from personal photos seemed to resonate well with our students.
That initial course evolved into a more complete computer vision syllabus and projectoriented course structure that I used to co-teach general computer vision courses both at the University of Washington and at Stanford. (The latter was a course I co-taught with David Fleet in 2003.) Similar curricula were then adopted at a number of other universities and also incorporated into more specialized courses on computational photography. (For ideas on how to use this book in your own course, please see Table 1.1 in Section 1.4.)
This book also reﬂects my 30 years’ experience doing computer vision research in corporate research labs, mostly at Digital Equipment Corporation’s Cambridge Research Lab, Microsoft Research, and Facebook. In pursuing my work, I have mostly focused on problems and solution techniques (algorithms) that have practical real-world applications and that work well in practice. Thus, this book has more emphasis on basic techniques that work under realworld conditions and less on more esoteric mathematics that has intrinsic elegance but less practical applicability.
This book is suitable for teaching a senior-level undergraduate course in computer vision to students in both computer science and electrical engineering. I prefer students to have either an image processing or a computer graphics course as a prerequisite, so that they can spend less time learning general background mathematics and more time studying computer vision techniques. The book is also suitable for teaching graduate-level courses in computer vision, e.g., by delving into more specialized topics, and as a general reference to fundamental

viii

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

techniques and the recent research literature. To this end, I have attempted wherever possible to at least cite the newest research in each sub-ﬁeld, even if the technical details are too complex to cover in the book itself.
In teaching our courses, we have found it useful for the students to attempt a number of small implementation projects, which often build on one another, in order to get them used to working with real-world images and the challenges that these present. The students are then asked to choose an individual topic for each of their small-group, ﬁnal projects. (Sometimes these projects even turn into conference papers!) The exercises at the end of each chapter contain numerous suggestions for smaller mid-term projects, as well as more open-ended problems whose solutions are still active research topics. Wherever possible, I encourage students to try their algorithms on their own personal photographs, since this better motivates them, often leads to creative variants on the problems, and better acquaints them with the variety and complexity of real-world imagery.
In formulating and solving computer vision problems, I have often found it useful to draw inspiration from four high-level approaches:
• Scientiﬁc: build detailed models of the image formation process and develop mathematical techniques to invert these in order to recover the quantities of interest (where necessary, making simplifying assumptions to make the mathematics more tractable).
• Statistical: use probabilistic models to quantify the prior likelihood of your unknowns and the noisy measurement processes that produce the input images, then infer the best possible estimates of your desired quantities and analyze their resulting uncertainties. The inference algorithms used are often closely related to the optimization techniques used to invert the (scientiﬁc) image formation processes.
• Engineering: develop techniques that are simple to describe and implement but that are also known to work well in practice. Test these techniques to understand their limitation and failure modes, as well as their expected computational costs (run-time performance).
• Data-driven: collect a representative set of test data (ideally, with labels or groundtruth answers) and use these data to either tune or learn your model parameters, or at least to validate and quantify its performance.
These four approaches build on each other and are used throughout the book. My personal research and development philosophy (and hence the exercises in the book)
have a strong emphasis on testing algorithms. It’s too easy in computer vision to develop an

Preface

ix

algorithm that does something plausible on a few images rather than something correct. The best way to validate your algorithms is to use a three-part strategy.
First, test your algorithm on clean synthetic data, for which the exact results are known. Second, add noise to the data and evaluate how the performance degrades as a function of noise level. Finally, test the algorithm on real-world data, preferably drawn from a wide variety of sources, such as photos found on the web. Only then can you truly know if your algorithm can deal with real-world complexity, i.e., images that do not ﬁt some simpliﬁed model or assumptions.
In order to help students in this process, Appendix C includes pointers to commonly used datasets and software libraries that contain implementations of a wide variety of computer vision algorithms, which can enable you to tackle more ambitious projects (with your instructor’s consent).

Notes on the Second Edition
The last decade has seen a truly dramatic explosion in the performance and applicability of computer vision algorithms, much of it engendered by the application of machine learning algorithms to large amounts of visual training data.
Deep neural networks now play an essential role in so many vision algorithms that the new edition of this book introduces them early on as a fundamental technique that gets used extensively in subsequent chapters.
The most notable changes in the second edition include:
• Machine learning, deep learning, and deep neural networks are introduced early on in Chapter 5, as they play just as fundamental a role in vision algorithms as more classical techniques, such as image processing, graphical/probabilistic models, and energy minimization, which are introduced in the preceding two chapters.
• The recognition chapter has been moved earlier in the book to Chapter 6, since end-toend deep learning systems no longer require the development of building blocks such as feature detection, matching, and segmentation. Many of the students taking vision classes are primarily interested in visual recognition, so presenting this material earlier in the course makes it easier for students to base their ﬁnal project on these topics. This chapter also includes sections on semantic segmentation, video understanding, and vision and language.
• The application of neural networks and deep learning to myriad computer vision algorithms and applications, including ﬂow and stereo, 3D shape modeling, and newly

x

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

emerging ﬁelds such as neural rendering.
• New technologies such as SLAM (simultaneous localization and mapping) and VIO (visual inertial odometry) that now run reliably and are used in real-time applications such as augmented reality and autonomous navigation.
In addition to these larger changes, the book has been updated to reﬂect the latest state-ofthe-art techniques such as internet-scale image search and phone-based computational photography. The new edition includes over 1500 new citations (papers) and has over 200 new ﬁgures.

Acknowledgements
I would like to gratefully acknowledge all of the people whose passion for research and inquiry as well as encouragement have helped me write this book.
Steve Zucker at McGill University ﬁrst introduced me to computer vision, taught all of his students to question and debate research results and techniques, and encouraged me to pursue a graduate career in this area.
Takeo Kanade and Geoff Hinton, my PhD thesis advisors at Carnegie Mellon University, taught me the fundamentals of good research, writing, and presentation and mentored several generations of outstanding students and researchers. They ﬁred up my interest in visual processing, 3D modeling, and statistical methods, while Larry Matthies introduced me to Kalman ﬁltering and stereo matching. Geoff continues to inspire so many of us with this undiminished passion for trying to ﬁgure out “what makes the brain work” and it’s been a delight to see his pursuit of connectionist ideas bear so much fruit in this past decade.
Demetri Terzopoulos was my mentor at my ﬁrst industrial research job and taught me the ropes of successful publishing. Yvan Leclerc and Pascal Fua, colleagues from my brief interlude at SRI International, gave me new perspectives on alternative approaches to computer vision.
During my six years of research at Digital Equipment Corporation’s Cambridge Research Lab, I was fortunate to work with a great set of colleagues, including Ingrid Carlbom, Gudrun Klinker, Keith Waters, William Hsu, Richard Weiss, Ste´phane Lavalle´e, and Sing Bing Kang, as well as to supervise the ﬁrst of a long string of outstanding summer interns, including David Tonnesen, Sing Bing Kang, James Coughlan, and Harry Shum. This is also where I began my long-term collaboration with Daniel Scharstein.
At Microsoft Research, I had the outstanding fortune to work with some of the world’s best researchers in computer vision and computer graphics, including Michael Cohen, Matt

Preface

xi

Uyttendaele, Sing Bing Kang, Harry Shum, Larry Zitnick, Sudipta Sinha, Drew Steedly, Simon Baker, Johannes Kopf, Neel Joshi, Krishnan Ramnath, Anandan, Phil Torr, Antonio Criminisi, Simon Winder, Matthew Brown, Michael Goesele, Richard Hartley, Hugues Hoppe, Stephen Gortler, Steve Shafer, Matthew Turk, Georg Petschnigg, Kentaro Toyama, Ramin Zabih, Shai Avidan, Patrice Simard, Chris Pal, Nebojsa Jojic, Patrick Baudisch, Dani Lischinski, Raanan Fattal, Eric Stollnitz, David Niste´r, Blaise Aguera y Arcas, Andrew Fitzgibbon, Jamie Shotton, Wolf Kienzle, Piotr Dollar, and Ross Girshick. I was also lucky to have as interns such great students as Polina Golland, Simon Baker, Mei Han, Arno Scho¨dl, Ron Dror, Ashley Eden, Jonathan Shade, Jinxiang Chai, Rahul Swaminathan, Yanghai Tsin, Sam Hasinoff, Anat Levin, Matthew Brown, Eric Bennett, Vaibhav Vaish, Jan-Michael Frahm, James Diebel, Ce Liu, Josef Sivic, Grant Schindler, Colin Zheng, Neel Joshi, Sudipta Sinha, Zeev Farbman, Rahul Garg, Tim Cho, Yekeun Jeong, Richard Roberts, Varsha Hedau, Dilip Krishnan, Adarsh Kowdle, Edward Hsiao, Yong Seok Heo, Fabian Langguth, Andrew Owens, and Tianfan Xue. Working with such outstanding students also gave me the opportunity to collaborate with some of their amazing advisors, including Bill Freeman, Irfan Essa, Marc Pollefeys, Michael Black, Marc Levoy, and Andrew Zisserman.
Since moving to Facebook, I’ve had the pleasure to continue my collaborations with Michael Cohen, Matt Uyttendaele, Johannes Kopf, Wolf Kienzle, and Krishnan Ramnath, and also new colleagues including Kevin Matzen, Bryce Evans, Suhib Alsisan, Changil Kim, David Geraghty, Jan Herling, Nils Plath, Jan-Michael Frahm, True Price, Richard Newcombe, Thomas Whelan, Michael Goesele, Steven Lovegrove, Julian Straub, Simon Green, Brian Cabral, Michael Toksvig, Albert Para Pozzo, Laura Sevilla-Lara, Georgia Gkioxari, Justin Johnson, Chris Sweeney, and Vassileios Balntas. I’ve also had the pleasure to collaborate with some outstanding summer interns, including Tianfan Xue, Scott Wehrwein, Peter Hedman, Joel Janai, Aleksander Hołyn´ski, Xuan Luo, Rui Wang, Olivia Wiles, and Yulun Tian. I’d like to thank in particular Michael Cohen, my mentor, colleague, and friend for the last 25 years for his unwavering support of my sprint to complete this second edition.
While working at Microsoft and Facebook, I’ve also had the opportunity to collaborate with wonderful colleagues at the University of Washington, where I hold an Afﬁliate Professor appointment. I’m indebted to Tony DeRose and David Salesin, who ﬁrst encouraged me to get involved with the research going on at UW, my long-time collaborators Brian Curless, Steve Seitz, Maneesh Agrawala, Sameer Agarwal, and Yasu Furukawa, as well as the students I have had the privilege to supervise and interact with, including Fre´deric Pighin, Yung-Yu Chuang, Doug Zongker, Colin Zheng, Aseem Agarwala, Dan Goldman, Noah Snavely, Ian Simon, Rahul Garg, Ryan Kaminsky, Juliet Fiss, Aleksander Hołyn´ski, and Yifan Wang. As I mentioned at the beginning of this preface, this book owes its inception to the vision course

xii

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

that Steve Seitz invited me to co-teach, as well as to Steve’s encouragement, course notes, and editorial input.
I’m also grateful to the many other computer vision researchers who have given me so many constructive suggestions about the book, including Sing Bing Kang, who was my informal book editor, Vladimir Kolmogorov, Daniel Scharstein, Richard Hartley, Simon Baker, Noah Snavely, Bill Freeman, Svetlana Lazebnik, Matthew Turk, Jitendra Malik, Alyosha Efros, Michael Black, Brian Curless, Sameer Agarwal, Li Zhang, Deva Ramanan, Olga Veksler, Yuri Boykov, Carsten Rother, Phil Torr, Bill Triggs, Bruce Maxwell, Rico Malvar, Jana Kosˇecka´, Eero Simoncelli, Aaron Hertzmann, Antonio Torralba, Tomaso Poggio, Theo Pavlidis, Baba Vemuri, Nando de Freitas, Chuck Dyer, Song Yi, Falk Schubert, Roman Pﬂugfelder, Marshall Tappen, James Coughlan, Sammy Rogmans, Klaus Strobel, Shanmuganathan, Andreas Siebert, Yongjun Wu, Fred Pighin, Juan Cockburn, Ronald Mallet, Tim Soper, Georgios Evangelidis, Dwight Fowler, Itzik Bayaz, Daniel O’Connor, Srikrishna Bhat, and Toru Tamaki, who wrote the Japanese translation and provided many useful errata.
For the second edition, I received signiﬁcant help and advice from three key contributors. Daniel Scharstein helped me update the chapter on stereo, Matt Deitke contributed descriptions of the newest papers in deep learning, including the sections on transformers, variational autoencoders, and text-to-image synthesis, along with the exercises in Chapters 5 and 6 and some illustrations. Sing Bing Kang who reviewed multiple drafts and provided useful suggestions. I’d also like to thank Andrew Glassner, whose book (Glassner 2018) and ﬁgures were a tremendous help, Justin Johnson, Sean Bell, Ishan Misra, David Fouhey, Michael Brown, Abdelrahman Abdelhamed, Vasileios Balntas, Daniel DeTone, Frank Dellaert, Xinlei Chen, Ross Girshick, Andreas Geiger, Yann LeCun, Larry Jackel, Andrew Owens, Joel Janai, Luowei Zhou, Howard Yen, Anton Papst, Syamprasad K. Rajagopalan, Ricardo Martin Brualla, Dave Howell, Holger Heidrich, Abhishek Nagar, and Vladimir Kuznetsov.
In preparing the second edition, I taught some of the new material in two courses that I helped co-teach in 2020 at Facebook and UW. I’d like to thank my co-instructors JanMichael Frahm, Michael Goesele, Georgia Gkioxari, Ross Girshick, Jakob Julian Engel, Daniel Scharstein, Fernando de la Torre, Steve Seitz, and Harpreet Sawhney, from whom I learned a lot about the latest techniques that are included in the new edition. I’d also like to thank the TAs, including David Geraghty, True Price, Kevin Matzen, Akash Bapat, Aleksander Hołyn´ski, Keunhong Park, and Svetoslav Kolev, for the wonderful jobs they did in creating and grading the assignments. I’d like to give a special thanks to Justin Johnson, whose excellent class slides (Johnson 2020), based on earlier slides from Stanford (Li, Johnson, and Yeung 2019), taught me the fundamentals of deep learning and which I used extensively in my own class and in preparing the new chapter on deep learning.

Preface

xiii

Shena Deuchers and Ian Kingston did a fantastic job copy-editing the ﬁrst and second editions, respectively and suggesting many useful improvements, and Wayne Wheeler and Simon Rees at Springer were most helpful throughout the whole book publishing process. Keith Price’s Annotated Computer Vision Bibliography was invaluable in tracking down references and ﬁnding related work.
If you have any suggestions for improving the book, please send me an e-mail, as I would like to keep the book as accurate, informative, and timely as possible.
The last year of writing this second edition took place during the worldwide COVID-19 pandemic. I would like to thank all of the ﬁrst responders, medical and front-line workers, and everyone else who helped get us through these difﬁcult and challenging times and to acknowledge the impact that this and other recent tragedies have had on all of us.
Lastly, this book would not have been possible or worthwhile without the incredible support and encouragement of my family. I dedicate this book to my parents, Zdzisław and Jadwiga, whose love, generosity, and accomplishments have always inspired me; to my sister Basia for her lifelong friendship; and especially to Lyn, Anne, and Stephen, whose love and support in all matters (including my book projects) makes it all worthwhile.

Lake Wenatchee May, 2021

xiv

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

Contents

Preface

vii

Contents

xv

1 Introduction

1

1.1 What is computer vision? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

1.2 A brief history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

1.3 Book overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

1.4 Sample syllabus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

1.5 A note on notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

1.6 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

2 Image formation

33

2.1 Geometric primitives and transformations . . . . . . . . . . . . . . . . . . . 36

2.1.1 2D transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

2.1.2 3D transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

2.1.3 3D rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

2.1.4 3D to 2D projections . . . . . . . . . . . . . . . . . . . . . . . . . . 51

2.1.5 Lens distortions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

2.2 Photometric image formation . . . . . . . . . . . . . . . . . . . . . . . . . . 66

2.2.1 Lighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

2.2.2 Reﬂectance and shading . . . . . . . . . . . . . . . . . . . . . . . . 67

2.2.3 Optics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

2.3 The digital camera . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

2.3.1 Sampling and aliasing . . . . . . . . . . . . . . . . . . . . . . . . . 84

2.3.2 Color . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

2.3.3 Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98

xvi

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

2.4 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 2.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

3 Image processing

107

3.1 Point operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109

3.1.1 Pixel transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

3.1.2 Color transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112

3.1.3 Compositing and matting . . . . . . . . . . . . . . . . . . . . . . . . 113

3.1.4 Histogram equalization . . . . . . . . . . . . . . . . . . . . . . . . . 115

3.1.5 Application: Tonal adjustment . . . . . . . . . . . . . . . . . . . . . 119

3.2 Linear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

3.2.1 Separable ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

3.2.2 Examples of linear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . 125

3.2.3 Band-pass and steerable ﬁlters . . . . . . . . . . . . . . . . . . . . . 127

3.3 More neighborhood operators . . . . . . . . . . . . . . . . . . . . . . . . . . 131

3.3.1 Non-linear ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

3.3.2 Bilateral ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

3.3.3 Binary image processing . . . . . . . . . . . . . . . . . . . . . . . . 138

3.4 Fourier transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142

3.4.1 Two-dimensional Fourier transforms . . . . . . . . . . . . . . . . . . 146

3.4.2 Application: Sharpening, blur, and noise removal . . . . . . . . . . . 148

3.5 Pyramids and wavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149

3.5.1 Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

3.5.2 Decimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153

3.5.3 Multi-resolution representations . . . . . . . . . . . . . . . . . . . . 155

3.5.4 Wavelets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

3.5.5 Application: Image blending . . . . . . . . . . . . . . . . . . . . . . 165

3.6 Geometric transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 168

3.6.1 Parametric transformations . . . . . . . . . . . . . . . . . . . . . . . 169

3.6.2 Mesh-based warping . . . . . . . . . . . . . . . . . . . . . . . . . . 175

3.6.3 Application: Feature-based morphing . . . . . . . . . . . . . . . . . 177

3.7 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178

3.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180

4 Model ﬁtting and optimization

191

4.1 Scattered data interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . 194

4.1.1 Radial basis functions . . . . . . . . . . . . . . . . . . . . . . . . . 196

Contents

xvii

4.1.2 Overﬁtting and underﬁtting . . . . . . . . . . . . . . . . . . . . . . . 199 4.1.3 Robust data ﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . 202 4.2 Variational methods and regularization . . . . . . . . . . . . . . . . . . . . . 204 4.2.1 Discrete energy minimization . . . . . . . . . . . . . . . . . . . . . 206 4.2.2 Total variation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 4.2.3 Bilateral solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 4.2.4 Application: Interactive colorization . . . . . . . . . . . . . . . . . . 211 4.3 Markov random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 4.3.1 Conditional random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . 222 4.3.2 Application: Interactive segmentation . . . . . . . . . . . . . . . . . 227 4.4 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 4.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232

5 Deep Learning

235

5.1 Supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239

5.1.1 Nearest neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241

5.1.2 Bayesian classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . 243

5.1.3 Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 248

5.1.4 Support vector machines . . . . . . . . . . . . . . . . . . . . . . . . 250

5.1.5 Decision trees and forests . . . . . . . . . . . . . . . . . . . . . . . 254

5.2 Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

5.2.1 Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

5.2.2 K-means and Gaussians mixture models . . . . . . . . . . . . . . . . 259

5.2.3 Principal component analysis . . . . . . . . . . . . . . . . . . . . . 262

5.2.4 Manifold learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265

5.2.5 Semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . 266

5.3 Deep neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268

5.3.1 Weights and layers . . . . . . . . . . . . . . . . . . . . . . . . . . . 270

5.3.2 Activation functions . . . . . . . . . . . . . . . . . . . . . . . . . . 272

5.3.3 Regularization and normalization . . . . . . . . . . . . . . . . . . . 274

5.3.4 Loss functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279

5.3.5 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283

5.3.6 Training and optimization . . . . . . . . . . . . . . . . . . . . . . . 287

5.4 Convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . . . 290

5.4.1 Pooling and unpooling . . . . . . . . . . . . . . . . . . . . . . . . . 295

5.4.2 Application: Digit classiﬁcation . . . . . . . . . . . . . . . . . . . . 298

xviii

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

5.4.3 Network architectures . . . . . . . . . . . . . . . . . . . . . . . . . 299 5.4.4 Model zoos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304 5.4.5 Visualizing weights and activations . . . . . . . . . . . . . . . . . . 307 5.4.6 Adversarial examples . . . . . . . . . . . . . . . . . . . . . . . . . . 311 5.4.7 Self-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . 313 5.5 More complex models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 5.5.1 Three-dimensional CNNs . . . . . . . . . . . . . . . . . . . . . . . 317 5.5.2 Recurrent neural networks . . . . . . . . . . . . . . . . . . . . . . . 321 5.5.3 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323 5.5.4 Generative models . . . . . . . . . . . . . . . . . . . . . . . . . . . 328 5.6 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 5.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337

6 Recognition

343

6.1 Instance recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346

6.2 Image classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349

6.2.1 Feature-based methods . . . . . . . . . . . . . . . . . . . . . . . . . 350

6.2.2 Deep networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358

6.2.3 Application: Visual similarity search . . . . . . . . . . . . . . . . . . 360

6.2.4 Face recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363

6.3 Object detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370

6.3.1 Face detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371

6.3.2 Pedestrian detection . . . . . . . . . . . . . . . . . . . . . . . . . . 376

6.3.3 General object detection . . . . . . . . . . . . . . . . . . . . . . . . 379

6.4 Semantic segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386

6.4.1 Application: Medical image segmentation . . . . . . . . . . . . . . . 390

6.4.2 Instance segmentation . . . . . . . . . . . . . . . . . . . . . . . . . 391

6.4.3 Panoptic segmentation . . . . . . . . . . . . . . . . . . . . . . . . . 393

6.4.4 Application: Intelligent photo editing . . . . . . . . . . . . . . . . . 394

6.4.5 Pose estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395

6.5 Video understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397

6.6 Vision and language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400

6.7 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409

6.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413

Contents

xix

7 Feature detection and matching

417

7.1 Points and patches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419

7.1.1 Feature detectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422

7.1.2 Feature descriptors . . . . . . . . . . . . . . . . . . . . . . . . . . . 434

7.1.3 Feature matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441

7.1.4 Large-scale matching and retrieval . . . . . . . . . . . . . . . . . . . 448

7.1.5 Feature tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452

7.1.6 Application: Performance-driven animation . . . . . . . . . . . . . . 454

7.2 Edges and contours . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455

7.2.1 Edge detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456

7.2.2 Contour detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461

7.2.3 Application: Edge editing and enhancement . . . . . . . . . . . . . . 465

7.3 Contour tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466

7.3.1 Snakes and scissors . . . . . . . . . . . . . . . . . . . . . . . . . . . 467

7.3.2 Level Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474

7.3.3 Application: Contour tracking and rotoscoping . . . . . . . . . . . . 476

7.4 Lines and vanishing points . . . . . . . . . . . . . . . . . . . . . . . . . . . 477

7.4.1 Successive approximation . . . . . . . . . . . . . . . . . . . . . . . 477

7.4.2 Hough transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477

7.4.3 Vanishing points . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481

7.5 Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483

7.5.1 Graph-based segmentation . . . . . . . . . . . . . . . . . . . . . . . 486

7.5.2 Mean shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487

7.5.3 Normalized cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489

7.6 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491

7.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495

8 Image alignment and stitching

503

8.1 Pairwise alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505

8.1.1 2D alignment using least squares . . . . . . . . . . . . . . . . . . . . 506

8.1.2 Application: Panography . . . . . . . . . . . . . . . . . . . . . . . . 508

8.1.3 Iterative algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 509

8.1.4 Robust least squares and RANSAC . . . . . . . . . . . . . . . . . . 512

8.1.5 3D alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515

8.2 Image stitching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516

8.2.1 Parametric motion models . . . . . . . . . . . . . . . . . . . . . . . 518

xx

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

8.2.2 Application: Whiteboard and document scanning . . . . . . . . . . . 519 8.2.3 Rotational panoramas . . . . . . . . . . . . . . . . . . . . . . . . . . 521 8.2.4 Gap closing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522 8.2.5 Application: Video summarization and compression . . . . . . . . . 524 8.2.6 Cylindrical and spherical coordinates . . . . . . . . . . . . . . . . . 525 8.3 Global alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528 8.3.1 Bundle adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . 529 8.3.2 Parallax removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533 8.3.3 Recognizing panoramas . . . . . . . . . . . . . . . . . . . . . . . . 535 8.4 Compositing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538 8.4.1 Choosing a compositing surface . . . . . . . . . . . . . . . . . . . . 538 8.4.2 Pixel selection and weighting (deghosting) . . . . . . . . . . . . . . 540 8.4.3 Application: Photomontage . . . . . . . . . . . . . . . . . . . . . . 546 8.4.4 Blending . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546 8.5 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550 8.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551

9 Motion estimation

557

9.1 Translational alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560

9.1.1 Hierarchical motion estimation . . . . . . . . . . . . . . . . . . . . . 564

9.1.2 Fourier-based alignment . . . . . . . . . . . . . . . . . . . . . . . . 565

9.1.3 Incremental reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . 568

9.2 Parametric motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572

9.2.1 Application: Video stabilization . . . . . . . . . . . . . . . . . . . . 575

9.2.2 Spline-based motion . . . . . . . . . . . . . . . . . . . . . . . . . . 577

9.2.3 Application: Medical image registration . . . . . . . . . . . . . . . . 579

9.3 Optical ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581

9.3.1 Deep learning approaches . . . . . . . . . . . . . . . . . . . . . . . 586

9.3.2 Application: Rolling shutter wobble removal . . . . . . . . . . . . . 589

9.3.3 Multi-frame motion estimation . . . . . . . . . . . . . . . . . . . . . 589

9.3.4 Application: Video denoising . . . . . . . . . . . . . . . . . . . . . 591

9.4 Layered motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591

9.4.1 Application: Frame interpolation . . . . . . . . . . . . . . . . . . . . 595

9.4.2 Transparent layers and reﬂections . . . . . . . . . . . . . . . . . . . 596

9.4.3 Video object segmentation . . . . . . . . . . . . . . . . . . . . . . . 599

9.4.4 Video object tracking . . . . . . . . . . . . . . . . . . . . . . . . . . 600

Contents

xxi

9.5 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 602 9.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604

10 Computational photography

609

10.1 Photometric calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612

10.1.1 Radiometric response function . . . . . . . . . . . . . . . . . . . . . 613

10.1.2 Noise level estimation . . . . . . . . . . . . . . . . . . . . . . . . . 616

10.1.3 Vignetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617

10.1.4 Optical blur (spatial response) estimation . . . . . . . . . . . . . . . 618

10.2 High dynamic range imaging . . . . . . . . . . . . . . . . . . . . . . . . . . 622

10.2.1 Tone mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629

10.2.2 Application: Flash photography . . . . . . . . . . . . . . . . . . . . 637

10.3 Super-resolution, denoising, and blur removal . . . . . . . . . . . . . . . . . 639

10.3.1 Color image demosaicing . . . . . . . . . . . . . . . . . . . . . . . 649

10.3.2 Lens blur (bokeh) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 651

10.4 Image matting and compositing . . . . . . . . . . . . . . . . . . . . . . . . . 653

10.4.1 Blue screen matting . . . . . . . . . . . . . . . . . . . . . . . . . . . 655

10.4.2 Natural image matting . . . . . . . . . . . . . . . . . . . . . . . . . 657

10.4.3 Optimization-based matting . . . . . . . . . . . . . . . . . . . . . . 660

10.4.4 Smoke, shadow, and ﬂash matting . . . . . . . . . . . . . . . . . . . 664

10.4.5 Video matting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666

10.5 Texture analysis and synthesis . . . . . . . . . . . . . . . . . . . . . . . . . 667

10.5.1 Application: Hole ﬁlling and inpainting . . . . . . . . . . . . . . . . 669

10.5.2 Application: Non-photorealistic rendering . . . . . . . . . . . . . . . 671

10.5.3 Neural style transfer and semantic image synthesis . . . . . . . . . . 673

10.6 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675

10.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678

11 Structure from motion and SLAM

685

11.1 Geometric intrinsic calibration . . . . . . . . . . . . . . . . . . . . . . . . . 688

11.1.1 Vanishing points . . . . . . . . . . . . . . . . . . . . . . . . . . . . 690

11.1.2 Application: Single view metrology . . . . . . . . . . . . . . . . . . 691

11.1.3 Rotational motion . . . . . . . . . . . . . . . . . . . . . . . . . . . 692

11.1.4 Radial distortion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694

11.2 Pose estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 696

11.2.1 Linear algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 696

11.2.2 Iterative algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 698

xxii

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

11.2.3 Application: Location recognition . . . . . . . . . . . . . . . . . . . 701 11.2.4 Triangulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 704 11.3 Two-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . . 706 11.3.1 Projective (uncalibrated) reconstruction . . . . . . . . . . . . . . . . 712 11.3.2 Self-calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714 11.3.3 Application: View morphing . . . . . . . . . . . . . . . . . . . . . . 716 11.4 Multi-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . 717 11.4.1 Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 717 11.4.2 Bundle adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . 719 11.4.3 Exploiting sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . 721 11.4.4 Application: Match move . . . . . . . . . . . . . . . . . . . . . . . 725 11.4.5 Uncertainty and ambiguities . . . . . . . . . . . . . . . . . . . . . . 725 11.4.6 Application: Reconstruction from internet photos . . . . . . . . . . . 726 11.4.7 Global structure from motion . . . . . . . . . . . . . . . . . . . . . . 730 11.4.8 Constrained structure and motion . . . . . . . . . . . . . . . . . . . 732 11.5 Simultaneous localization and mapping (SLAM) . . . . . . . . . . . . . . . 735 11.5.1 Application: Autonomous navigation . . . . . . . . . . . . . . . . . 739 11.5.2 Application: Smartphone augmented reality . . . . . . . . . . . . . . 740 11.6 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742 11.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 745

12 Depth estimation

753

12.1 Epipolar geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 757

12.1.1 Rectiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 759

12.1.2 Plane sweep . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 761

12.2 Sparse correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 764

12.2.1 3D curves and proﬁles . . . . . . . . . . . . . . . . . . . . . . . . . 764

12.3 Dense correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 766

12.3.1 Similarity measures . . . . . . . . . . . . . . . . . . . . . . . . . . . 768

12.4 Local methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770

12.4.1 Sub-pixel estimation and uncertainty . . . . . . . . . . . . . . . . . . 772

12.4.2 Application: Stereo-based head tracking . . . . . . . . . . . . . . . . 773

12.5 Global optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 775

12.5.1 Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . 778

12.5.2 Segmentation-based techniques . . . . . . . . . . . . . . . . . . . . 780

12.5.3 Application: Z-keying and background replacement . . . . . . . . . . 782

Contents

xxiii

12.6 Deep neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 783 12.7 Multi-view stereo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785
12.7.1 Scene ﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 790 12.7.2 Volumetric and 3D surface reconstruction . . . . . . . . . . . . . . . 791 12.7.3 Shape from silhouettes . . . . . . . . . . . . . . . . . . . . . . . . . 798 12.8 Monocular depth estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 800 12.9 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 803 12.10Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 804

13 3D reconstruction

809

13.1 Shape from X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 813

13.1.1 Shape from shading and photometric stereo . . . . . . . . . . . . . . 813

13.1.2 Shape from texture . . . . . . . . . . . . . . . . . . . . . . . . . . . 818

13.1.3 Shape from focus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 818

13.2 3D scanning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 820

13.2.1 Range data merging . . . . . . . . . . . . . . . . . . . . . . . . . . 824

13.2.2 Application: Digital heritage . . . . . . . . . . . . . . . . . . . . . . 828

13.3 Surface representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 829

13.3.1 Surface interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . 830

13.3.2 Surface simpliﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . 831

13.3.3 Geometry images . . . . . . . . . . . . . . . . . . . . . . . . . . . . 832

13.4 Point-based representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 833

13.5 Volumetric representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 834

13.5.1 Implicit surfaces and level sets . . . . . . . . . . . . . . . . . . . . . 835

13.6 Model-based reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . 837

13.6.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837

13.6.2 Facial modeling and tracking . . . . . . . . . . . . . . . . . . . . . . 841

13.6.3 Application: Facial animation . . . . . . . . . . . . . . . . . . . . . 844

13.6.4 Human body modeling and tracking . . . . . . . . . . . . . . . . . . 846

13.7 Recovering texture maps and albedos . . . . . . . . . . . . . . . . . . . . . 854

13.7.1 Estimating BRDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . 856

13.7.2 Application: 3D model capture . . . . . . . . . . . . . . . . . . . . . 858

13.8 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 859

13.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 861

xxiv

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

14 Image-based rendering

865

14.1 View interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 867

14.1.1 View-dependent texture maps . . . . . . . . . . . . . . . . . . . . . 869

14.1.2 Application: Photo Tourism . . . . . . . . . . . . . . . . . . . . . . 871

14.2 Layered depth images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 872

14.2.1 Impostors, sprites, and layers . . . . . . . . . . . . . . . . . . . . . . 873

14.2.2 Application: 3D photography . . . . . . . . . . . . . . . . . . . . . 877

14.3 Light ﬁelds and Lumigraphs . . . . . . . . . . . . . . . . . . . . . . . . . . 880

14.3.1 Unstructured Lumigraph . . . . . . . . . . . . . . . . . . . . . . . . 884

14.3.2 Surface light ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . 884

14.3.3 Application: Concentric mosaics . . . . . . . . . . . . . . . . . . . . 886

14.3.4 Application: Synthetic re-focusing . . . . . . . . . . . . . . . . . . . 887

14.4 Environment mattes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 887

14.4.1 Higher-dimensional light ﬁelds . . . . . . . . . . . . . . . . . . . . . 889

14.4.2 The modeling to rendering continuum . . . . . . . . . . . . . . . . . 890

14.5 Video-based rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 891

14.5.1 Video-based animation . . . . . . . . . . . . . . . . . . . . . . . . . 892

14.5.2 Video textures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 893

14.5.3 Application: Animating pictures . . . . . . . . . . . . . . . . . . . . 896

14.5.4 3D and free-viewpoint Video . . . . . . . . . . . . . . . . . . . . . . 897

14.5.5 Application: Video-based walkthroughs . . . . . . . . . . . . . . . . 900

14.6 Neural rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 904

14.7 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912

14.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 914

15 Conclusion

919

A Linear algebra and numerical techniques

923

A.1 Matrix decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 924

A.1.1 Singular value decomposition . . . . . . . . . . . . . . . . . . . . . 925

A.1.2 Eigenvalue decomposition . . . . . . . . . . . . . . . . . . . . . . . 926

A.1.3 QR factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 929

A.1.4 Cholesky factorization . . . . . . . . . . . . . . . . . . . . . . . . . 930

A.2 Linear least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 931

A.2.1 Total least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . 933

A.3 Non-linear least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 935

A.4 Direct sparse matrix techniques . . . . . . . . . . . . . . . . . . . . . . . . . 936

Preface

xxv

A.4.1 Variable reordering . . . . . . . . . . . . . . . . . . . . . . . . . . . 937 A.5 Iterative techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 938
A.5.1 Conjugate gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . 939 A.5.2 Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 941 A.5.3 Multigrid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 942

B Bayesian modeling and inference

945

B.1 Estimation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 947

B.1.1 Likelihood for multivariate Gaussian noise . . . . . . . . . . . . . . 948

B.2 Maximum likelihood estimation and least squares . . . . . . . . . . . . . . . 949

B.3 Robust statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 951

B.4 Prior models and Bayesian inference . . . . . . . . . . . . . . . . . . . . . . 954

B.5 Markov random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 955

B.6 Uncertainty estimation (error analysis) . . . . . . . . . . . . . . . . . . . . . 958

C Supplementary material

959

C.1 Datasets and benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 960

C.2 Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 967

C.3 Slides and lectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 976

References

977

Index

1177

xxvi

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

Chapter 1
Introduction
1.1 What is computer vision? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 A brief history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.3 Book overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 1.4 Sample syllabus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 1.5 A note on notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.6 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
Figure 1.1 The human visual system has no problem interpreting the subtle variations in translucency and shading in this photograph and correctly segmenting the object from its background.

2

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a)

(b)

(c)

(d)

Figure 1.2 Some examples of computer vision algorithms and applications. (a) Face detection algorithms, coupled with color-based clothing and hair detection algorithms, can locate and recognize the individuals in this image (Sivic, Zitnick, and Szeliski 2006) © 2006 Springer. (b) Object instance segmentation can delineate each person and object in a complex scene (He, Gkioxari et al. 2017) © 2017 IEEE. (c) Structure from motion algorithms can reconstruct a sparse 3D point model of a large complex scene from hundreds of partially overlapping photographs (Snavely, Seitz, and Szeliski 2006) © 2006 ACM. (d) Stereo matching algorithms can build a detailed 3D model of a building fac¸ade from hundreds of differently exposed photographs taken from the internet (Goesele, Snavely et al. 2007) © 2007 IEEE.

1.1 What is computer vision?

3

1.1 What is computer vision?

As humans, we perceive the three-dimensional structure of the world around us with apparent ease. Think of how vivid the three-dimensional percept is when you look at a vase of ﬂowers sitting on the table next to you. You can tell the shape and translucency of each petal through the subtle patterns of light and shading that play across its surface and effortlessly segment each ﬂower from the background of the scene (Figure 1.1). Looking at a framed group portrait, you can easily count and name all of the people in the picture and even guess at their emotions from their facial expressions (Figure 1.2a). Perceptual psychologists have spent decades trying to understand how the visual system works and, even though they can devise optical illusions1 to tease apart some of its principles (Figure 1.3), a complete solution to this puzzle remains elusive (Marr 1982; Wandell 1995; Palmer 1999; Livingstone 2008; Frisby and Stone 2010).
Researchers in computer vision have been developing, in parallel, mathematical techniques for recovering the three-dimensional shape and appearance of objects in imagery. Here, the progress in the last two decades has been rapid. We now have reliable techniques for accurately computing a 3D model of an environment from thousands of partially overlapping photographs (Figure 1.2c). Given a large enough set of views of a particular object or fac¸ade, we can create accurate dense 3D surface models using stereo matching (Figure 1.2d). We can even, with moderate success, delineate most of the people and objects in a photograph (Figure 1.2a). However, despite all of these advances, the dream of having a computer explain an image at the same level of detail and causality as a two-year old remains elusive.
Why is vision so difﬁcult? In part, it is because it is an inverse problem, in which we seek to recover some unknowns given insufﬁcient information to fully specify the solution. We must therefore resort to physics-based and probabilistic models, or machine learning from large sets of examples, to disambiguate between potential solutions. However, modeling the visual world in all of its rich complexity is far more difﬁcult than, say, modeling the vocal tract that produces spoken sounds.
The forward models that we use in computer vision are usually developed in physics (radiometry, optics, and sensor design) and in computer graphics. Both of these ﬁelds model how objects move and animate, how light reﬂects off their surfaces, is scattered by the atmosphere, refracted through camera lenses (or human eyes), and ﬁnally projected onto a ﬂat (or curved) image plane. While computer graphics are not yet perfect, in many domains, such as rendering a still scene composed of everyday objects or animating extinct creatures such
1Some fun pages with striking illusions include https://michaelbach.de/ot, https://www.illusionsindex.org, and http://www.ritsumei.ac.jp/∼akitaoka/index-e.html.

4

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a)

(b)

XXXXXXX XXXXXXX XXXXXXX XXXXXXX XXXXXXX XXXXXXX XXXXXXX XXXXXXX XXXXXXX XXXXXXX

OXOXOXX XOXXXOX OXXOXXO X XOXOOX OXXOXXX XOXXXOX OXXOXXO XOXXXOX X X XOOX X XOXXXOX

(c)

(d)

Figure 1.3 Some common optical illusions and what they might tell us about the visual system: (a) The classic Mu¨ller-Lyer illusion, where the lengths of the two horizontal lines appear different, probably due to the imagined perspective effects. (b) The “white” square B in the shadow and the “black” square A in the light actually have the same absolute intensity value. The percept is due to brightness constancy, the visual system’s attempt to discount illumination when interpreting colors. Image courtesy of Ted Adelson, http:// persci.mit.edu/ gallery/ checkershadow. (c) A variation of the Hermann grid illusion, courtesy of Hany Farid. As you move your eyes over the ﬁgure, gray spots appear at the intersections. (d) Count the red Xs in the left half of the ﬁgure. Now count them in the right half. Is it signiﬁcantly harder? The explanation has to do with a pop-out effect (Treisman 1985), which tells us about the operations of parallel perception and integration pathways in the brain.

1.1 What is computer vision?

5

as dinosaurs, the illusion of reality is essentially there. In computer vision, we are trying to do the inverse, i.e., to describe the world that we
see in one or more images and to reconstruct its properties, such as shape, illumination, and color distributions. It is amazing that humans and animals do this so effortlessly, while computer vision algorithms are so error prone. People who have not worked in the ﬁeld often underestimate the difﬁculty of the problem. This misperception that vision should be easy dates back to the early days of artiﬁcial intelligence (see Section 1.2), when it was initially believed that the cognitive (logic proving and planning) parts of intelligence were intrinsically more difﬁcult than the perceptual components (Boden 2006).
The good news is that computer vision is being used today in a wide variety of real-world applications, which include:
• Optical character recognition (OCR): reading handwritten postal codes on letters (Figure 1.4a) and automatic number plate recognition (ANPR);
• Machine inspection: rapid parts inspection for quality assurance using stereo vision with specialized illumination to measure tolerances on aircraft wings or auto body parts (Figure 1.4b) or looking for defects in steel castings using X-ray vision;
• Retail: object recognition for automated checkout lanes and fully automated stores (Wingﬁeld 2019);
• Warehouse logistics: autonomous package delivery and pallet-carrying “drives” (Guizzo 2008; O’Brian 2019) and parts picking by robotic manipulators (Figure 1.4c; Ackerman 2020);
• Medical imaging: registering pre-operative and intra-operative imagery (Figure 1.4d) or performing long-term studies of people’s brain morphology as they age;
• Self-driving vehicles: capable of driving point-to-point between cities (Figure 1.4e; Montemerlo, Becker et al. 2008; Urmson, Anhalt et al. 2008; Janai, Gu¨ney et al. 2020) as well as autonomous ﬂight (Kaufmann, Gehrig et al. 2019);
• 3D model building (photogrammetry): fully automated construction of 3D models from aerial and drone photographs (Figure 1.4f);
• Match move: merging computer-generated imagery (CGI) with live action footage by tracking feature points in the source video to estimate the 3D camera motion and shape of the environment. Such techniques are widely used in Hollywood, e.g., in movies such as Jurassic Park (Roble 1999; Roble and Zafar 2009); they also require the use of

6

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1.4 Some industrial applications of computer vision: (a) optical character recognition (OCR), http:// yann.lecun.com/ exdb/ lenet; (b) mechanical inspection, http:// www.cognitens.com; (c) warehouse picking, https:// covariant.ai; (d) medical imaging, http:// www.clarontech.com; (e) self-driving cars, (Montemerlo, Becker et al. 2008) © 2008 Wiley; (f) drone-based photogrammetry, https:// www.pix4d.com/ blog/ mapping-chillon-castle-with-drone.

1.1 What is computer vision?

7

precise matting to insert new elements between foreground and background elements (Chuang, Agarwala et al. 2002).
• Motion capture (mocap): using retro-reﬂective markers viewed from multiple cameras or other vision-based techniques to capture actors for computer animation;
• Surveillance: monitoring for intruders, analyzing highway trafﬁc and monitoring pools for drowning victims (e.g., https://swimeye.com);
• Fingerprint recognition and biometrics: for automatic access authentication as well as forensic applications.
David Lowe’s website of industrial vision applications (http://www.cs.ubc.ca/spider/lowe/ vision.html) lists many other interesting industrial applications of computer vision. While the above applications are all extremely important, they mostly pertain to fairly specialized kinds of imagery and narrow domains.
In addition to all of these industrial applications, there exist myriad consumer-level applications, such as things you can do with your own personal photographs and video. These include:
• Stitching: turning overlapping photos into a single seamlessly stitched panorama (Figure 1.5a), as described in Section 8.2;
• Exposure bracketing: merging multiple exposures taken under challenging lighting conditions (strong sunlight and shadows) into a single perfectly exposed image (Figure 1.5b), as described in Section 10.2;
• Morphing: turning a picture of one of your friends into another, using a seamless morph transition (Figure 1.5c);
• 3D modeling: converting one or more snapshots into a 3D model of the object or person you are photographing (Figure 1.5d), as described in Section 13.6;
• Video match move and stabilization: inserting 2D pictures or 3D models into your videos by automatically tracking nearby reference points (see Section 11.4.4)2 or using motion estimates to remove shake from your videos (see Section 9.2.1);
• Photo-based walkthroughs: navigating a large collection of photographs, such as the interior of your house, by ﬂying between different photos in 3D (see Sections 14.1.2 and 14.5.5);
2For a fun student project on this topic, see the “PhotoBook” project at http://www.cc.gatech.edu/dvfx/videos/ dvfx2005.html.

8

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

• Face detection: for improved camera focusing as well as more relevant image searching (see Section 6.3.1);
• Visual authentication: automatically logging family members onto your home computer as they sit down in front of the webcam (see Section 6.2.4).
The great thing about these applications is that they are already familiar to most students; they are, at least, technologies that students can immediately appreciate and use with their own personal media. Since computer vision is a challenging topic, given the wide range of mathematics being covered3 and the intrinsically difﬁcult nature of the problems being solved, having fun and relevant problems to work on can be highly motivating and inspiring.
The other major reason why this book has a strong focus on applications is that they can be used to formulate and constrain the potentially open-ended problems endemic in vision. Thus, it is better to think back from the problem at hand to suitable techniques, rather than to grab the ﬁrst technique that you may have heard of. This kind of working back from problems to solutions is typical of an engineering approach to the study of vision and reﬂects my own background in the ﬁeld.
First, I come up with a detailed problem deﬁnition and decide on the constraints and speciﬁcations for the problem. Then, I try to ﬁnd out which techniques are known to work, implement a few of these, evaluate their performance, and ﬁnally make a selection. In order for this process to work, it is important to have realistic test data, both synthetic, which can be used to verify correctness and analyze noise sensitivity, and real-world data typical of the way the system will ﬁnally be used. If machine learning is being used, it is even more important to have representative unbiased training data in sufﬁcient quantity to obtain good results on real-world inputs.
However, this book is not just an engineering text (a source of recipes). It also takes a scientiﬁc approach to basic vision problems. Here, I try to come up with the best possible models of the physics of the system at hand: how the scene is created, how light interacts with the scene and atmospheric effects, and how the sensors work, including sources of noise and uncertainty. The task is then to try to invert the acquisition process to come up with the best possible description of the scene.
The book often uses a statistical approach to formulating and solving computer vision problems. Where appropriate, probability distributions are used to model the scene and the noisy image acquisition process. The association of prior distributions with unknowns is often called Bayesian modeling (Appendix B). It is possible to associate a risk or loss function with
3These techniques include physics, Euclidean and projective geometry, statistics, and optimization. They make computer vision a fascinating ﬁeld to study and a great way to learn techniques widely applicable in other ﬁelds.

1.1 What is computer vision?

9

(a)
(b)
(c)
(d) Figure 1.5 Some consumer applications of computer vision: (a) image stitching: merging different views (Szeliski and Shum 1997) © 1997 ACM; (b) exposure bracketing: merging different exposures; (c) morphing: blending between two photographs (Gomes, Darsa et al. 1999) © 1999 Morgan Kaufmann; (d) smartphone augmented reality showing real-time depth occlusion effects (Valentin, Kowdle et al. 2018) © 2018 ACM.

10

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

misestimating the answer (Section B.2) and to set up your inference algorithm to minimize the expected risk. (Consider a robot trying to estimate the distance to an obstacle: it is usually safer to underestimate than to overestimate.) With statistical techniques, it often helps to gather lots of training data from which to learn probabilistic models. Finally, statistical approaches enable you to use proven inference techniques to estimate the best answer (or distribution of answers) and to quantify the uncertainty in the resulting estimates.
Because so much of computer vision involves the solution of inverse problems or the estimation of unknown quantities, my book also has a heavy emphasis on algorithms, especially those that are known to work well in practice. For many vision problems, it is all too easy to come up with a mathematical description of the problem that either does not match realistic real-world conditions or does not lend itself to the stable estimation of the unknowns. What we need are algorithms that are both robust to noise and deviation from our models and reasonably efﬁcient in terms of run-time resources and space. In this book, I go into these issues in detail, using Bayesian techniques, where applicable, to ensure robustness, and efﬁcient search, minimization, and linear system solving algorithms to ensure efﬁciency.4 Most of the algorithms described in this book are at a high level, being mostly a list of steps that have to be ﬁlled in by students or by reading more detailed descriptions elsewhere. In fact, many of the algorithms are sketched out in the exercises.
Now that I’ve described the goals of this book and the frameworks that I use, I devote the rest of this chapter to two additional topics. Section 1.2 is a brief synopsis of the history of computer vision. It can easily be skipped by those who want to get to “the meat” of the new material in this book and do not care as much about who invented what when.
The second is an overview of the book’s contents, Section 1.3, which is useful reading for everyone who intends to make a study of this topic (or to jump in partway, since it describes chapter interdependencies). This outline is also useful for instructors looking to structure one or more courses around this topic, as it provides sample curricula based on the book’s contents.

1.2 A brief history
In this section, I provide a brief personal synopsis of the main developments in computer vision over the last ﬁfty years (Figure 1.6); at least, those that I ﬁnd personally interesting and which appear to have stood the test of time. Readers not interested in the provenance of various ideas and the evolution of this ﬁeld should skip ahead to the book overview in
4In some cases, deep neural networks have also been shown to be an effective way to speed up algorithms that previously relied on iteration (Chen, Xu, and Koltun 2017).

1.2 A brief history

1970

1980

1990

2000

2010

11 2020

Digital image processing Blocks world, line labeling
Generalized cylinders Pattern recognition
Stereo correspondence Intrinsic images Optical flow
Structure from motion Image pyramids
Shape from shading, texture, and focus Physically-based modeling Regularization Markov random fields Kalman filters 3D range data processing Projective invariants Factorization Physics-based vision Graph cuts Particle filtering Energy-based segmentation
Face recognition and detection Image-based modeling and rendering
Texture synthesis and inpainting Computational photography Feature-based recognition Category recognition Machine learning
Modeling and tracking humans Semantic segmentation SLAM and VIO Deep learning Vision and language

Figure 1.6 A rough timeline of some of the most active topics of research in computer vision.
Section 1.3.
1970s. When computer vision ﬁrst started out in the early 1970s, it was viewed as the visual perception component of an ambitious agenda to mimic human intelligence and to endow robots with intelligent behavior. At the time, it was believed by some of the early pioneers of artiﬁcial intelligence and robotics (at places such as MIT, Stanford, and CMU) that solving the “visual input” problem would be an easy step along the path to solving more difﬁcult problems such as higher-level reasoning and planning. According to one well-known story, in 1966, Marvin Minsky at MIT asked his undergraduate student Gerald Jay Sussman to “spend the summer linking a camera to a computer and getting the computer to describe what it saw” (Boden 2006, p. 781).5 We now know that the problem is slightly more difﬁcult than that.6
What distinguished computer vision from the already existing ﬁeld of digital image processing (Rosenfeld and Pfaltz 1966; Rosenfeld and Kak 1976) was a desire to recover the three-dimensional structure of the world from images and to use this as a stepping stone towards full scene understanding. Winston (1975) and Hanson and Riseman (1978) provide
5Boden (2006) cites (Crevier 1993) as the original source. The actual Vision Memo was authored by Seymour Papert (1966) and involved a whole cohort of students.
6To see how far robotic vision has come in the last six decades, have a look at some of the videos on the Boston Dynamics https://www.bostondynamics.com, Skydio https://www.skydio.com, and Covariant https://covariant.ai websites.

12

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1.7 Some early (1970s) examples of computer vision algorithms: (a) line labeling (Nalwa 1993) © 1993 Addison-Wesley, (b) pictorial structures (Fischler and Elschlager 1973) © 1973 IEEE, (c) articulated body model (Marr 1982) © 1982 David Marr, (d) intrinsic images (Barrow and Tenenbaum 1981) © 1973 IEEE, (e) stereo correspondence (Marr 1982) © 1982 David Marr, (f) optical ﬂow (Nagel and Enkelmann 1986) © 1986 IEEE.

two nice collections of classic papers from this early period. Early attempts at scene understanding involved extracting edges and then inferring the
3D structure of an object or a “blocks world” from the topological structure of the 2D lines (Roberts 1965). Several line labeling algorithms (Figure 1.7a) were developed at that time (Huffman 1971; Clowes 1971; Waltz 1975; Rosenfeld, Hummel, and Zucker 1976; Kanade 1980). Nalwa (1993) gives a nice review of this area. The topic of edge detection was also an active area of research; a nice survey of contemporaneous work can be found in (Davis 1975).
Three-dimensional modeling of non-polyhedral objects was also being studied (Baumgart 1974; Baker 1977). One popular approach used generalized cylinders, i.e., solids of revolution and swept closed curves (Agin and Binford 1976; Nevatia and Binford 1977), often arranged into parts relationships7 (Hinton 1977; Marr 1982) (Figure 1.7c). Fischler and Elschlager (1973) called such elastic arrangements of parts pictorial structures (Figure 1.7b).
A qualitative approach to understanding intensities and shading variations and explaining them by the effects of image formation phenomena, such as surface orientation and shadows,
7In robotics and computer animation, these linked-part graphs are often called kinematic chains.

1.2 A brief history

13

was championed by Barrow and Tenenbaum (1981) in their paper on intrinsic images (Figure 1.7d), along with the related 2 1/2 -D sketch ideas of Marr (1982). This approach has seen periodic revivals, e.g., in the work of Tappen, Freeman, and Adelson (2005) and Barron and Malik (2012).
More quantitative approaches to computer vision were also developed at the time, including the ﬁrst of many feature-based stereo correspondence algorithms (Figure 1.7e) (Dev 1974; Marr and Poggio 1976, 1979; Barnard and Fischler 1982; Ohta and Kanade 1985; Grimson 1985; Pollard, Mayhew, and Frisby 1985) and intensity-based optical ﬂow algorithms (Figure 1.7f) (Horn and Schunck 1981; Huang 1981; Lucas and Kanade 1981; Nagel 1986). The early work in simultaneously recovering 3D structure and camera motion (see Chapter 11) also began around this time (Ullman 1979; Longuet-Higgins 1981).
A lot of the philosophy of how vision was believed to work at the time is summarized in David Marr’s (1982) book.8 In particular, Marr introduced his notion of the three levels of description of a (visual) information processing system. These three levels, very loosely paraphrased according to my own interpretation, are:
• Computational theory: What is the goal of the computation (task) and what are the constraints that are known or can be brought to bear on the problem?
• Representations and algorithms: How are the input, output, and intermediate information represented and which algorithms are used to calculate the desired result?
• Hardware implementation: How are the representations and algorithms mapped onto actual hardware, e.g., a biological vision system or a specialized piece of silicon? Conversely, how can hardware constraints be used to guide the choice of representation and algorithm? With the prevalent use of graphics chips (GPUs) and many-core architectures for computer vision, this question is again quite relevant.
As I mentioned earlier in this introduction, it is my conviction that a careful analysis of the problem speciﬁcation and known constraints from image formation and priors (the scientiﬁc and statistical approaches) must be married with efﬁcient and robust algorithms (the engineering approach) to design successful vision algorithms. Thus, it seems that Marr’s philosophy is as good a guide to framing and solving problems in our ﬁeld today as it was 25 years ago.

1980s. In the 1980s, a lot of attention was focused on more sophisticated mathematical techniques for performing quantitative image and scene analysis.
8More recent developments in visual perception theory are covered in (Wandell 1995; Palmer 1999; Livingstone 2008; Frisby and Stone 2010).

14

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1.8 Examples of computer vision algorithms from the 1980s: (a) pyramid blending (Burt and Adelson 1983b) © 1983 ACM, (b) shape from shading (Freeman and Adelson 1991) © 1991 IEEE, (c) edge detection (Freeman and Adelson 1991) © 1991 IEEE, (d) physically based models (Terzopoulos and Witkin 1988) © 1988 IEEE, (e) regularization-based surface reconstruction (Terzopoulos 1988) © 1988 IEEE, (f) range data acquisition and merging (Banno, Masuda et al. 2008) © 2008 Springer.

Image pyramids (see Section 3.5) started being widely used to perform tasks such as image blending (Figure 1.8a) and coarse-to-ﬁne correspondence search (Rosenfeld 1980; Burt and Adelson 1983b; Rosenfeld 1984; Quam 1984; Anandan 1989). Continuous versions of pyramids using the concept of scale-space processing were also developed (Witkin 1983; Witkin, Terzopoulos, and Kass 1986; Lindeberg 1990). In the late 1980s, wavelets (see Section 3.5.4) started displacing or augmenting regular image pyramids in some applications (Mallat 1989; Simoncelli and Adelson 1990a; Simoncelli, Freeman et al. 1992).
The use of stereo as a quantitative shape cue was extended by a wide variety of shapefrom-X techniques, including shape from shading (Figure 1.8b) (see Section 13.1.1 and Horn 1975; Pentland 1984; Blake, Zisserman, and Knowles 1985; Horn and Brooks 1986, 1989), photometric stereo (see Section 13.1.1 and Woodham 1981), shape from texture (see Section 13.1.2 and Witkin 1981; Pentland 1984; Malik and Rosenholtz 1997), and shape from focus (see Section 13.1.3 and Nayar, Watanabe, and Noguchi 1995). Horn (1986) has a nice discussion of most of these techniques.
Research into better edge and contour detection (Figure 1.8c) (see Section 7.2) was also

1.2 A brief history

15

active during this period (Canny 1986; Nalwa and Binford 1986), including the introduction of dynamically evolving contour trackers (Section 7.3.1) such as snakes (Kass, Witkin, and Terzopoulos 1988), as well as three-dimensional physically based models (Figure 1.8d) (Terzopoulos, Witkin, and Kass 1987; Kass, Witkin, and Terzopoulos 1988; Terzopoulos and Fleischer 1988).
Researchers noticed that a lot of the stereo, ﬂow, shape-from-X, and edge detection algorithms could be uniﬁed, or at least described, using the same mathematical framework if they were posed as variational optimization problems and made more robust (well-posed) using regularization (Figure 1.8e) (see Section 4.2 and Terzopoulos 1983; Poggio, Torre, and Koch 1985; Terzopoulos 1986b; Blake and Zisserman 1987; Bertero, Poggio, and Torre 1988; Terzopoulos 1988). Around the same time, Geman and Geman (1984) pointed out that such problems could equally well be formulated using discrete Markov random ﬁeld (MRF) models (see Section 4.3), which enabled the use of better (global) search and optimization algorithms, such as simulated annealing.
Online variants of MRF algorithms that modeled and updated uncertainties using the Kalman ﬁlter were introduced a little later (Dickmanns and Graefe 1988; Matthies, Kanade, and Szeliski 1989; Szeliski 1989). Attempts were also made to map both regularized and MRF algorithms onto parallel hardware (Poggio and Koch 1985; Poggio, Little et al. 1988; Fischler, Firschein et al. 1989). The book by Fischler and Firschein (1987) contains a nice collection of articles focusing on all of these topics (stereo, ﬂow, regularization, MRFs, and even higher-level vision).
Three-dimensional range data processing (acquisition, merging, modeling, and recognition; see Figure 1.8f) continued being actively explored during this decade (Agin and Binford 1976; Besl and Jain 1985; Faugeras and Hebert 1987; Curless and Levoy 1996). The compilation by Kanade (1987) contains a lot of the interesting papers in this area.

1990s. While a lot of the previously mentioned topics continued to be explored, a few of them became signiﬁcantly more active.
A burst of activity in using projective invariants for recognition (Mundy and Zisserman 1992) evolved into a concerted effort to solve the structure from motion problem (see Chapter 11). A lot of the initial activity was directed at projective reconstructions, which did not require knowledge of camera calibration (Faugeras 1992; Hartley, Gupta, and Chang 1992; Hartley 1994a; Faugeras and Luong 2001; Hartley and Zisserman 2004). Simultaneously, factorization techniques (Section 11.4.1) were developed to solve efﬁciently problems for which orthographic camera approximations were applicable (Figure 1.9a) (Tomasi and Kanade 1992; Poelman and Kanade 1997; Anandan and Irani 2002) and then later extended

16

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1.9 Examples of computer vision algorithms from the 1990s: (a) factorizationbased structure from motion (Tomasi and Kanade 1992) © 1992 Springer, (b) dense stereo matching (Boykov, Veksler, and Zabih 2001), (c) multi-view reconstruction (Seitz and Dyer 1999) © 1999 Springer, (d) face tracking (Matthews, Xiao, and Baker 2007), (e) image segmentation (Belongie, Fowlkes et al. 2002) © 2002 Springer, (f) face recognition (Turk and Pentland 1991).

to the perspective case (Christy and Horaud 1996; Triggs 1996). Eventually, the ﬁeld started using full global optimization (see Section 11.4.2 and Taylor, Kriegman, and Anandan 1991; Szeliski and Kang 1994; Azarbayejani and Pentland 1995), which was later recognized as being the same as the bundle adjustment techniques traditionally used in photogrammetry (Triggs, McLauchlan et al. 1999). Fully automated (sparse) 3D modeling systems were built using such techniques (Beardsley, Torr, and Zisserman 1996; Schaffalitzky and Zisserman 2002; Brown and Lowe 2003; Snavely, Seitz, and Szeliski 2006; Agarwal, Furukawa et al. 2011; Frahm, Fite-Georgel et al. 2010).
Work begun in the 1980s on using detailed measurements of color and intensity combined with accurate physical models of radiance transport and color image formation created its own subﬁeld known as physics-based vision. A good survey of the ﬁeld can be found in the threevolume collection on this topic (Wolff, Shafer, and Healey 1992a; Healey and Shafer 1992; Shafer, Healey, and Wolff 1992).

1.2 A brief history

17

Optical ﬂow methods (see Chapter 9) continued to be improved (Nagel and Enkelmann 1986; Bolles, Baker, and Marimont 1987; Horn and Weldon Jr. 1988; Anandan 1989; Bergen, Anandan et al. 1992; Black and Anandan 1996; Bruhn, Weickert, and Schno¨rr 2005; Papenberg, Bruhn et al. 2006), with (Nagel 1986; Barron, Fleet, and Beauchemin 1994; Baker, Scharstein et al. 2011) being good surveys. Similarly, a lot of progress was made on dense stereo correspondence algorithms (see Chapter 12, Okutomi and Kanade (1993, 1994); Boykov, Veksler, and Zabih (1998); Birchﬁeld and Tomasi (1999); Boykov, Veksler, and Zabih (2001), and the survey and comparison in Scharstein and Szeliski (2002)), with the biggest breakthrough being perhaps global optimization using graph cut techniques (Figure 1.9b) (Boykov, Veksler, and Zabih 2001).
Multi-view stereo algorithms (Figure 1.9c) that produce complete 3D surfaces (see Section 12.7) were also an active topic of research (Seitz and Dyer 1999; Kutulakos and Seitz 2000) that continues to be active today (Seitz, Curless et al. 2006; Scho¨ps, Scho¨nberger et al. 2017; Knapitsch, Park et al. 2017). Techniques for producing 3D volumetric descriptions from binary silhouettes (see Section 12.7.3) continued to be developed (Potmesil 1987; Srivasan, Liang, and Hackwood 1990; Szeliski 1993; Laurentini 1994), along with techniques based on tracking and reconstructing smooth occluding contours (see Section 12.2.1 and Cipolla and Blake 1992; Vaillant and Faugeras 1992; Zheng 1994; Boyer and Berger 1997; Szeliski and Weiss 1998; Cipolla and Giblin 2000).
Tracking algorithms also improved a lot, including contour tracking using active contours (see Section 7.3), such as snakes (Kass, Witkin, and Terzopoulos 1988), particle ﬁlters (Blake and Isard 1998), and level sets (Malladi, Sethian, and Vemuri 1995), as well as intensity-based (direct) techniques (Lucas and Kanade 1981; Shi and Tomasi 1994; Rehg and Kanade 1994), often applied to tracking faces (Figure 1.9d) (Lanitis, Taylor, and Cootes 1997; Matthews and Baker 2004; Matthews, Xiao, and Baker 2007) and whole bodies (Sidenbladh, Black, and Fleet 2000; Hilton, Fua, and Ronfard 2006; Moeslund, Hilton, and Kru¨ger 2006).
Image segmentation (see Section 7.5) (Figure 1.9e), a topic which has been active since the earliest days of computer vision (Brice and Fennema 1970; Horowitz and Pavlidis 1976; Riseman and Arbib 1977; Rosenfeld and Davis 1979; Haralick and Shapiro 1985; Pavlidis and Liow 1990), was also an active topic of research, producing techniques based on minimum energy (Mumford and Shah 1989) and minimum description length (Leclerc 1989), normalized cuts (Shi and Malik 2000), and mean shift (Comaniciu and Meer 2002).
Statistical learning techniques started appearing, ﬁrst in the application of principal component eigenface analysis to face recognition (Figure 1.9f) (see Section 5.2.3 and Turk and Pentland 1991) and linear dynamical systems for curve tracking (see Section 7.3.1 and Blake and Isard 1998).

18

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1.10 Examples of computer vision algorithms from the 2000s: (a) image-based rendering (Gortler, Grzeszczuk et al. 1996), (b) image-based modeling (Debevec, Taylor, and Malik 1996) © 1996 ACM, (c) interactive tone mapping (Lischinski, Farbman et al. 2006) (d) texture synthesis (Efros and Freeman 2001), (e) feature-based recognition (Fergus, Perona, and Zisserman 2007), (f) region-based recognition (Mori, Ren et al. 2004) © 2004 IEEE.

Perhaps the most notable development in computer vision during this decade was the increased interaction with computer graphics (Seitz and Szeliski 1999), especially in the cross-disciplinary area of image-based modeling and rendering (see Chapter 14). The idea of manipulating real-world imagery directly to create new animations ﬁrst came to prominence with image morphing techniques (Figure1.5c) (see Section 3.6.3 and Beier and Neely 1992) and was later applied to view interpolation (Chen and Williams 1993; Seitz and Dyer 1996), panoramic image stitching (Figure1.5a) (see Section 8.2 and Mann and Picard 1994; Chen 1995; Szeliski 1996; Szeliski and Shum 1997; Szeliski 2006a), and full light-ﬁeld rendering (Figure 1.10a) (see Section 14.3 and Gortler, Grzeszczuk et al. 1996; Levoy and Hanrahan 1996; Shade, Gortler et al. 1998). At the same time, image-based modeling techniques (Figure 1.10b) for automatically creating realistic 3D models from collections of images were also being introduced (Beardsley, Torr, and Zisserman 1996; Debevec, Taylor, and Malik 1996; Taylor, Debevec, and Malik 1996).

1.2 A brief history

19

2000s. This decade continued to deepen the interplay between the vision and graphics ﬁelds, but more importantly embraced data-driven and learning approaches as core components of vision. Many of the topics introduced under the rubric of image-based rendering, such as image stitching (see Section 8.2), light-ﬁeld capture and rendering (see Section 14.3), and high dynamic range (HDR) image capture through exposure bracketing (Figure1.5b) (see Section 10.2 and Mann and Picard 1995; Debevec and Malik 1997), were re-christened as computational photography (see Chapter 10) to acknowledge the increased use of such techniques in everyday digital photography. For example, the rapid adoption of exposure bracketing to create high dynamic range images necessitated the development of tone mapping algorithms (Figure 1.10c) (see Section 10.2.1) to convert such images back to displayable results (Fattal, Lischinski, and Werman 2002; Durand and Dorsey 2002; Reinhard, Stark et al. 2002; Lischinski, Farbman et al. 2006). In addition to merging multiple exposures, techniques were developed to merge ﬂash images with non-ﬂash counterparts (Eisemann and Durand 2004; Petschnigg, Agrawala et al. 2004) and to interactively or automatically select different regions from overlapping images (Agarwala, Dontcheva et al. 2004).
Texture synthesis (Figure 1.10d) (see Section 10.5), quilting (Efros and Leung 1999; Efros and Freeman 2001; Kwatra, Scho¨dl et al. 2003), and inpainting (Bertalmio, Sapiro et al. 2000; Bertalmio, Vese et al. 2003; Criminisi, Pe´rez, and Toyama 2004) are additional topics that can be classiﬁed as computational photography techniques, since they re-combine input image samples to produce new photographs.
A second notable trend during this decade was the emergence of feature-based techniques (combined with learning) for object recognition (see Section 6.1 and Ponce, Hebert et al. 2006). Some of the notable papers in this area include the constellation model of Fergus, Perona, and Zisserman (2007) (Figure 1.10e) and the pictorial structures of Felzenszwalb and Huttenlocher (2005). Feature-based techniques also dominate other recognition tasks, such as scene recognition (Zhang, Marszalek et al. 2007) and panorama and location recognition (Brown and Lowe 2007; Schindler, Brown, and Szeliski 2007). And while interest point (patch-based) features tend to dominate current research, some groups are pursuing recognition based on contours (Belongie, Malik, and Puzicha 2002) and region segmentation (Figure 1.10f) (Mori, Ren et al. 2004).
Another signiﬁcant trend from this decade was the development of more efﬁcient algorithms for complex global optimization problems (see Chapter 4 and Appendix B.5 and Szeliski, Zabih et al. 2008; Blake, Kohli, and Rother 2011). While this trend began with work on graph cuts (Boykov, Veksler, and Zabih 2001; Kohli and Torr 2007), a lot of progress has also been made in message passing algorithms, such as loopy belief propagation (LBP) (Yedidia, Freeman, and Weiss 2001; Kumar and Torr 2006).

20

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1.11 Examples of computer vision algorithms from the 2010s: (a) the SuperVision deep neural network © Krizhevsky, Sutskever, and Hinton (2012); (b) object instance segmentation (He, Gkioxari et al. 2017) © 2017 IEEE; (c) whole body, expression, and gesture ﬁtting from a single image (Pavlakos, Choutas et al. 2019) © 2019 IEEE; (d) fusing multiple color depth images using the KinectFusion real-time system (Newcombe, Izadi et al. 2011) © 2011 IEEE; (e) smartphone augmented reality with real-time depth occlusion effects (Valentin, Kowdle et al. 2018) © 2018 ACM; (f) 3D map computed in real-time on a fully autonomous Skydio R1 drone (Cross 2019).

The most notable trend from this decade, which has by now completely taken over visual recognition and most other aspects of computer vision, was the application of sophisticated machine learning techniques to computer vision problems (see Chapters 5 and 6). This trend coincided with the increased availability of immense quantities of partially labeled data on the internet, as well as signiﬁcant increases in computational power, which makes it more feasible to learn object categories without the use of careful human supervision.
2010s. The trend towards using large labeled (and also self-supervised) datasets to develop machine learning algorithms became a tidal wave that totally revolutionized the development of image recognition algorithms as well as other applications, such as denoising and optic ﬂow, which previously used Bayesian and global optimization techniques.
This trend was enabled by the development of high-quality large-scale annotated datasets

1.2 A brief history

21

such as ImageNet (Deng, Dong et al. 2009; Russakovsky, Deng et al. 2015), Microsoft COCO (Common Objects in Context) (Lin, Maire et al. 2014), and LVIS (Gupta, Dolla´r, and Girshick 2019). These datasets provided not only reliable metrics for tracking the progress of recognition and semantic segmentation algorithms, but more importantly, sufﬁcient labeled data to develop complete solutions based on machine learning.
Another major trend was the dramatic increase in computational power available from the development of general purpose (data-parallel) algorithms on graphical processing units (GPGPU). The breakthrough SuperVision (“AlexNet”) deep neural network (Figure 1.11a; Krizhevsky, Sutskever, and Hinton 2012), which was the ﬁrst neural network to win the yearly ImageNet large-scale visual recognition challenge, relied on GPU training, as well as a number of technical advances, for its dramatic performance. After the publication of this paper, progress in using deep convolutional architectures accelerated dramatically, to the point where they are now the only architecture considered for recognition and semantic segmentation tasks (Figure 1.11b), as well as the preferred architecture for many other vision tasks (Chapter 5; LeCun, Bengio, and Hinton 2015), including optic ﬂow (Sun, Yang et al. 2018)), denoising, and monocular depth inference (Li, Dekel et al. 2019).
Large datasets and GPU architectures, coupled with the rapid dissemination of ideas through timely publications on arXiv as well as the development of languages for deep learning and the open sourcing of neural network models, all contributed to an explosive growth in this area, both in rapid advances and capabilities, and also in the sheer number of publications and researchers now working on these topics. They also enabled the extension of image recognition approaches to video understanding tasks such as action recognition (Feichtenhofer, Fan et al. 2019), as well as structured regression tasks such as real-time multi-person body pose estimation (Cao, Simon et al. 2017).
Specialized sensors and hardware for computer vision tasks also continued to advance. The Microsoft Kinect depth camera, released in 2010, quickly became an essential component of many 3D modeling (Figure 1.11d) and person tracking (Shotton, Fitzgibbon et al. 2011) systems. Over the decade, 3D body shape modeling and tracking systems continued to evolve, to the point where it is now possible to infer a person’s 3D model with gestures and expression from a single image (Figure 1.11c).
And while depth sensors have not yet become ubiquitous (except for security applications on high-end phones), computational photography algorithms run on all of today’s smartphones. Innovations introduced in the computer vision community, such as panoramic image stitching and bracketed high dynamic range image merging, are now standard features, and multi-image low-light denoising algorithms are also becoming commonplace (Liba, Murthy et al. 2019). Lightﬁeld imaging algorithms, which allow the creation of soft depth-of-ﬁeld

22

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

effects, are now also becoming more available (Garg, Wadhwa et al. 2019). Finally, mobile augmented reality applications that perform real-time pose estimation and environment augmentation using combinations of feature tracking and inertial measurements are commonplace, and are currently being extended to include pixel-accurate depth occlusion effects (Figure 1.11e).
On higher-end platforms such as autonomous vehicles and drones, powerful real-time SLAM (simultaneous localization and mapping) and VIO (visual inertial odometry) algorithms (Engel, Scho¨ps, and Cremers 2014; Forster, Zhang et al. 2017; Engel, Koltun, and Cremers 2018) can build accurate 3D maps that enable, e.g., autonomous ﬂight through challenging scenes such as forests (Figure 1.11f).
In summary, this past decade has seen incredible advances in the performance and reliability of computer vision algorithms, brought in part by the shift to machine learning and training on very large sets of real-world data. It has also seen the application of vision algorithms in myriad commercial and consumer scenarios.

1.3 Book overview
In the ﬁnal part of this introduction, I give a brief tour of the material in this book, as well as a few notes on notation and some additional general references. Since computer vision is such a broad ﬁeld, it is possible to study certain aspects of it, e.g., geometric image formation and 3D structure recovery, without requiring other parts, e.g., the modeling of reﬂectance and shading. Some of the chapters in this book are only loosely coupled with others, and it is not strictly necessary to read all of the material in sequence.
Figure 1.12 shows a rough layout of the contents of this book. Since computer vision involves going from images to both a semantic understanding as well as a 3D structural description of the scene, I have positioned the chapters horizontally in terms of where in this spectrum they land, in addition to vertically according to their dependence.9
Interspersed throughout the book are sample applications, which relate the algorithms and mathematical material being presented in various chapters to useful, real-world applications. Many of these applications are also presented in the exercises sections, so that students can write their own.
At the end of each section, I provide a set of exercises that the students can use to implement, test, and reﬁne the algorithms and techniques presented in each section. Some of the
9For an interesting comparison with what is known about the human visual system, e.g., the largely parallel what and where pathways (Goodale and Milner 1992), see some textbooks on human perception (Palmer 1999; Livingstone 2008; Frisby and Stone 2010).

1.3 Book overview

23

2D (what?)

3D (where?)

2. Image formation

3. Image processing

4. Model fitting and optimization

5. Deep learning 6. Recognition

7. Feature detection and matching
8. Image alignment and stitching

10. Computational photography
14. Image-based rendering

9. Motion estimation
11. Structure from motion and SLAM
12. Depth estimation
13. 3D reconstruction

Figure 1.12 A taxonomy of the topics covered in this book, showing the (rough) dependencies between different chapters, which are roughly positioned along the left–right axis depending on whether they are more closely related to images (left) or 3D geometry (right) representations. The “what-where” along the top axis is a reference to separate visual pathways in the visual system (Goodale and Milner 1992), but should not be taken too seriously. Foundational techniques such as optimization and deep learning are widely used in subsequent chapters.

24

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

exercises are suitable as written homework assignments, others as shorter one-week projects, and still others as open-ended research problems that make for challenging ﬁnal projects. Motivated students who implement a reasonable subset of these exercises will, by the end of the book, have a computer vision software library that can be used for a variety of interesting tasks and projects.
If the students or curriculum do not have a strong preference for programming languages, Python, with the NumPy scientiﬁc and array arithmetic library plus the OpenCV vision library, are a good environment to develop algorithms and learn about vision. Not only will the students learn how to program using array/tensor notation and linear/matrix algebra (which is a good foundation for later use of PyTorch for deep learning), you can also prepare classroom assignments using Jupyter notebooks, giving you the option to combine descriptive tutorials, sample code, and code to be extended/modiﬁed in one convenient location.10
As this is a reference book, I try wherever possible to discuss which techniques and algorithms work well in practice, as well as provide up-to-date pointers to the latest research results in the areas that I cover. The exercises can be used to build up your own personal library of self-tested and validated vision algorithms, which is more worthwhile in the long term (assuming you have the time) than simply pulling algorithms out of a library whose performance you do not really understand.
The book begins in Chapter 2 with a review of the image formation processes that create the images that we see and capture. Understanding this process is fundamental if you want to take a scientiﬁc (model-based) approach to computer vision. Students who are eager to just start implementing algorithms (or courses that have limited time) can skip ahead to the next chapter and dip into this material later. In Chapter 2, we break down image formation into three major components. Geometric image formation (Section 2.1) deals with points, lines, and planes, and how these are mapped onto images using projective geometry and other models (including radial lens distortion). Photometric image formation (Section 2.2) covers radiometry, which describes how light interacts with surfaces in the world, and optics, which projects light onto the sensor plane. Finally, Section 2.3 covers how sensors work, including topics such as sampling and aliasing, color sensing, and in-camera compression.
Chapter 3 covers image processing, which is needed in almost all computer vision applications. This includes topics such as linear and non-linear ﬁltering (Section 3.3), the Fourier transform (Section 3.4), image pyramids and wavelets (Section 3.5), and geometric transformations such as image warping (Section 3.6). Chapter 3 also presents applications such as seamless image blending and image morphing.
10You may also be able to run your notebooks and train your models using the Google Colab service at https: //colab.research.google.com.

1.3 Book overview

25

n^

2. Image formation

3. Image processing

4. Optimization

5. Deep learning

6. Recognition

7–8. Features & alignment

9. Motion estimation

10. Computational Photography 11. Structure from motion

12. Depth estimation

13. 3D reconstruction

14. Image-based Rendering

Figure 1.13 A pictorial summary of the chapter contents. Sources: Burt and Adelson (1983b); Agarwala, Dontcheva et al. (2004); Glassner (2018); He, Gkioxari et al. (2017); Brown, Szeliski, and Winder (2005); Butler, Wulff et al. (2012); Debevec and Malik (1997); Snavely, Seitz, and Szeliski (2006); Scharstein, Hirschmu¨ller et al. (2014); Curless and Levoy (1996); Gortler, Grzeszczuk et al. (1996)—see the ﬁgures in the respective chapters for copyright information.

26

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

Chapter 4 begins with a new section on data ﬁtting and interpolation, which provides a conceptual framework for global optimization techniques such as regularization and Markov random ﬁelds (MRFs), as well as machine learning, which we cover in the next chapter. Section 4.2 covers classic regularization techniques, i.e., piecewise-continuous smoothing splines (aka variational techniques) implemented using fast iterated linear system solvers, which are still often the method of choice in time-critical applications such as mobile augmented reality. The next section (4.3) presents the related topic of MRFs, which also serve as an introduction to Bayesian inference techniques, covered at a more abstract level in Appendix B. The chapter also discusses applications to interactive colorization and segmentation.
Chapter 5 is a completely new chapter covering machine learning, deep learning, and deep neural networks. It begins in Section 5.1 with a review of classic supervised machine learning approaches, which are designed to classify images (or regress values) based on intermediate-level features. Section 5.2 looks at unsupervised learning, which is useful for both understanding unlabeled training data and providing models of real-world distributions. Section 5.3 presents the basic elements of feedforward neural networks, including weights, layers, and activation functions, as well as methods for network training. Section 5.4 goes into more detail on convolutional networks and their applications to both recognition and image processing. The last section in the chapter discusses more complex networks, including 3D, spatio-temporal, recurrent, and generative networks.
Chapter 6 covers the topic of recognition. In the ﬁrst edition of this book this chapter came last, since it built upon earlier methods such as segmentation and feature matching. With the advent of deep networks, many of these intermediate representations are no longer necessary, since the network can learn them as part of the training process. As so much of computer vision research is now devoted to various recognition topics, I decided to move this chapter up so that students can learn about it earlier in the course.
The chapter begins with the classic problem of instance recognition, i.e., ﬁnding instances of known 3D objects in cluttered scenes. Section 6.2 covers both traditional and deep network approaches to whole image classiﬁcation, i.e., what used to be called category recognition. It also discusses the special case of facial recognition. Section 6.3 presents algorithms for object detection (drawing bounding boxes around recognized objects), with a brief review of older approaches to face and pedestrian detection. Section 6.4 covers various ﬂavors of semantic segmentation (generating per-pixel labels), including instance segmentation (delineating separate objects), pose estimation (labeling pixels with body parts), and panoptic segmentation (labeling both things and stuff). In Section 6.5, we brieﬂy look at some recent papers in video understanding and action recognition, while in Section 6.6 we mention some recent work in image captioning and visual question answering (vision and language).

1.3 Book overview

27

In Chapter 7, we cover feature detection and matching. A lot of current 3D reconstruction and recognition techniques are built on extracting and matching feature points (Section 7.1), so this is a fundamental technique required by many subsequent chapters (Chapters 8 and 11) and even in instance recognition (Section 6.1). We also cover edge and straight line detection in Sections 7.2 and 7.4, contour tracking in Section 7.3, and low-level segmentation techniques in Section 7.5.
Feature detection and matching are used in Chapter 8 to perform image alignment (or registration) and image stitching. We introduce the basic techniques of feature-based alignment and show how this problem can be solved using either linear or non-linear least squares, depending on the motion involved. We also introduce additional concepts, such as uncertainty weighting and robust regression, which are essential to making real-world systems work. Feature-based alignment is then used as a building block for both 2D applications such as image stitching (Section 8.2) and computational photography (Chapter 10), as well as 3D geometric alignment tasks such as pose estimation and structure from motion (Chapter 11).
The second part of Chapter 8 is devoted to image stitching, i.e., the construction of large panoramas and composites. While stitching is just one example of computation photography (see Chapter 10), there is enough depth here to warrant a separate section. We start by discussing various possible motion models (Section 8.2.1), including planar motion and pure camera rotation. We then discuss global alignment (Section 8.3), which is a special (simpliﬁed) case of general bundle adjustment, and then present panorama recognition, i.e., techniques for automatically discovering which images actually form overlapping panoramas. Finally, we cover the topics of image compositing and blending (Section 8.4), which involve both selecting which pixels from which images to use and blending them together so as to disguise exposure differences.
Image stitching is a wonderful application that ties together most of the material covered in earlier parts of this book. It also makes for a good mid-term course project that can build on previously developed techniques such as image warping and feature detection and matching. Sections 8.2–8.4 also present more specialized variants of stitching such as whiteboard and document scanning, video summarization, panography, full 360° spherical panoramas, and interactive photomontage for blending repeated action shots together.
In Chapter 9, we generalize the concept of feature-based image alignment to cover dense intensity-based motion estimation, i.e., optical ﬂow. We start with the simplest possible motion models, translational motion (Section 9.1), and cover topics such as hierarchical (coarse-to-ﬁne) motion estimation, Fourier-based techniques, and iterative reﬁnement. We then present parametric motion models, which can be used to compensate for camera rotation and zooming, as well as afﬁne or planar perspective motion (Section 9.2). This is then

28

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

generalized to spline-based motion models (Section 9.2.2) and ﬁnally to general per-pixel optical ﬂow (Section 9.3). We close the chapter in Section 9.4 with a discussion of layered and learned motion models as well as video object segmentation and tracking. Applications of motion estimation techniques include automated morphing, video denoising, and frame interpolation (slow motion).
Chapter 10 presents additional examples of computational photography, which is the process of creating new images from one or more input photographs, often based on the careful modeling and calibration of the image formation process (Section 10.1). Computational photography techniques include merging multiple exposures to create high dynamic range images (Section 10.2), increasing image resolution through blur removal and super-resolution (Section 10.3), and image editing and compositing operations (Section 10.4). We also cover the topics of texture analysis, synthesis, and inpainting (hole ﬁlling) in Section 10.5, as well as non-photorealistic rendering and style transfer.
Starting in Chapter 11, we delve more deeply into techniques for reconstructing 3D models from images. We begin by introducing methods for intrinsic camera calibration in Section 11.1 and 3D pose estimation, i.e., extrinsic calibration, in Section 11.2. These sections also describe the applications of single-view reconstruction of building models and 3D location recognition. We then cover the topic of triangulation (Section 11.2.4), which is the 3D reconstruction of points from matched features when the camera positions are known.
Chapter 11 then moves on to the topic of structure from motion, which involves the simultaneous recovery of 3D camera motion and 3D scene structure from a collection of tracked 2D features. We begin with two-frame structure from motion (Section 11.3), for which algebraic techniques exist, as well as robust sampling techniques such as RANSAC that can discount erroneous feature matches. We then cover techniques for multi-frame structure from motion, including factorization (Section 11.4.1), bundle adjustment (Section 11.4.2), and constrained motion and structure models (Section 11.4.8). We present applications in visual effects (match move) and sparse 3D model construction for large (e.g., internet) photo collections. The ﬁnal part of this chapter (Section 11.5) has a new section on simultaneous localization and mapping (SLAM) as well as its applications to autonomous navigation and mobile augmented reality (AR).
In Chapter 12, we turn to the topic of stereo correspondence, which can be thought of as a special case of motion estimation where the camera positions are already known (Section 12.1). This additional knowledge enables stereo algorithms to search over a much smaller space of correspondences to produce dense depth estimates using various combinations of matching criteria, optimization algorithm, and/or deep networks (Sections 12.3–12.6). We also cover multi-view stereo algorithms that build a true 3D surface representation instead

1.3 Book overview

29

of just a single depth map (Section 12.7), as well as monocular depth inference algorithms that hallucinate depth maps from just a single image (Section 12.8). Applications of stereo matching include head and gaze tracking, as well as depth-based background replacement (Z-keying).
Chapter 13 covers additional 3D shape and appearance modeling techniques. These include classic shape-from-X techniques such as shape from shading, shape from texture, and shape from focus (Section 13.1). An alternative to all of these passive computer vision techniques is to use active rangeﬁnding (Section 13.2), i.e., to project patterned light onto scenes and recover the 3D geometry through triangulation. Processing all of these 3D representations often involves interpolating or simplifying the geometry (Section 13.3), or using alternative representations such as surface point sets (Section 13.4) or implicit functions (Section 13.5).
The collection of techniques for going from one or more images to partial or full 3D models is often called image-based modeling or 3D photography. Section 13.6 examines three more specialized application areas (architecture, faces, and human bodies), which can use model-based reconstruction to ﬁt parameterized models to the sensed data. Section 13.7 examines the topic of appearance modeling, i.e., techniques for estimating the texture maps, albedos, or even sometimes complete bi-directional reﬂectance distribution functions (BRDFs) that describe the appearance of 3D surfaces.
In Chapter 14, we discuss the large number of image-based rendering techniques that have been developed in the last three decades, including simpler techniques such as view interpolation (Section 14.1), layered depth images (Section 14.2), and sprites and layers (Section 14.2.1), as well as the more general framework of light ﬁelds and Lumigraphs (Section 14.3) and higher-order ﬁelds such as environment mattes (Section 14.4). Applications of these techniques include navigating 3D collections of photographs using photo tourism.
Next, we discuss video-based rendering, which is the temporal extension of image-based rendering. The topics we cover include video-based animation (Section 14.5.1), periodic video turned into video textures (Section 14.5.2), and 3D video constructed from multiple video streams (Section 14.5.4). Applications of these techniques include animating still images and creating home tours based on 360° video. We ﬁnish the chapter with an overview of the new emerging ﬁeld of neural rendering.
To support the book’s use as a textbook, the appendices and associated website contain more detailed mathematical topics and additional material. Appendix A covers linear algebra and numerical techniques, including matrix algebra, least squares, and iterative techniques. Appendix B covers Bayesian estimation theory, including maximum likelihood estimation, robust statistics, Markov random ﬁelds, and uncertainty modeling. Appendix C describes the supplementary material that can be used to complement this book, including images and

30

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

Week Chapter

Topics

1. Chapters 1–2 Introduction and image formation

2. Chapter 3

Image processing

3. Chapters 4–5 Optimization and learning

4. Chapter 5

Deep learning

5. Chapter 6

Recognition

6. Chapter 7

Feature detection and matching

7. Chapter 8

Image alignment and stitching

8. Chapter 9

Motion estimation

9. Chapter 10 Computational photography

10. Chapter 11 Structure from motion

11. Chapter 12 Depth estimation

12. Chapter 13 3D reconstruction

13. Chapter 14 Image-based rendering

Table 1.1 Sample syllabus for a one semester 13-week course. A 10-week quarter could go into lesser depth or omit some topics.

datasets, pointers to software, and course slides.

1.4 Sample syllabus
Teaching all of the material covered in this book in a single quarter or semester course is a Herculean task and likely one not worth attempting.11 It is better to simply pick and choose topics related to the lecturer’s preferred emphasis and tailored to the set of mini-projects envisioned for the students.
Steve Seitz and I have successfully used a 10-week syllabus similar to the one shown in Table 1.1 as both an undergraduate and a graduate-level course in computer vision. The undergraduate course12 tends to go lighter on the mathematics and takes more time reviewing basics, while the graduate-level course13 dives more deeply into techniques and assumes the students already have a decent grounding in either vision or related mathematical techniques. Related courses have also been taught on the topics of 3D photography and computational
11Some universities, such as Stanford (CS231A & 231N), Berkeley (CS194-26/294-26 & 280), and the University of Michigan (EECS 498/598 & 442), now split the material over two courses.
12http://www.cs.washington.edu/education/courses/455 13http://www.cs.washington.edu/education/courses/576

1.5 A note on notation

31

photography. Appendix C.3 and the book’s website list other courses that use this book to teach a similar curriculum.
When Steve and I teach the course, we prefer to give the students several small programming assignments early in the course rather than focusing on written homework or quizzes. With a suitable choice of topics, it is possible for these projects to build on each other. For example, introducing feature matching early on can be used in a second assignment to do image alignment and stitching. Alternatively, direct (optical ﬂow) techniques can be used to do the alignment and more focus can be put on either graph cut seam selection or multi-resolution blending techniques.
In the past, we have also asked the students to propose a ﬁnal project (we provide a set of suggested topics for those who need ideas) by the middle of the course and reserved the last week of the class for student presentations. Sometimes, a few of these projects have actually turned into conference submissions!
No matter how you decide to structure the course or how you choose to use this book, I encourage you to try at least a few small programming tasks to get a feel for how vision techniques work and how they fail. Better yet, pick topics that are fun and can be used on your own photographs, and try to push your creative boundaries to come up with surprising results.

1.5 A note on notation
For better or worse, the notation found in computer vision and multi-view geometry textbooks tends to vary all over the map (Faugeras 1993; Hartley and Zisserman 2004; Girod, Greiner, and Niemann 2000; Faugeras and Luong 2001; Forsyth and Ponce 2003). In this book, I use the convention I ﬁrst learned in my high school physics class (and later multi-variate calculus and computer graphics courses), which is that vectors v are lower case bold, matrices M are upper case bold, and scalars (T, s) are mixed case italic. Unless otherwise noted, vectors operate as column vectors, i.e., they post-multiply matrices, Mv, although they are sometimes written as comma-separated parenthesized lists x = (x, y) instead of bracketed column vectors x = [x y]T . Some commonly used matrices are R for rotations, K for calibration matrices, and I for the identity matrix. Homogeneous coordinates (Section 2.1) are denoted with a tilde over the vector, e.g., x˜ = (x˜, y˜, w˜) = w˜(x, y, 1) = w˜x¯ in P2. The cross product operator in matrix form is denoted by [ ]×.

32

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

1.6 Additional reading

This book attempts to be self-contained, so that students can implement the basic assignments and algorithms described here without the need for outside references. However, it does presuppose a general familiarity with basic concepts in linear algebra and numerical techniques, which are reviewed in Appendix A, and image processing, which is reviewed in Chapter 3.
Students who want to delve more deeply into these topics can look in Golub and Van Loan (1996) for matrix algebra and Strang (1988) for linear algebra. In image processing, there are a number of popular textbooks, including Crane (1997), Gomes and Velho (1997), Ja¨hne (1997), Pratt (2007), Russ (2007), Burger and Burge (2008), and Gonzalez and Woods (2017). For computer graphics, popular texts include Hughes, van Dam et al. (2013) and Marschner and Shirley (2015), with Glassner (1995) providing a more in-depth look at image formation and rendering. For statistics and machine learning, Chris Bishop’s (2006) book is a wonderful and comprehensive introduction with a wealth of exercises, while Murphy (2012) provides a more recent take on the ﬁeld and Hastie, Tibshirani, and Friedman (2009) a more classic treatment. A great introductory text to deep learning is Glassner (2018), while Goodfellow, Bengio, and Courville (2016) and Zhang, Lipton et al. (2021) provide more comprehensive treatments. Students may also want to look in other textbooks on computer vision for material that we do not cover here, as well as for additional project ideas (Nalwa 1993; Trucco and Verri 1998; Hartley and Zisserman 2004; Forsyth and Ponce 2011; Prince 2012; Davies 2017).
There is, however, no substitute for reading the latest research literature, both for the latest ideas and techniques and for the most up-to-date references to related literature.14 In this book, I have attempted to cite the most recent work in each ﬁeld so that students can read them directly and use them as inspiration for their own work. Browsing the last few years’ conference proceedings from the major vision, graphics, and machine learning conferences, such as CVPR, ECCV, ICCV, SIGGRAPH, and NeurIPS, as well as keeping an eye out for the latest publications on arXiv, will provide a wealth of new ideas. The tutorials offered at these conferences, for which slides or notes are often available online, are also an invaluable resource.

14For a comprehensive bibliography and taxonomy of computer vision research, Keith Price’s Annotated Computer Vision Bibliography http://www.visionbib.com/bibliography/contents.html is an invaluable resource.

Chapter 2
Image formation
2.1 Geometric primitives and transformations . . . . . . . . . . . . . . . . . . . 36 2.1.1 2D transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 2.1.2 3D transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 2.1.3 3D rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 2.1.4 3D to 2D projections . . . . . . . . . . . . . . . . . . . . . . . . . . 51 2.1.5 Lens distortions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
2.2 Photometric image formation . . . . . . . . . . . . . . . . . . . . . . . . . . 66 2.2.1 Lighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 2.2.2 Reﬂectance and shading . . . . . . . . . . . . . . . . . . . . . . . . 67 2.2.3 Optics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
2.3 The digital camera . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 2.3.1 Sampling and aliasing . . . . . . . . . . . . . . . . . . . . . . . . . 84 2.3.2 Color . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 2.3.3 Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
2.4 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 2.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

34

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

n^

(a)

(b)

f = 100mm d

GRGR BGBG GRGR

zi=102mm

zo=5m
(c)

BGBG
(d)

Figure 2.1 A few components of the image formation process: (a) perspective projection; (b) light scattering when hitting a surface; (c) lens optics; (d) Bayer color ﬁlter array.

2 Image formation

35

Before we can intelligently analyze and manipulate images, we need to establish a vocabulary for describing the geometry of a scene. We also need to understand the image formation process that produced a particular image given a set of lighting conditions, scene geometry, surface properties, and camera optics. In this chapter, we present a simpliﬁed model of such an image formation process.
Section 2.1 introduces the basic geometric primitives used throughout the book (points, lines, and planes) and the geometric transformations that project these 3D quantities into 2D image features (Figure 2.1a). Section 2.2 describes how lighting, surface properties (Figure 2.1b), and camera optics (Figure 2.1c) interact to produce the color values that fall onto the image sensor. Section 2.3 describes how continuous color images are turned into discrete digital samples inside the image sensor (Figure 2.1d) and how to avoid (or at least characterize) sampling deﬁciencies, such as aliasing.
The material covered in this chapter is but a brief summary of a very rich and deep set of topics, traditionally covered in a number of separate ﬁelds. A more thorough introduction to the geometry of points, lines, planes, and projections can be found in textbooks on multi-view geometry (Hartley and Zisserman 2004; Faugeras and Luong 2001) and computer graphics (Hughes, van Dam et al. 2013). The image formation (synthesis) process is traditionally taught as part of a computer graphics curriculum (Glassner 1995; Watt 1995; Hughes, van Dam et al. 2013; Marschner and Shirley 2015) but it is also studied in physics-based computer vision (Wolff, Shafer, and Healey 1992a). The behavior of camera lens systems is studied in optics (Mo¨ller 1988; Ray 2002; Hecht 2015). Some good books on color theory are Healey and Shafer (1992), Wandell (1995), and Wyszecki and Stiles (2000), with Livingstone (2008) providing a more fun and informal introduction to the topic of color perception. Topics relating to sampling and aliasing are covered in textbooks on signal and image processing (Crane 1997; Ja¨hne 1997; Oppenheim and Schafer 1996; Oppenheim, Schafer, and Buck 1999; Pratt 2007; Russ 2007; Burger and Burge 2008; Gonzalez and Woods 2017). The recent book by Ikeuchi, Matsushita et al. (2020) also covers 3D geometry, photometry, and sensor models, with an emphasis on active illumination systems.
A note to students: If you have already studied computer graphics, you may want to skim the material in Section 2.1, although the sections on projective depth and object-centered projection near the end of Section 2.1.4 may be new to you. Similarly, physics students (as well as computer graphics students) will mostly be familiar with Section 2.2. Finally, students with a good background in image processing will already be familiar with sampling issues (Section 2.3) as well as some of the material in Chapter 3.

36

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

2.1 Geometric primitives and transformations

In this section, we introduce the basic 2D and 3D primitives used in this textbook, namely points, lines, and planes. We also describe how 3D features are projected into 2D features. More detailed descriptions of these topics (along with a gentler and more intuitive introduction) can be found in textbooks on multiple-view geometry (Hartley and Zisserman 2004; Faugeras and Luong 2001).
Geometric primitives form the basic building blocks used to describe three-dimensional shapes. In this section, we introduce points, lines, and planes. Later sections of the book discuss curves (Sections 7.3 and 12.2), surfaces (Section 13.3), and volumes (Section 13.5).

2D points. 2D points (pixel coordinates in an image) can be denoted using a pair of values, x = (x, y) ∈ R2, or alternatively,

x= x .

(2.1)

y

(As stated in the introduction, we use the (x1, x2, . . .) notation to denote column vectors.) 2D points can also be represented using homogeneous coordinates, x˜ = (x˜, y˜, w˜) ∈ P2,
where vectors that differ only by scale are considered to be equivalent. P2 = R3 − (0, 0, 0) is called the 2D projective space.
A homogeneous vector x˜ can be converted back into an inhomogeneous vector x by dividing through by the last element w˜, i.e.,

x˜ = (x˜, y˜, w˜) = w˜(x, y, 1) = w˜x¯,

(2.2)

where x¯ = (x, y, 1) is the augmented vector. Homogeneous points whose last element is w˜ = 0 are called ideal points or points at inﬁnity and do not have an equivalent inhomogeneous representation.

2D lines. 2D lines can also be represented using homogeneous coordinates ˜l = (a, b, c). The corresponding line equation is

x¯ ·˜l = ax + by + c = 0.

(2.3)

We can normalize the line equation vector so that l = (nˆx, nˆy, d) = (nˆ, d) with nˆ = 1. In this case, nˆ is the normal vector perpendicular to the line and d is its distance to the origin (Figure 2.2). (The one exception to this normalization is the line at inﬁnity ˜l = (0, 0, 1), which includes all (ideal) points at inﬁnity.)

2.1 Geometric primitives and transformations

37

y n^ l
d θ

z

n^

m

x

d

x

y

(a)

(b)

Figure 2.2 (a) 2D line equation and (b) 3D plane equation, expressed in terms of the normal nˆ and distance to the origin d.

We can also express nˆ as a function of rotation angle θ, nˆ = (nˆx, nˆy) = (cos θ, sin θ) (Figure 2.2a). This representation is commonly used in the Hough transform line-ﬁnding algorithm, which is discussed in Section 7.4.2. The combination (θ, d) is also known as polar coordinates.
When using homogeneous coordinates, we can compute the intersection of two lines as

x˜ = ˜l1 ×˜l2,

(2.4)

where × is the cross product operator. Similarly, the line joining two points can be written as

˜l = x˜1 × x˜2.

(2.5)

When trying to ﬁt an intersection point to multiple lines or, conversely, a line to multiple points, least squares techniques (Section 8.1.1 and Appendix A.2) can be used, as discussed in Exercise 2.1.

2D conics. There are other algebraic curves that can be expressed with simple polynomial homogeneous equations. For example, the conic sections (so called because they arise as the intersection of a plane and a 3D cone) can be written using a quadric equation

x˜T Qx˜ = 0.

(2.6)

Quadric equations play useful roles in the study of multi-view geometry and camera calibration (Hartley and Zisserman 2004; Faugeras and Luong 2001) but are not used extensively in this book.

3D points. Point coordinates in three dimensions can be written using inhomogeneous coordinates x = (x, y, z) ∈ R3 or homogeneous coordinates x˜ = (x˜, y˜, z˜, w˜) ∈ P3. As before,

38

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

z p
λ
r=(1-λ)p+λq

q

x

y

Figure 2.3 3D line equation, r = (1 − λ)p + λq.

it is sometimes useful to denote a 3D point using the augmented vector x¯ = (x, y, z, 1) with x˜ = w˜x¯.

3D planes. 3D planes can also be represented as homogeneous coordinates m˜ = (a, b, c, d) with a corresponding plane equation

x¯ · m˜ = ax + by + cz + d = 0.

(2.7)

We can also normalize the plane equation as m = (nˆx, nˆy, nˆz, d) = (nˆ, d) with nˆ = 1. In this case, nˆ is the normal vector perpendicular to the plane and d is its distance to the origin (Figure 2.2b). As with the case of 2D lines, the plane at inﬁnity m˜ = (0, 0, 0, 1), which contains all the points at inﬁnity, cannot be normalized (i.e., it does not have a unique normal or a ﬁnite distance).
We can express nˆ as a function of two angles (θ, φ),

nˆ = (cos θ cos φ, sin θ cos φ, sin φ),

(2.8)

i.e., using spherical coordinates, but these are less commonly used than polar coordinates since they do not uniformly sample the space of possible normal vectors.

3D lines. Lines in 3D are less elegant than either lines in 2D or planes in 3D. One possible representation is to use two points on the line, (p, q). Any other point on the line can be expressed as a linear combination of these two points

r = (1 − λ)p + λq,

(2.9)

as shown in Figure 2.3. If we restrict 0 ≤ λ ≤ 1, we get the line segment joining p and q. If we use homogeneous coordinates, we can write the line as

˜r = µp˜ + λq˜.

(2.10)

2.1 Geometric primitives and transformations

39

A special case of this is when the second point is at inﬁnity, i.e., q˜ = (dˆx, dˆy, dˆz, 0) = (dˆ, 0). Here, we see that dˆ is the direction of the line. We can then re-write the inhomogeneous 3D

line equation as

r = p + λdˆ.

(2.11)

A disadvantage of the endpoint representation for 3D lines is that it has too many degrees

of freedom, i.e., six (three for each endpoint) instead of the four degrees that a 3D line truly

has. However, if we ﬁx the two points on the line to lie in speciﬁc planes, we obtain a rep-

resentation with four degrees of freedom. For example, if we are representing nearly vertical

lines, then z = 0 and z = 1 form two suitable planes, i.e., the (x, y) coordinates in both

planes provide the four coordinates describing the line. This kind of two-plane parameteri-

zation is used in the light ﬁeld and Lumigraph image-based rendering systems described in

Chapter 14 to represent the collection of rays seen by a camera as it moves in front of an

object. The two-endpoint representation is also useful for representing line segments, even

when their exact endpoints cannot be seen (only guessed at).

If we wish to represent all possible lines without bias towards any particular orientation,

we can use Plu¨cker coordinates (Hartley and Zisserman 2004, Section 3.2; Faugeras and

Luong 2001, Chapter 3). These coordinates are the six independent non-zero entries in the 4

× 4 skew symmetric matrix

L = p˜q˜T − q˜p˜T ,

(2.12)

where p˜ and q˜ are any two (non-identical) points on the line. This representation has only four degrees of freedom, since L is homogeneous and also satisﬁes |L| = 0, which results in a quadratic constraint on the Plu¨cker coordinates.
In practice, the minimal representation is not essential for most applications. An adequate model of 3D lines can be obtained by estimating their direction (which may be known ahead of time, e.g., for architecture) and some point within the visible portion of the line (see Section 11.4.8) or by using the two endpoints, since lines are most often visible as ﬁnite line segments. However, if you are interested in more details about the topic of minimal line parameterizations, Fo¨rstner (2005) discusses various ways to infer and model 3D lines in projective geometry, as well as how to estimate the uncertainty in such ﬁtted models.

3D quadrics. The 3D analog of a conic section is a quadric surface

x¯T Qx¯ = 0

(2.13)

(Hartley and Zisserman 2004, Chapter 3). Again, while quadric surfaces are useful in the study of multi-view geometry and can also serve as useful modeling primitives (spheres, ellipsoids, cylinders), we do not study them in great detail in this book.

40

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

y
translation

similarity

projective

Euclidean

affine

x

Figure 2.4 Basic set of 2D planar transformations.

2.1.1 2D transformations
Having deﬁned our basic primitives, we can now turn our attention to how they can be transformed. The simplest transformations occur in the 2D plane and are illustrated in Figure 2.4.

Translation. 2D translations can be written as x = x + t or

x = I t x¯,

(2.14)

where I is the (2 × 2) identity matrix or

x¯ =

I 0T

t x¯, 1

(2.15)

where 0 is the zero vector. Using a 2 × 3 matrix results in a more compact notation, whereas using a full-rank 3 × 3 matrix (which can be obtained from the 2 × 3 matrix by appending a [0T 1] row) makes it possible to chain transformations using matrix multiplication as well as to compute inverse transforms. Note that in any equation where an augmented vector such as x¯ appears on both sides, it can always be replaced with a full homogeneous vector x˜.

Rotation + translation. This transformation is also known as 2D rigid body motion or the

2D Euclidean transformation (since Euclidean distances are preserved). It can be written as

x = Rx + t or

x = R t x¯.

(2.16)

where

R = cos θ − sin θ sin θ cos θ

(2.17)

is an orthonormal rotation matrix with RRT = I and |R| = 1.

2.1 Geometric primitives and transformations

41

Scaled rotation. Also known as the similarity transform, this transformation can be expressed as x = sRx + t, where s is an arbitrary scale factor. It can also be written as

x = sR t x¯ = a −b tx x¯, b a ty

(2.18)

where we no longer require that a2 + b2 = 1. The similarity transform preserves angles between lines.

Afﬁne. The afﬁne transformation is written as x = Ax¯, where A is an arbitrary 2 × 3 matrix, i.e.,

x = a00 a01 a02 x¯. a10 a11 a12

(2.19)

Parallel lines remain parallel under afﬁne transformations.

Projective. This transformation, also known as a perspective transform or homography, operates on homogeneous coordinates,

x˜ = H˜ x˜,

(2.20)

where H˜ is an arbitrary 3 × 3 matrix. Note that H˜ is homogeneous, i.e., it is only deﬁned

up to a scale, and that two H˜ matrices that differ only by scale are equivalent. The resulting

homogeneous coordinate x˜ must be normalized in order to obtain an inhomogeneous result

x, i.e.,

x

=

h00x h20x

+ h01y + h21y

+ h02 + h22

and

y

=

h10x + h11y h20x + h21y

+ +

h12 h22

.

(2.21)

Perspective transformations preserve straight lines (i.e., they remain straight after the trans-

formation).

Hierarchy of 2D transformations. The preceding set of transformations are illustrated in Figure 2.4 and summarized in Table 2.1. The easiest way to think of them is as a set of (potentially restricted) 3 × 3 matrices operating on 2D homogeneous coordinate vectors. Hartley and Zisserman (2004) contains a more detailed description of the hierarchy of 2D planar transformations.
The above transformations form a nested set of groups, i.e., they are closed under composition and have an inverse that is a member of the same group. (This will be important later when applying these transformations to images in Section 3.6.) Each (simpler) group is a subgroup of the more complex group below it. The mathematics of such Lie groups and their

42

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

Transformation

Matrix # DoF Preserves

Icon

translation

It
2×3

2 orientation

rigid (Euclidean) R t
2×3

3 lengths

&& &&

similarity

sR t

4 angles

&

2×3

&

afﬁne projective

A
2×3
H˜
3×3

6 parallelism ¢ ¢ ¢¢
22 8 straight lines


Table 2.1 Hierarchy of 2D coordinate transformations, listing the transformation name, its matrix form, the number of degrees of freedom, what geometric properties it preserves, and a mnemonic icon. Each transformation also preserves the properties listed in the rows below it, i.e., similarity preserves not only angles but also parallelism and straight lines. The 2 × 3 matrices are extended with a third [0T 1] row to form a full 3 × 3 matrix for homogeneous coordinate transformations.

related algebras (tangent spaces at the origin) are discussed in a number of recent robotics tutorials (Dellaert and Kaess 2017; Blanco 2019; Sola`, Deray, and Atchuthan 2019), where the 2D rotation and rigid transforms are called SO(2) and SE(2), which stand for the special orthogonal and special Euclidean groups.1

Co-vectors. While the above transformations can be used to transform points in a 2D plane, can they also be used directly to transform a line equation? Consider the homogeneous equation ˜l · x˜ = 0. If we transform x˜ = H˜ x˜, we obtain

˜l · x˜ = ˜l T H˜ x˜ = (H˜ T˜l )T x˜ = ˜l · x˜ = 0,

(2.22)

i.e., ˜l = H˜ −T˜l. Thus, the action of a projective transformation on a co-vector such as a 2D line or 3D normal can be represented by the transposed inverse of the matrix, which is equivalent to the adjoint of H˜ , since projective transformation matrices are homogeneous. Jim Blinn (1998) describes (in Chapters 9 and 10) the ins and outs of notating and manipulating co-vectors.
1The term special refers to the desired condition of no reﬂection, i.e., det|R| = 1.

2.1 Geometric primitives and transformations

43

While the above transformations are the ones we use most extensively, a number of additional transformations are sometimes used.

Stretch/squash. This transformation changes the aspect ratio of an image,
x = sxx + tx y = syy + ty,
and is a restricted form of an afﬁne transformation. Unfortunately, it does not nest cleanly with the groups listed in Table 2.1.

Planar surface ﬂow. This eight-parameter transformation (Horn 1986; Bergen, Anandan et al. 1992; Girod, Greiner, and Niemann 2000),
x = a0 + a1x + a2y + a6x2 + a7xy y = a3 + a4x + a5y + a6xy + a7y2,
arises when a planar surface undergoes a small 3D motion. It can thus be thought of as a small motion approximation to a full homography. Its main attraction is that it is linear in the motion parameters, ak, which are often the quantities being estimated.
Bilinear interpolant. This eight-parameter transform (Wolberg 1990),
x = a0 + a1x + a2y + a6xy y = a3 + a4x + a5y + a7xy,
can be used to interpolate the deformation due to the motion of the four corner points of a square. (In fact, it can interpolate the motion of any four non-collinear points.) While the deformation is linear in the motion parameters, it does not generally preserve straight lines (only lines parallel to the square axes). However, it is often quite useful, e.g., in the interpolation of sparse grids using splines (Section 9.2.2).

2.1.2 3D transformations
The set of three-dimensional coordinate transformations is very similar to that available for 2D transformations and is summarized in Table 2.2. As in 2D, these transformations form a nested set of groups. Hartley and Zisserman (2004, Section 2.4) give a more detailed description of this hierarchy.

44

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

Transformation

Matrix # DoF Preserves

Icon

translation

It
3×4

3 orientation

rigid (Euclidean) R t
3×4

6 lengths

&& &&

similarity

sR t

7 angles

&

3×4

&

afﬁne projective

A
3×4
H˜
4×4

12 parallelism ¢ ¢ ¢¢
22 15 straight lines


Table 2.2 Hierarchy of 3D coordinate transformations. Each transformation also preserves the properties listed in the rows below it, i.e., similarity preserves not only angles but also parallelism and straight lines. The 3 × 4 matrices are extended with a fourth [0T 1] row to form a full 4 × 4 matrix for homogeneous coordinate transformations. The mnemonic icons are drawn in 2D but are meant to suggest transformations occurring in a full 3D cube.

Translation. 3D translations can be written as x = x + t or x = I t x¯,
where I is the (3 × 3) identity matrix.

(2.23)

Rotation + translation. Also known as 3D rigid body motion or the 3D Euclidean transformation or SE(3), it can be written as x = Rx + t or

x = R t x¯,

(2.24)

where R is a 3 × 3 orthonormal rotation matrix with RRT = I and |R| = 1. Note that sometimes it is more convenient to describe a rigid motion using

x = R(x − c) = Rx − Rc,

(2.25)

where c is the center of rotation (often the camera center). Compactly parameterizing a 3D rotation is a non-trivial task, which we describe in more
detail below.

2.1 Geometric primitives and transformations

45

Scaled rotation. The 3D similarity transform can be expressed as x = sRx + t where s is an arbitrary scale factor. It can also be written as

x = sR t x¯.

(2.26)

This transformation preserves angles between lines and planes.

Afﬁne. The afﬁne transform is written as x = Ax¯, where A is an arbitrary 3 × 4 matrix,

i.e.,





x = aa0100

a01 a11

a02 a12

a03 a13



x¯.

(2.27)

a20 a21 a22 a23

Parallel lines and planes remain parallel under afﬁne transformations.

Projective. This transformation, variously known as a 3D perspective transform, homography, or collineation, operates on homogeneous coordinates,

x˜ = H˜ x˜,

(2.28)

where H˜ is an arbitrary 4 × 4 homogeneous matrix. As in 2D, the resulting homogeneous coordinate x˜ must be normalized in order to obtain an inhomogeneous result x. Perspective transformations preserve straight lines (i.e., they remain straight after the transformation).

2.1.3 3D rotations
The biggest difference between 2D and 3D coordinate transformations is that the parameterization of the 3D rotation matrix R is not as straightforward, as several different possibilities exist.

Euler angles
A rotation matrix can be formed as the product of three rotations around three cardinal axes, e.g., x, y, and z, or x, y, and x. This is generally a bad idea, as the result depends on the order in which the transforms are applied.2 What is worse, it is not always possible to move smoothly in the parameter space, i.e., sometimes one or more of the Euler angles change dramatically in response to a small change in rotation.3 For these reasons, we do not even
2However, in special situations, such as describing the motion of a pan-tilt head, these angles may be more intuitive.
3In robotics, this is sometimes referred to as gimbal lock.

46

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

n^ u

v

v║

v××

v┴ θ v× u┴

Figure 2.5 Rotation around an axis nˆ by an angle θ.

give the formula for Euler angles in this book—interested readers can look in other textbooks or technical reports (Faugeras 1993; Diebel 2006). Note that, in some applications, if the rotations are known to be a set of uni-axial transforms, they can always be represented using an explicit set of rigid transformations.

Axis/angle (exponential twist)
A rotation can be represented by a rotation axis nˆ and an angle θ, or equivalently by a 3D vector ω = θnˆ. Figure 2.5 shows how we can compute the equivalent rotation. First, we project the vector v onto the axis nˆ to obtain

v = nˆ(nˆ · v) = (nˆnˆT )v,

(2.29)

which is the component of v that is not affected by the rotation. Next, we compute the perpendicular residual of v from nˆ,

v⊥ = v − v = (I − nˆnˆT )v. We can rotate this vector by 90° using the cross product,

(2.30)

v× = nˆ × v = [nˆ]×v,

(2.31)

where [nˆ]× is the matrix form of the cross product operator with the vector nˆ = (nˆx, nˆy, nˆz),

 0
[nˆ]× =  nˆz



−nˆz 0

−nˆnˆyx .

(2.32)

−nˆy nˆx 0

Note that rotating this vector by another 90° is equivalent to taking the cross product again,

v×× = nˆ × v× = [nˆ]2×v = −v⊥,

2.1 Geometric primitives and transformations

47

and hence

v = v − v⊥ = v + v×× = (I + [nˆ]2×)v.

We can now compute the in-plane component of the rotated vector u as

u⊥ = cos θv⊥ + sin θv× = (sin θ[nˆ]× − cos θ[nˆ]2×)v. Putting all these terms together, we obtain the ﬁnal rotated vector as

u = u⊥ + v = (I + sin θ[nˆ]× + (1 − cos θ)[nˆ]2×)v.

(2.33)

We can therefore write the rotation matrix corresponding to a rotation by θ around an axis nˆ

as

R(nˆ, θ) = I + sin θ[nˆ]× + (1 − cos θ)[nˆ]2×,

(2.34)

which is known as Rodrigues’ formula (Ayache 1989).

The product of the axis nˆ and angle θ, ω = θnˆ = (ωx, ωy, ωz), is a minimal representation for a 3D rotation. Rotations through common angles such as multiples of 90° can be represented exactly (and converted to exact matrices) if θ is stored in degrees. Unfortunately, this representation is not unique, since we can always add a multiple of 360° (2π radians) to θ and get the same rotation matrix. As well, (nˆ, θ) and (−nˆ, −θ) represent the same rotation.
However, for small rotations (e.g., corrections to rotations), this is an excellent choice.

In particular, for small (inﬁnitesimal or instantaneous) rotations and θ expressed in radians,

Rodrigues’ formula simpliﬁes to

 1
R(ω) ≈ I + sin θ[nˆ]× ≈ I + [θnˆ]× =  ωz



−ωz 1

ωy −ωx



,

(2.35)

−ωy ωx 1

which gives a nice linearized relationship between the rotation parameters ω and R. We can

also write R(ω)v ≈ v + ω × v, which is handy when we want to compute the derivative of

Rv with respect to ω,





0 z −y

∂Rv ∂ωT

=

−[v]×

=

−z

0

x  .

y −x 0

(2.36)

Another way to derive a rotation through a ﬁnite angle is called the exponential twist (Murray, Li, and Sastry 1994). A rotation by an angle θ is equivalent to k rotations through θ/k. In the limit as k → ∞, we obtain

R(nˆ,

θ)

=

lim (I
k→∞

+

1 k

[θnˆ]×

)k

=

exp

[ω]×.

(2.37)

48

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

z

║q║=1 q2 x

q1
w q0

-q2

y

Figure 2.6 Unit quaternions live on the unit sphere q = 1. This ﬁgure shows a smooth trajectory through the three quaternions q0, q1, and q2. The antipodal point to q2, namely −q2, represents the same rotation as q2.

If we expand the matrix exponential as a Taylor series (using the identity [nˆ]k×+2 = −[nˆ]k×, k > 0, and again assuming θ is in radians),

exp

[ω]×

=

I

+

θ[nˆ]×

+

θ2 2

[nˆ]2×

+

θ3 3!

[nˆ]3×

+

·

·

·

=

I+

(θ

−

θ3 3!

+ · · · )[nˆ]×

+

(

θ2 2

−

θ3 4!

+ · · · )[nˆ]2×

= I + sin θ[nˆ]× + (1 − cos θ)[nˆ]2×,

(2.38)

which yields the familiar Rodrigues’ formula. In robotics (and group theory), rotations are called SO(3), i.e., the special orthogonal
group in 3D. The incremental rotations ω are associated with a Lie algebra se(3) and are the preferred way to formulate rotation derivatives and to model uncertainties in rotation estimates (Blanco 2019; Sola`, Deray, and Atchuthan 2019).

Unit quaternions
The unit quaternion representation is closely related to the angle/axis representation. A unit quaternion is a unit length 4-vector whose components can be written as q = (qx, qy, qz, qw) or q = (x, y, z, w) for short. Unit quaternions live on the unit sphere q = 1 and antipodal (opposite sign) quaternions, q and −q, represent the same rotation (Figure 2.6). Other than this ambiguity (dual covering), the unit quaternion representation of a rotation is unique. Furthermore, the representation is continuous, i.e., as rotation matrices vary continuously, you can ﬁnd a continuous quaternion representation, although the path on the quaternion sphere may wrap all the way around before returning to the “origin” qo = (0, 0, 0, 1). For

2.1 Geometric primitives and transformations

49

these and other reasons given below, quaternions are a very popular representation for pose and for pose interpolation in computer graphics (Shoemake 1985).
Quaternions can be derived from the axis/angle representation through the formula

q

=

(v,

w)

=

(sin

θ 2

nˆ,

cos

θ 2

),

(2.39)

where nˆ and θ are the rotation axis and angle. Using the trigonometric identities sin θ =

2 sin

θ 2

cos

θ 2

and

(1

−

cos θ)

=

2 sin2

θ 2

,

Rodrigues’

formula

can

be

converted

to

R(nˆ, θ) = I + sin θ[nˆ]× + (1 − cos θ)[nˆ]2× = I + 2w[v]× + 2[v]2×.

(2.40)

This suggests a quick way to rotate a vector v by a quaternion using a series of cross products,

scalings, and additions. To obtain a formula for R(q) as a function of (x, y, z, w), recall that

 0
[v]× =  z





−z y

−y2 − z2

xy

0 −x and [v]2× =  xy

−x2 − z2

 xz yz  .

−y x 0

xz

yz

−x2 − y2

We thus obtain

 1

−

2(y2

+

z2)

R(q) =  2(xy + zw)

2(xz − yw)

2(xy − zw) 1 − 2(x2 + z2)
2(yz + xw)

 2(xz + yw) 2(yz − xw)  . 1 − 2(x2 + y2)

(2.41)

The diagonal terms can be made more symmetrical by replacing 1 − 2(y2 + z2) with (x2 + w2 − y2 − z2), etc.
The nicest aspect of unit quaternions is that there is a simple algebra for composing rotations expressed as unit quaternions. Given two quaternions q0 = (v0, w0) and q1 = (v1, w1), the quaternion multiply operator is deﬁned as

q2 = q0q1 = (v0 × v1 + w0v1 + w1v0, w0w1 − v0 · v1),

(2.42)

with the property that R(q2) = R(q0)R(q1). Note that quaternion multiplication is not commutative, just as 3D rotations and matrix multiplications are not.
Taking the inverse of a quaternion is easy: Just ﬂip the sign of v or w (but not both!). (You can verify this has the desired effect of transposing the R matrix in (2.41).) Thus, we can also deﬁne quaternion division as

q2 = q0/q1 = q0q−1 1 = (v0 × v1 + w0v1 − w1v0, −w0w1 − v0 · v1).

(2.43)

50

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

procedure slerp(q0, q1, α):

1. qr = q1/q0 = (vr, wr)

2. if wr < 0 then qr ← −qr

3. θr = 2 tan−1( vr /wr)

4. nˆr = N (vr) = vr/ vr

5. θα = α θr

6.

qα

=

(sin

θα 2

nˆr ,

cos

θα 2

)

7. return q2 = qαq0

Algorithm 2.1 Spherical linear interpolation (slerp). The axis and total angle are ﬁrst computed from the quaternion ratio. (This computation can be lifted outside an inner loop that generates a set of interpolated position for animation.) An incremental quaternion is then computed and multiplied by the starting rotation quaternion.

This is useful when the incremental rotation between two rotations is desired.

In particular, if we want to determine a rotation that is partway between two given rota-

tions, we can compute the incremental rotation, take a fraction of the angle, and compute the

new rotation. This procedure is called spherical linear interpolation or slerp for short (Shoe-

make 1985) and is given in Algorithm 2.1. Note that Shoemake presents two formulas other

than the one given here. The ﬁrst exponentiates qr by alpha before multiplying the original quaternion,

q2 = qαr q0,

(2.44)

while the second treats the quaternions as 4-vectors on a sphere and uses

q2

=

sin(1 − α)θ sin θ

q0

+

sin αθ sin θ

q1

,

(2.45)

where θ = cos−1(q0 · q1) and the dot product is directly between the quaternion 4-vectors. All of these formulas give comparable results, although care should be taken when q0 and q1 are close together, which is why I prefer to use an arctangent to establish the rotation angle.

Which rotation representation is better? The choice of representation for 3D rotations depends partly on the application.

2.1 Geometric primitives and transformations

51

The axis/angle representation is minimal, and hence does not require any additional constraints on the parameters (no need to re-normalize after each update). If the angle is expressed in degrees, it is easier to understand the pose (say, 90° twist around x-axis), and also easier to express exact rotations. When the angle is in radians, the derivatives of R with respect to ω can easily be computed (2.36).
Quaternions, on the other hand, are better if you want to keep track of a smoothly moving camera, since there are no discontinuities in the representation. It is also easier to interpolate between rotations and to chain rigid transformations (Murray, Li, and Sastry 1994; Bregler and Malik 1998).
My usual preference is to use quaternions, but to update their estimates using an incremental rotation, as described in Section 11.2.2.

2.1.4 3D to 2D projections
Now that we know how to represent 2D and 3D geometric primitives and how to transform them spatially, we need to specify how 3D primitives are projected onto the image plane. We can do this using a linear 3D to 2D projection matrix. The simplest model is orthography, which requires no division to get the ﬁnal (inhomogeneous) result. The more commonly used model is perspective, since this more accurately models the behavior of real cameras.

Orthography and para-perspective
An orthographic projection simply drops the z component of the three-dimensional coordinate p to obtain the 2D point x. (In this section, we use p to denote 3D points and x to denote 2D points.) This can be written as

x = [I2×2|0] p.

(2.46)

If we are using homogeneous (projective) coordinates, we can write





1000

x˜ = 0 1 0 0 p˜,

0001

(2.47)

i.e., we drop the z component but keep the w component. Orthography is an approximate model for long focal length (telephoto) lenses and objects whose depth is shallow relative to their distance to the camera (Sawhney and Hanson 1991). It is exact only for telecentric lenses (Baker and Nayar 1999, 2001).

52

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

(a) 3D view

(b) orthography

(c) scaled orthography

(d) para-perspective

(e) perspective

(f) object-centered

Figure 2.7 Commonly used projection models: (a) 3D view of world, (b) orthography, (c) scaled orthography, (d) para-perspective, (e) perspective, (f) object-centered. Each diagram shows a top-down view of the projection. Note how parallel lines on the ground plane and box sides remain parallel in the non-perspective projections.

2.1 Geometric primitives and transformations

53

In practice, world coordinates (which may measure dimensions in meters) need to be scaled to ﬁt onto an image sensor (physically measured in millimeters, but ultimately measured in pixels). For this reason, scaled orthography is actually more commonly used,

x = [sI2×2|0] p.

(2.48)

This model is equivalent to ﬁrst projecting the world points onto a local fronto-parallel image plane and then scaling this image using regular perspective projection. The scaling can be the same for all parts of the scene (Figure 2.7b) or it can be different for objects that are being modeled independently (Figure 2.7c). More importantly, the scaling can vary from frame to frame when estimating structure from motion, which can better model the scale change that occurs as an object approaches the camera.
Scaled orthography is a popular model for reconstructing the 3D shape of objects far away from the camera, since it greatly simpliﬁes certain computations. For example, pose (camera orientation) can be estimated using simple least squares (Section 11.2.1). Under orthography, structure and motion can simultaneously be estimated using factorization (singular value decomposition), as discussed in Section 11.4.1 (Tomasi and Kanade 1992).
A closely related projection model is para-perspective (Aloimonos 1990; Poelman and Kanade 1997). In this model, object points are again ﬁrst projected onto a local reference parallel to the image plane. However, rather than being projected orthogonally to this plane, they are projected parallel to the line of sight to the object center (Figure 2.7d). This is followed by the usual projection onto the ﬁnal image plane, which again amounts to a scaling. The combination of these two projections is therefore afﬁne and can be written as





x˜ = aa0100

a01 a11

a02 a12

aa0133 p˜.

0001

(2.49)

Note how parallel lines in 3D remain parallel after projection in Figure 2.7b–d. Para-perspective provides a more accurate projection model than scaled orthography, without incurring the added complexity of per-pixel perspective division, which invalidates traditional factorization methods (Poelman and Kanade 1997).

Perspective
The most commonly used projection in computer graphics and computer vision is true 3D perspective (Figure 2.7e). Here, points are projected onto the image plane by dividing them

54

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

by their z component. Using inhomogeneous coordinates, this can be written as  x/z
x¯ = Pz(p) = y/z .
1

(2.50)

In homogeneous coordinates, the projection has a simple linear form,





1000

x˜ = 0 1 0 0 p˜,

0010

(2.51)

i.e., we drop the w component of p. Thus, after projection, it is not possible to recover the

distance of the 3D point from the image, which makes sense for a 2D imaging sensor.

A form often seen in computer graphics systems is a two-step projection that ﬁrst projects

3D coordinates into normalized device coordinates in the range (x, y, z) ∈ [−1, 1]×[−1, 1]×

[0, 1], and then rescales these coordinates to integer pixel coordinates using a viewport trans-

formation (Watt 1995; OpenGL-ARB 1997). The (initial) perspective projection is then rep-

resented using a 4 × 4 matrix





10

0

0

x˜ = 00

1 0

0 −zfar/zrange

znearzfa0r/zrange p˜,

(2.52)

00

1

0

where znear and zfar are the near and far z clipping planes and zrange = zfar − znear. Note that the ﬁrst two rows are actually scaled by the focal length and the aspect ratio so that visible rays are mapped to (x, y, z) ∈ [−1, 1]2. The reason for keeping the third row, rather than dropping it, is that visibility operations, such as z-buffering, require a depth for every graphical element that is being rendered.
If we set znear = 1, zfar → ∞, and switch the sign of the third row, the third element of the normalized screen vector becomes the inverse depth, i.e., the disparity (Okutomi and Kanade 1993). This can be quite convenient in many cases since, for cameras moving around outdoors, the inverse depth to the camera is often a more well-conditioned parameterization than direct 3D distance.
While a regular 2D image sensor has no way of measuring distance to a surface point, range sensors (Section 13.2) and stereo matching algorithms (Chapter 12) can compute such values. It is then convenient to be able to map from a sensor-based depth or disparity value d directly back to a 3D location using the inverse of a 4 × 4 matrix (Section 2.1.4). We can do this if we represent perspective projection using a full-rank 4 × 4 matrix, as in (2.64).

2.1 Geometric primitives and transformations

55

yc

sx

xs

cs

pc

sy p
ys

xc

Oc

zc

Figure 2.8 Projection of a 3D camera-centered point pc onto the sensor planes at location p. Oc is the optical center (nodal point), cs is the 3D origin of the sensor plane coordinate system, and sx and sy are the pixel spacings.

Camera intrinsics

Once we have projected a 3D point through an ideal pinhole using a projection matrix, we must still transform the resulting coordinates according to the pixel sensor spacing and the relative position of the sensor plane to the origin. Figure 2.8 shows an illustration of the geometry involved. In this section, we ﬁrst present a mapping from 2D pixel coordinates to 3D rays using a sensor homography Ms, since this is easier to explain in terms of physically measurable quantities. We then relate these quantities to the more commonly used camera intrinsic matrix K, which is used to map 3D camera-centered points pc to 2D pixel coordinates x˜s.
Image sensors return pixel values indexed by integer pixel coordinates (xs, ys), often with the coordinates starting at the upper-left corner of the image and moving down and to the right. (This convention is not obeyed by all imaging libraries, but the adjustment for other coordinate systems is straightforward.) To map pixel centers to 3D coordinates, we ﬁrst scale the (xs, ys) values by the pixel spacings (sx, sy) (sometimes expressed in microns for solid-state sensors) and then describe the orientation of the sensor array relative to the camera projection center Oc with an origin cs and a 3D rotation Rs (Figure 2.8).
The combined 2D to 3D projection can then be written as

p = Rs



sx

cs



0 0

0 sy 0

 0





00 xyss = Msx¯s.

1

0 01

(2.53)

The ﬁrst two columns of the 3 × 3 matrix Ms are the 3D vectors corresponding to unit steps in the image pixel array along the xs and ys directions, while the third column is the 3D

56

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

image array origin cs. The matrix Ms is parameterized by eight unknowns: the three parameters describing
the rotation Rs, the three parameters describing the translation cs, and the two scale factors (sx, sy). Note that we ignore here the possibility of skew between the two axes on the image plane, since solid-state manufacturing techniques render this negligible. In practice, unless we have accurate external knowledge of the sensor spacing or sensor orientation, there are only seven degrees of freedom, since the distance of the sensor from the origin cannot be teased apart from the sensor spacing, based on external image measurement alone.
However, estimating a camera model Ms with the required seven degrees of freedom (i.e., where the ﬁrst two columns are orthogonal after an appropriate re-scaling) is impractical, so most practitioners assume a general 3 × 3 homogeneous matrix form.
The relationship between the 3D pixel center p and the 3D camera-centered point pc is given by an unknown scaling s, p = spc. We can therefore write the complete projection between pc and a homogeneous version of the pixel address x˜s as

x˜s = αM−s 1pc = Kpc.

(2.54)

The 3 × 3 matrix K is called the calibration matrix and describes the camera intrinsics (as

opposed to the camera’s orientation in space, which are called the extrinsics).

From the above discussion, we see that K has seven degrees of freedom in theory and

eight degrees of freedom (the full dimensionality of a 3 × 3 homogeneous matrix) in practice.

Why, then, do most textbooks on 3D computer vision and multi-view geometry (Faugeras

1993; Hartley and Zisserman 2004; Faugeras and Luong 2001) treat K as an upper-triangular

matrix with ﬁve degrees of freedom?

While this is usually not made explicit in these books, it is because we cannot recover

the full K matrix based on external measurement alone. When calibrating a camera (Sec-

tion 11.1) based on external 3D points or other measurements (Tsai 1987), we end up esti-

mating the intrinsic (K) and extrinsic (R, t) camera parameters simultaneously using a series

of measurements,

x˜s = K R t pw = Ppw,

(2.55)

where pw are known 3D world coordinates and

P = K[R|t]

(2.56)

is known as the camera matrix. Inspecting this equation, we see that we can post-multiply
K by R1 and pre-multiply [R|t] by RT1 , and still end up with a valid calibration. Thus, it is impossible based on image measurements alone to know the true orientation of the sensor
and the true camera intrinsics.

2.1 Geometric primitives and transformations

57

W-1

0

xs

yc

0 (cx,cy)

f

xc

- zc

H-1 ys

Figure 2.9 Simpliﬁed camera intrinsics showing the focal length f and the image center (cx, cy). The image width and height are W and H.

The choice of an upper-triangular form for K seems to be conventional. Given a full 3 × 4 camera matrix P = K[R|t], we can compute an upper-triangular K matrix using QR factorization (Golub and Van Loan 1996). (Note the unfortunate clash of terminologies: In matrix algebra textbooks, R represents an upper-triangular (right of the diagonal) matrix; in computer vision, R is an orthogonal rotation.)
There are several ways to write the upper-triangular form of K. One possibility is





K = f0x

s fy

ccxy ,

001

(2.57)

which uses independent focal lengths fx and fy for the sensor x and y dimensions. The entry s encodes any possible skew between the sensor axes due to the sensor not being mounted

perpendicular to the optical axis and (cx, cy) denotes the image center expressed in pixel coordinates. The image center is also often called the principal point in the computer vision

literature (Hartley and Zisserman 2004), although in optics, the principal points are 3D points

usually inside the lens where the principal planes intersect the principal (optical) axis (Hecht

2015). Another possibility is





f K = 0

s af

cx cy



,

(2.58)

00 1

where the aspect ratio a has been made explicit and a common focal length f is used. In practice, for many applications an even simpler form can be obtained by setting a = 1

58

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

θ/2

f

W/2 Z

(x,y,1)

(X,Y,Z)

Figure 2.10 Central projection, showing the relationship between the 3D and 2D coordinates, p and x, as well as the relationship between the focal length f , image width W , and the horizontal ﬁeld of view θH.

and s = 0,





f K = 0

0 f

ccxy .

00 1

(2.59)

Often, setting the origin at roughly the center of the image, e.g., (cx, cy) = (W/2, H/2), where W and H are the image height and width, can result in a perfectly usable camera model with a single unknown, i.e., the focal length f .
Figure 2.9 shows how these quantities can be visualized as part of a simpliﬁed imaging model. Note that now we have placed the image plane in front of the nodal point (projection center of the lens). The sense of the y-axis has also been ﬂipped to get a coordinate system compatible with the way that most imaging libraries treat the vertical (row) coordinate.

A note on focal lengths

The issue of how to express focal lengths is one that often causes confusion in implementing computer vision algorithms and discussing their results. This is because the focal length depends on the units used to measure pixels.
If we number pixel coordinates using integer values, say [0, W ) × [0, H), the focal length f and camera center (cx, cy) in (2.59) can be expressed as pixel values. How do these quantities relate to the more familiar focal lengths used by photographers?
Figure 2.10 illustrates the relationship between the focal length f , the sensor width W , and the horizontal ﬁeld of view θH, which obey the formula

tan

θH 2

=

W 2f

or

f

=

W 2

tan

θH 2

−1
.

(2.60)

2.1 Geometric primitives and transformations

59

For a traditional 35mm ﬁlm camera, whose active exposure area is 24mm × 36mm, we have W = 36mm, and hence f is also expressed in millimeters.4 For example, the “stock” lens that often comes with SLR (single lens reﬂex) cameras is 50mm, which is a good length, whereas 85mm is the standard for portrait photography. Since we work with digital images, however, it is more convenient to express W in pixels so that the focal length f can be used directly in the calibration matrix K as in (2.59).
Another possibility is to scale the pixel coordinates so that they go from [−1, 1) along the longer image dimension and [−a−1, a−1) along the shorter axis, where a ≥ 1 is the image aspect ratio (as opposed to the sensor cell aspect ratio introduced earlier). This can be accomplished using modiﬁed normalized device coordinates,

xs = (2xs − W )/S and ys = (2ys − H)/S, where S = max(W, H). (2.61)

This has the advantage that the focal length f and image center (cx, cy) become independent of the image resolution, which can be useful when using multi-resolution, image-processing algorithms, such as image pyramids (Section 3.5).5 The use of S instead of W also makes the focal length the same for landscape (horizontal) and portrait (vertical) pictures, as is the case in 35mm photography. (In some computer graphics textbooks and systems, normalized device coordinates go from [−1, 1] × [−1, 1], which requires the use of two different focal lengths to describe the camera intrinsics (Watt 1995).) Setting S = W = 2 in (2.60), we obtain the simpler (unitless) relationship

f −1

=

tan

θH 2

.

(2.62)

The conversion between the various focal length representations is straightforward, e.g., to go from a unitless f to one expressed in pixels, multiply by W/2, while to convert from an f expressed in pixels to the equivalent 35mm focal length, multiply by 18mm.

Camera matrix
Now that we have shown how to parameterize the calibration matrix K, we can put the camera intrinsics and extrinsics together to obtain a single 3 × 4 camera matrix

P=K R t .

(2.63)

435mm denotes the width of the ﬁlm strip, of which 24mm is used for exposing each frame and the remaining 11mm for perforation and frame numbering.
5To make the conversion truly accurate after a downsampling step in a pyramid, ﬂoating point values of W and H would have to be maintained, as they can become non-integer if they are ever odd at a larger resolution in the pyramid.

60

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

It is sometimes preferable to use an invertible 4 × 4 matrix, which can be obtained by not dropping the last row in the P matrix,

P˜ =

K 0T

0 1

R 0T

t = K˜ E, 1

(2.64)

where E is a 3D rigid-body (Euclidean) transformation and K˜ is the full-rank calibration matrix. The 4 × 4 camera matrix P˜ can be used to map directly from 3D world coordinates p¯w = (xw, yw, zw, 1) to screen coordinates (plus disparity), xs = (xs, ys, 1, d),

xs ∼ P˜ p¯w,

(2.65)

where ∼ indicates equality up to scale. Note that after multiplication by P˜ , the vector is divided by the third element of the vector to obtain the normalized form xs = (xs, ys, 1, d).

Plane plus parallax (projective depth)

In general, when using the 4 × 4 matrix P˜ , we have the freedom to remap the last row to

whatever suits our purpose (rather than just being the “standard” interpretation of disparity as

inverse depth). Let us re-write the last row of P˜ as p3 = s3[nˆ0|c0], where nˆ0 = 1. We

then have the equation

d

=

s3 z

(nˆ0

· pw

+

c0),

(2.66)

where z = p2 · p¯w = rz · (pw − c) is the distance of pw from the camera center C (2.25) along the optical axis Z (Figure 2.11). Thus, we can interpret d as the projective disparity

or projective depth of a 3D scene point pw from the reference plane nˆ0 · pw + c0 = 0 (Szeliski and Coughlan 1997; Szeliski and Golland 1999; Shade, Gortler et al. 1998; Baker,

Szeliski, and Anandan 1998). (The projective depth is also sometimes called parallax in

reconstruction algorithms that use the term plane plus parallax (Kumar, Anandan, and Hanna

1994; Sawhney 1994).) Setting nˆ0 = 0 and c0 = 1, i.e., putting the reference plane at inﬁnity, results in the more standard d = 1/z version of disparity (Okutomi and Kanade 1993).

Another way to see this is to invert the P˜ matrix so that we can map pixels plus disparity

directly back to 3D points,

p˜w = P˜ −1xs.

(2.67)

In general, we can choose P˜ to have whatever form is convenient, i.e., to sample space using an arbitrary projection. This can come in particularly handy when setting up multi-view stereo reconstruction algorithms, since it allows us to sweep a series of planes (Section 12.1.2) through space with a variable (projective) sampling that best matches the sensed image motions (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).

2.1 Geometric primitives and transformations

61

d=1.0 d=0.67 d=0.5 d

d=0.5 d=0 d=-0.25

C

(xs,ys,d)

(xw,yw,zw)

zZ

C

(xs,ys,d)

parallax (xw,yw,zw) zZ

image plane d = inverse depth

image plane

plane

d = projective depth

Figure 2.11 Regular disparity (inverse depth) and projective depth (parallax from a reference plane).

Mapping from one camera to another

What happens when we take two images of a 3D scene from different camera positions or orientations (Figure 2.12a)? Using the full rank 4 × 4 camera matrix P˜ = K˜ E from (2.64), we can write the projection from world to screen coordinates as

x˜0 ∼ K˜ 0E0p = P˜ 0p.

(2.68)

Assuming that we know the z-buffer or disparity value d0 for a pixel in one image, we can compute the 3D point location p using

p ∼ E−0 1K˜ −0 1x˜0 and then project it into another image yielding

(2.69)

x˜1 ∼ K˜ 1E1p = K˜ 1E1E−0 1K˜ −0 1x˜0 = P˜ 1P˜ −0 1x˜0 = M10x˜0.

(2.70)

Unfortunately, we do not usually have access to the depth coordinates of pixels in a regular photographic image. However, for a planar scene, as discussed above in (2.66), we can replace the last row of P0 in (2.64) with a general plane equation, nˆ0 · p + c0, that maps points on the plane to d0 = 0 values (Figure 2.12b). Thus, if we set d0 = 0, we can ignore the last column of M10 in (2.70) and also its last row, since we do not care about the ﬁnal z-buffer depth. The mapping Equation (2.70) thus reduces to

x˜1 ∼ H˜ 10x˜0,

(2.71)

where H˜ 10 is a general 3 × 3 homography matrix and x˜1 and x˜0 are now 2D homogeneous coordinates (i.e., 3-vectors) (Szeliski 1996). This justiﬁes the use of the 8-parameter homog-

62

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

p = (X,Y,Z,1)

n^0·p+c0= 0

x~0 = (x0,y0,1,d0)

x~1 = (x1,y1,1,d1)

M10
(a)

x~0 = (x0,y0,1) H10
.
(b)

~x1 = (x1,y1,1)

Figure 2.12 A point is projected into two images: (a) relationship between the 3D point coordinate (X, Y, Z, 1) and the 2D projected point (x, y, 1, d); (b) planar homography induced by points all lying on a common plane nˆ0 · p + c0 = 0.

raphy as a general alignment model for mosaics of planar scenes (Mann and Picard 1994; Szeliski 1996).
The other special case where we do not need to know depth to perform inter-camera mapping is when the camera is undergoing pure rotation (Section 8.2.3), i.e., when t0 = t1. In this case, we can write

x˜1 ∼ K1R1R−0 1K−0 1x˜0 = K1R10K−0 1x˜0,

(2.72)

which again can be represented with a 3 × 3 homography. If we assume that the calibration matrices have known aspect ratios and centers of projection (2.59), this homography can be parameterized by the rotation amount and the two unknown focal lengths. This particular formulation is commonly used in image-stitching applications (Section 8.2.3).

Object-centered projection
When working with long focal length lenses, it often becomes difﬁcult to reliably estimate the focal length from image measurements alone. This is because the focal length and the distance to the object are highly correlated and it becomes difﬁcult to tease these two effects apart. For example, the change in scale of an object viewed through a zoom telephoto lens can either be due to a zoom change or to a motion towards the user. (This effect was put to dramatic use in some scenes of Alfred Hitchcock’s ﬁlm Vertigo, where the simultaneous change of zoom and camera motion produces a disquieting effect.)
This ambiguity becomes clearer if we write out the projection equation corresponding to

2.1 Geometric primitives and transformations

63

the simple calibration matrix K (2.59),

xs

=

f

rx rz

· p + tx · p + tz

+ cx

ys

=

f

ry rz

·p ·p

+ ty + tz

+

cy ,

(2.73) (2.74)

where rx, ry, and rz are the three rows of R. If the distance to the object center tz p (the size of the object), the denominator is approximately tz and the overall scale of the projected object depends on the ratio of f to tz. It therefore becomes difﬁcult to disentangle these two quantities.

To see this more clearly, let ηz = t−z 1 and s = ηzf . We can then re-write the above equations as

xs

=

s

rx · p 1 + ηz

+ rz

tx ·p

+

cx

ys

=

s

ry · p 1 + ηz

+ rz

ty ·p

+

cy

(2.75) (2.76)

(Szeliski and Kang 1994; Pighin, Hecker et al. 1998). The scale of the projection s can

be reliably estimated if we are looking at a known object (i.e., the 3D coordinates p are

known). The inverse distance ηz is now mostly decoupled from the estimates of s and can be estimated from the amount of foreshortening as the object rotates. Furthermore, as the

lens becomes longer, i.e., the projection model becomes orthographic, there is no need to

replace a perspective imaging model with an orthographic one, since the same equation can

be used, with ηz → 0 (as opposed to f and tz both going to inﬁnity). This allows us to form a natural link between orthographic reconstruction techniques such as factorization and their

projective/perspective counterparts (Section 11.4.1).

2.1.5 Lens distortions
The above imaging models all assume that cameras obey a linear projection model where straight lines in the world result in straight lines in the image. (This follows as a natural consequence of linear matrix operations being applied to homogeneous coordinates.) Unfortunately, many wide-angle lenses have noticeable radial distortion, which manifests itself as a visible curvature in the projection of straight lines. (See Section 2.2.3 for a more detailed discussion of lens optics, including chromatic aberration.) Unless this distortion is taken into account, it becomes impossible to create highly accurate photorealistic reconstructions. For example, image mosaics constructed without taking radial distortion into account will often exhibit blurring due to the misregistration of corresponding features before pixel blending (Section 8.2).

64

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

Fortunately, compensating for radial distortion is not that difﬁcult in practice. For most lenses, a simple quartic model of distortion can produce good results. Let (xc, yc) be the pixel coordinates obtained after perspective division but before scaling by focal length f and shifting by the image center (cx, cy), i.e.,

xc

=

rx rz

· ·

p p

+ +

tx tz

yc

=

ry rz

· ·

p p

+ +

ty tz

.

(2.77)

The radial distortion model says that coordinates in the observed images are displaced towards (barrel distortion) or away (pincushion distortion) from the image center by an amount proportional to their radial distance (Figure 2.13a–b).6 The simplest radial distortion models use low-order polynomials, e.g.,

xˆc = xc(1 + κ1rc2 + κ2rc4) yˆc = yc(1 + κ1rc2 + κ2rc4),

(2.78)

where rc2 = x2c + yc2 and κ1 and κ2 are called the radial distortion parameters.7 This model, which also includes a tangential component to account for lens decentering, was ﬁrst proposed in the photogrammetry literature by Brown (1966), and so is sometimes called the Brown or Brown–Conrady model. However, the tangential components of the distortion are usually ignored because they can lead to less stable estimates (Zhang 2000).
After the radial distortion step, the ﬁnal pixel coordinates can be computed using

xs = f xˆc + cx ys = f yˆc + cy.

(2.79)

A variety of techniques can be used to estimate the radial distortion parameters for a given lens, as discussed in Section 11.1.4.
Sometimes the above simpliﬁed model does not model the true distortions produced by complex lenses accurately enough (especially at very wide angles). A more complete analytic model also includes tangential distortions and decentering distortions (Slama 1980), but these distortions are not covered in this book.
6Anamorphic lenses, which are widely used in feature ﬁlm production, do not follow this radial distortion model. Instead, they can be thought of, to a ﬁrst approximation, as inducing different vertical and horizontal scaling, i.e., non-square pixels.
7Sometimes the relationship between xc and xˆc is expressed the other way around, i.e., xc = xˆc(1 + κ1rˆc2 + κ2rˆc4). This is convenient if we map image pixels into (warped) rays by dividing through by f . We can then undistort the rays and have true 3D rays in space.

2.1 Geometric primitives and transformations

65

(a)

(b)

(c)

Figure 2.13 Radial lens distortions: (a) barrel, (b) pincushion, and (c) ﬁsheye. The ﬁsheye image spans almost 180° from side-to-side.

Fisheye lenses (Figure 2.13c) require a model that differs from traditional polynomial models of radial distortion. Fisheye lenses behave, to a ﬁrst approximation, as equi-distance projectors of angles away from the optical axis (Xiong and Turkowski 1997),

r = fθ,

(2.80)

which is the same as the polar projection described by Equations (8.55–8.57). Because of the mostly linear mapping between distance from the center (pixels) and viewing angle, such lenses are sometimes called f-theta lenses, which is likely where the popular RICOH THETA 360° camera got its name. Xiong and Turkowski (1997) describe how this model can be extended with the addition of an extra quadratic correction in φ and how the unknown parameters (center of projection, scaling factor s, etc.) can be estimated from a set of overlapping ﬁsheye images using a direct (intensity-based) non-linear minimization algorithm.
For even larger, less regular distortions, a parametric distortion model using splines may be necessary (Goshtasby 1989). If the lens does not have a single center of projection, it may become necessary to model the 3D line (as opposed to direction) corresponding to each pixel separately (Gremban, Thorpe, and Kanade 1988; Champleboux, Lavalle´e et al. 1992a; Grossberg and Nayar 2001; Sturm and Ramalingam 2004; Tardif, Sturm et al. 2009). Some of these techniques are described in more detail in Section 11.1.4, which discusses how to calibrate lens distortions.
There is one subtle issue associated with the simple radial distortion model that is often glossed over. We have introduced a non-linearity between the perspective projection and ﬁnal sensor array projection steps. Therefore, we cannot, in general, post-multiply an arbitrary 3 × 3 matrix K with a rotation to put it into upper-triangular form and absorb this into the global rotation. However, this situation is not as bad as it may at ﬁrst appear. For many applications,

66

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

image plane

light source
n^

sensor plane

surface

optics
Figure 2.14 A simpliﬁed model of photometric image formation. Light is emitted by one or more light sources and is then reﬂected from an object’s surface. A portion of this light is directed towards the camera. This simpliﬁed model ignores multiple reﬂections, which often occur in real-world scenes.
keeping the simpliﬁed diagonal form of (2.59) is still an adequate model. Furthermore, if we correct radial and other distortions to an accuracy where straight lines are preserved, we have essentially converted the sensor back into a linear imager and the previous decomposition still applies.

2.2 Photometric image formation
In modeling the image formation process, we have described how 3D geometric features in the world are projected into 2D features in an image. However, images are not composed of 2D features. Instead, they are made up of discrete color or intensity values. Where do these values come from? How do they relate to the lighting in the environment, surface properties and geometry, camera optics, and sensor properties (Figure 2.14)? In this section, we develop a set of models to describe these interactions and formulate a generative process of image formation. A more detailed treatment of these topics can be found in textbooks on computer graphics and image synthesis (Cohen and Wallace 1993; Sillion and Puech 1994; Watt 1995; Glassner 1995; Weyrich, Lawrence et al. 2009; Hughes, van Dam et al. 2013; Marschner and Shirley 2015).
2.2.1 Lighting
Images cannot exist without light. To produce an image, the scene must be illuminated with one or more light sources. (Certain modalities such as ﬂuorescence microscopy and X-ray

2.2 Photometric image formation

67

tomography do not ﬁt this model, but we do not deal with them in this book.) Light sources can generally be divided into point and area light sources.
A point light source originates at a single location in space (e.g., a small light bulb), potentially at inﬁnity (e.g., the Sun). (Note that for some applications such as modeling soft shadows (penumbras), the Sun may have to be treated as an area light source.) In addition to its location, a point light source has an intensity and a color spectrum, i.e., a distribution over wavelengths L(λ). The intensity of a light source falls off with the square of the distance between the source and the object being lit, because the same light is being spread over a larger (spherical) area. A light source may also have a directional falloff (dependence), but we ignore this in our simpliﬁed model.
Area light sources are more complicated. A simple area light source such as a ﬂuorescent ceiling light ﬁxture with a diffuser can be modeled as a ﬁnite rectangular area emitting light equally in all directions (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995). When the distribution is strongly directional, a four-dimensional lightﬁeld can be used instead (Ashdown 1993).
A more complex light distribution that approximates, say, the incident illumination on an object sitting in an outdoor courtyard, can often be represented using an environment map (Greene 1986) (originally called a reﬂection map (Blinn and Newell 1976)). This representation maps incident light directions vˆ to color values (or wavelengths, λ),

L(vˆ; λ),

(2.81)

and is equivalent to assuming that all light sources are at inﬁnity. Environment maps can be represented as a collection of cubical faces (Greene 1986), as a single longitude–latitude map (Blinn and Newell 1976), or as the image of a reﬂecting sphere (Watt 1995). A convenient way to get a rough model of a real-world environment map is to take an image of a reﬂective mirrored sphere (sometimes accompanied by a darker sphere to capture highlights) and to unwrap this image onto the desired environment map (Debevec 1998). Watt (1995) gives a nice discussion of environment mapping, including the formulas needed to map directions to pixels for the three most commonly used representations.

2.2.2 Reﬂectance and shading
When light hits an object’s surface, it is scattered and reﬂected (Figure 2.15a). Many different models have been developed to describe this interaction. In this section, we ﬁrst describe the most general form, the bidirectional reﬂectance distribution function, and then look at some more specialized models, including the diffuse, specular, and Phong shading models. We also

68

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

n^

n^ ^vr

v^i θi θr d^y

φi φr d^x

(a)

(b)

Figure 2.15 (a) Light scatters when it hits a surface. (b) The bidirectional reﬂectance distribution function (BRDF) f (θi, φi, θr, φr) is parameterized by the angles that the incident, vˆi, and reﬂected, vˆr, light ray directions make with the local surface coordinate frame (dˆx, dˆy, nˆ).

discuss how these models can be used to compute the global illumination corresponding to a scene.

The Bidirectional Reﬂectance Distribution Function (BRDF)

The most general model of light scattering is the bidirectional reﬂectance distribution function (BRDF).8 Relative to some local coordinate frame on the surface, the BRDF is a fourdimensional function that describes how much of each wavelength arriving at an incident direction vˆi is emitted in a reﬂected direction vˆr (Figure 2.15b). The function can be written in terms of the angles of the incident and reﬂected directions relative to the surface frame as

fr(θi, φi, θr, φr; λ).

(2.82)

The BRDF is reciprocal, i.e., because of the physics of light transport, you can interchange the roles of vˆi and vˆr and still get the same answer (this is sometimes called Helmholtz reciprocity).
Most surfaces are isotropic, i.e., there are no preferred directions on the surface as far as light transport is concerned. (The exceptions are anisotropic surfaces such as brushed (scratched) aluminum, where the reﬂectance depends on the light orientation relative to the direction of the scratches.) For an isotropic material, we can simplify the BRDF to

fr(θi, θr, |φr − φi|; λ) or fr(vˆi, vˆr, nˆ; λ),

(2.83)

8Actually, even more general models of light transport exist, including some that model spatial variation along the surface, sub-surface scattering, and atmospheric effects—see Section 13.7.1—(Dorsey, Rushmeier, and Sillion 2007; Weyrich, Lawrence et al. 2009).

2.2 Photometric image formation

69

as the quantities θi, θr, and φr − φi can be computed from the directions vˆi, vˆr, and nˆ. To calculate the amount of light exiting a surface point p in a direction vˆr under a given
lighting condition, we integrate the product of the incoming light Li(vˆi; λ) with the BRDF (some authors call this step a convolution). Taking into account the foreshortening factor cos+ θi, we obtain

Lr(vˆr; λ) = Li(vˆi; λ)fr(vˆi, vˆr, nˆ; λ) cos+ θi dvˆi,

(2.84)

where

cos+ θi = max(0, cos θi).

(2.85)

If the light sources are discrete (a ﬁnite number of point light sources), we can replace the integral with a summation,

Lr(vˆr; λ) = Li(λ)fr(vˆi, vˆr, nˆ; λ) cos+ θi.
i

(2.86)

BRDFs for a given surface can be obtained through physical modeling (Torrance and Sparrow 1967; Cook and Torrance 1982; Glassner 1995), heuristic modeling (Phong 1975; Lafortune, Foo et al. 1997), or through empirical observation (Ward 1992; Westin, Arvo, and Torrance 1992; Dana, van Ginneken et al. 1999; Marschner, Westin et al. 2000; Matusik, Pﬁster et al. 2003; Dorsey, Rushmeier, and Sillion 2007; Weyrich, Lawrence et al. 2009; Shi, Mo et al. 2019).9 Typical BRDFs can often be split into their diffuse and specular components, as described below.

Diffuse reﬂection
The diffuse component (also known as Lambertian or matte reﬂection) scatters light uniformly in all directions and is the phenomenon we most normally associate with shading, e.g., the smooth (non-shiny) variation of intensity with surface normal that is seen when observing a statue (Figure 2.16). Diffuse reﬂection also often imparts a strong body color to the light, as it is caused by selective absorption and re-emission of light inside the object’s material (Shafer 1985; Glassner 1995).
While light is scattered uniformly in all directions, i.e., the BRDF is constant,

fd(vˆi, vˆr, nˆ; λ) = fd(λ),

(2.87)

the amount of light depends on the angle between the incident light direction and the surface normal θi. This is because the surface area exposed to a given amount of light becomes larger
9See http://www1.cs.columbia.edu/CAVE/software/curet for a database of some empirically sampled BRDFs.

70

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

Figure 2.16 This close-up of a statue shows both diffuse (smooth shading) and specular (shiny highlight) reﬂection, as well as darkening in the grooves and creases due to reduced light visibility and interreﬂections. (Photo courtesy of the Caltech Vision Lab, http:// www. vision.caltech.edu/ archive.html.)

at oblique angles, becoming completely self-shadowed as the outgoing surface normal points away from the light (Figure 2.17a). (Think about how you orient yourself towards the Sun or ﬁreplace to get maximum warmth and how a ﬂashlight projected obliquely against a wall is less bright than one pointing directly at it.) The shading equation for diffuse reﬂection can thus be written as

where

Ld(vˆr; λ) = Li(λ)fd(λ) cos+ θi = Li(λ)fd(λ)[vˆi · nˆ]+,

i

i

[vˆi · nˆ]+ = max(0, vˆi · nˆ).

(2.88) (2.89)

Specular reﬂection

The second major component of a typical BRDF is specular (gloss or highlight) reﬂection, which depends strongly on the direction of the outgoing light. Consider light reﬂecting off a mirrored surface (Figure 2.17b). Incident light rays are reﬂected in a direction that is rotated by 180° around the surface normal nˆ. Using the same notation as in Equations (2.29–2.30), we can compute the specular reﬂection direction ˆsi as

ˆsi = v − v⊥ = (2nˆnˆT − I)vi.

(2.90)

2.2 Photometric image formation

71

0 < v^i•n^ < 1 v^i•n^ = 1 v^i•n^ = 0
v^i•n^ < 0 (a)

^si

n^

v^i

v║

v║

-v┴

v┴ 180°

(b)

Figure 2.17 (a) The diminution of returned light caused by foreshortening depends on vˆi · nˆ, the cosine of the angle between the incident light direction vˆi and the surface normal nˆ. (b) Mirror (specular) reﬂection: The incident light ray direction vˆi is reﬂected onto the specular direction ˆsi around the surface normal nˆ.

The amount of light reﬂected in a given direction vˆr thus depends on the angle θs = cos−1(vˆr · ˆsi) between the view direction vˆr and the specular direction ˆsi. For example, the Phong (1975) model uses a power of the cosine of the angle,

fs(θs; λ) = ks(λ) coske θs, while the Torrance and Sparrow (1967) micro-facet model uses a Gaussian,

(2.91)

fs(θs; λ) = ks(λ) exp(−c2sθs2).

(2.92)

Larger exponents ke (or inverse Gaussian widths cs) correspond to more specular surfaces with distinct highlights, while smaller exponents better model materials with softer gloss.

Phong shading

Phong (1975) combined the diffuse and specular components of reﬂection with another term, which he called the ambient illumination. This term accounts for the fact that objects are generally illuminated not only by point light sources but also by a general diffuse illumination corresponding to inter-reﬂection (e.g., the walls in a room) or distant sources, such as the blue sky. In the Phong model, the ambient term does not depend on surface orientation, but depends on the color of both the ambient illumination La(λ) and the object ka(λ),

fa(λ) = ka(λ)La(λ).

(2.93)

72

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

0.5

Ambient

0.4

Diffuse

Exp=10

Exp=100

0.3

Exp=1000

0.5

Ambient

0.4

Diffuse

Exp=10

Exp=100

0.3

Exp=1000

0.2

0.2

0.1

0.1

0.0 -90 -80 -70 -60 -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90
(a)

0.0 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5
(b)

Figure 2.18 Cross-section through a Phong shading model BRDF for a ﬁxed incident illumination direction: (a) component values as a function of angle away from surface normal; (b) polar plot. The value of the Phong exponent ke is indicated by the “Exp” labels and the light source is at an angle of 30° away from the normal.

Putting all of these terms together, we arrive at the Phong shading model,

Lr(vˆr; λ) = ka(λ)La(λ) + kd(λ) Li(λ)[vˆi · nˆ]+ + ks(λ) Li(λ)(vˆr · ˆsi)ke . (2.94)

i

i

Figure 2.18 shows a typical set of Phong shading model components as a function of the

angle away from the surface normal (in a plane containing both the lighting direction and the

viewer).

Typically, the ambient and diffuse reﬂection color distributions ka(λ) and kd(λ) are the same, since they are both due to sub-surface scattering (body reﬂection) inside the surface

material (Shafer 1985). The specular reﬂection distribution ks(λ) is often uniform (white), since it is caused by interface reﬂections that do not change the light color. (The exception

to this is emphmetallic materials, such as copper, as opposed to the more common dielectric

materials, such as plastics.)

The ambient illumination La(λ) often has a different color cast from the direct light sources Li(λ), e.g., it may be blue for a sunny outdoor scene or yellow for an interior lit with candles or incandescent lights. (The presence of ambient sky illumination in shadowed

areas is what often causes shadows to appear bluer than the corresponding lit portions of a

scene). Note also that the diffuse component of the Phong model (or of any shading model)

depends on the angle of the incoming light source vˆi, while the specular component depends on the relative angle between the viewer vr and the specular reﬂection direction ˆsi (which itself depends on the incoming light direction vˆi and the surface normal nˆ).
The Phong shading model has been superseded in terms of physical accuracy by newer

models in computer graphics, including the model developed by Cook and Torrance (1982)

based on the original micro-facet model of Torrance and Sparrow (1967). While, initially,

2.2 Photometric image formation

73

computer graphics hardware implemented the Phong model, the advent of programmable pixel shaders has made the use of more complex models feasible.

Di-chromatic reﬂection model
The Torrance and Sparrow (1967) model of reﬂection also forms the basis of Shafer’s (1985) di-chromatic reﬂection model, which states that the apparent color of a uniform material lit from a single source depends on the sum of two terms,

Lr(vˆr; λ) = Li(vˆr, vˆi, nˆ; λ) + Lb(vˆr, vˆi, nˆ; λ) = ci(λ)mi(vˆr, vˆi, nˆ) + cb(λ)mb(vˆr, vˆi, nˆ),

(2.95) (2.96)

i.e., the radiance of the light reﬂected at the interface, Li, and the radiance reﬂected at the surface body, Lb. Each of these, in turn, is a simple product between a relative power spectrum c(λ), which depends only on wavelength, and a magnitude m(vˆr, vˆi, nˆ), which depends only on geometry. (This model can easily be derived from a generalized version of Phong’s model by assuming a single light source and no ambient illumination, and rearranging terms.) The di-chromatic model has been successfully used in computer vision to segment specular colored objects with large variations in shading (Klinker 1993) and has inspired local twocolor models for applications such as Bayer pattern demosaicing (Bennett, Uyttendaele et al. 2006).

Global illumination (ray tracing and radiosity)
The simple shading model presented thus far assumes that light rays leave the light sources, bounce off surfaces visible to the camera, thereby changing in intensity or color, and arrive at the camera. In reality, light sources can be shadowed by occluders and rays can bounce multiple times around a scene while making their trip from a light source to the camera.
Two methods have traditionally been used to model such effects. If the scene is mostly specular (the classic example being scenes made of glass objects and mirrored or highly polished balls), the preferred approach is ray tracing or path tracing (Glassner 1995; AkenineMo¨ller and Haines 2002; Marschner and Shirley 2015), which follows individual rays from the camera across multiple bounces towards the light sources (or vice versa). If the scene is composed mostly of uniform albedo simple geometry illuminators and surfaces, radiosity (global illumination) techniques are preferred (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995). Combinations of the two techniques have also been developed (Wallace, Cohen, and Greenberg 1987), as well as more general light transport techniques for simulating effects such as the caustics cast by rippling water.

74

Computer Vision: Algorithms and Applications (July 2, 2021 draft)

The basic ray tracing algorithm associates a light ray with each pixel in the camera image and ﬁnds its intersection with the nearest surface. A primary contribution can then be computed using the simple shading equations presented previously (e.g., Equation (2.94)) for all light sources that are visible for that surface element. (An alternative technique for computing which surfaces are illuminated by a light source is to compute a shadow map, or shadow buffer, i.e., a rendering of the scene from the light source’s perspective, and then compare the depth of pixels being rendered with the map (Williams 1983; Akenine-Mo¨ller and Haines 2002).) Additional secondary rays can then be cast along the specular direction towards other objects in the scene, keeping track of any attenuation or color change that the specular reﬂection induces.
Radiosity works by associating lightness values with rectangular surface areas in the scene (including area light sources). The amount of light interchanged between any two (mutually visible) areas in the scene can be captured as a form factor, which depends on their relative orientation and surface reﬂectance properties, as well as the 1/r2 fall-off as light is distributed over a larger effective sphere the further away it is (Cohen and Wallace 1993; Sillion and Puech 1994; Glassner 1995). A large linear system can then be set up to solve for the ﬁnal lightness of each area patch, using the light sources as the forcing function (right-hand side). Once the system has been solved, the scene can be rendered from any desired point of view. Under certain circumstances, it is possible to recover the global illumination in a scene from photographs using computer vision techniques (Yu, Debevec et al. 1999).
The basic radiosity algorithm does not take into account certain near ﬁeld effects, such as the darkening inside corners and scratches, or the limited ambient illumination caused by partial shadowing from other surfaces. Such effects have been exploited in a number of computer vision algorithms (Nayar, Ikeuchi, and Kanade 1991; Langer and Zucker 1994).
While all of these global illumination effects can have a strong effect on the appearance of a scene, and hence its 3D interpretation, they are not covered in more detail in this book. (But see Section 13.7.1 for a discussion of recovering BRDFs from real scenes and objects.)

2.2.3 Optics
Once the light from a scene reaches the camera, it must still pass through the lens before reaching the sensor (analog ﬁlm or digital silicon). For many applications, it sufﬁces to treat the lens as an ideal pinhole that simply projects all rays through a common center of projection (Figures 2.8 and 2.9).
However, if we want to deal with issues such as focus, exposure, vignetting, and aberration, we need to develop a more sophisticated model, which is where the study of optics comes in (Mo¨ller 1988; Ray 2002; Hecht 2015).

