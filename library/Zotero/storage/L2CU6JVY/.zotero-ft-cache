Sold to
stieizc.33@gmail.com
A Programmer’s Introduction to Mathematics
Jeremy Kun

Copyright © 2020 Jeremy Kun
All rights reserved. This book or any portion thereof may not be reproduced or used in any manner whatsoever without the express written permission of the publisher except for the use of brief quotations in a book review.
All images used in this book are either the author’s original works or in the public domain. In particular, the only non-original images are in the chapter on group theory, specifically the textures from Owen Jones’s design masterpiece, The Grammar of the Ornament (1856), M.C. Escher’s Circle Limit IV (1960), and two diagrams in the public domain, sourced from Wikipedia.
Second edition, 2020.
pimbook.org

To my wife, Erin.

My unbounded, uncountable thanks goes out to the many people who read drafts at various stages of roughness and gave feedback, including (in alphabetical order by first name), Aaron Shifman, Adam Lelkes, Alex Walchli, Ali Fathalian, Arun Koshy, Ben Fish, Craig Stuntz, Devin Ivy, Erin Kelly, Fred Ross, Ian Sharkey, Jasper Slusallek, Jean-Gabriel Young, João Rico, John Granata, Julian Leonardo Cuevas Rozo, Kevin Finn, Landon Kavlie, Louis Maddox, Matthijs Hollemans, Olivia Simpson, Pablo González de Aledo, Paige Bailey, Patrick Regan, Patrick Stein, Rodrigo Zhou, Stephanie Labasan, Temple Keller, Trent McCormick.
An extra thanks to the readers who submitted errata at pimbook.org for the first edition, including Abhinav Upadhyay, Abhishek Bhatia, Alejandro Baldominos, Andrei Paleyes, Arman Yessenamanov, Arthur Allshire, Arunoda Susiripala, Bilal Karim Reffas, Brian Cloutier, Brian van den Broek, Britton Winterrose, Cedric Bucerius, Changyoung Koh, Charlie Mead, Chris G, Chrislain Razafimahefa, Darin Brezeale, David Bimmler, David Furcy, David Shockley, David Wu, Devin Conathan, Don-Duong Quach, Fidel Barrera-Cruz, Francis Huynh, Glen De Cauwsemaecker, Harry Altman, Ivan Katanic, Jaime, Jan Moren, Jason Hooper, K. Alex Mills, Kenytt Avery, Konstantin Weitz, Leandro Motta Barros, Luke A., Marco Craveiro, Matthijs, Maximilian Schlund, Meji Abidoye, Michael Cohen, Michaël Defferrard, Nicolas Krause, Nikita V., Oliver Sampson, Ondrej Slamecka, Patrick Stingley, Rich Yonts, Rodrigo Ariel Sota, Ryan Troxler, Seth Yastrov, Simon Skrede, Sriram Srinivasan, Steve Dwyer, Steven D. Brown, Tim Wilkens, Timo Vesalainen, Tyler Smith, Wojciech Kryscinski, and Zorro.
Special thanks to John Peloquin for his thorough technical review for the second edition, and to Devin Ivy for technical review of parts of the first edition.

Contents

Our Goal

i

Chapter 1. Like Programming, Mathematics has a Culture

1

Chapter 2. Polynomials

5

2.1 Polynomials, Java, and Definitions . . . . . . . . . . . . . . . . . . . . . . 5

2.2 A Little More Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

2.3 Existence & Uniqueness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

2.4 Realizing it in Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

2.5 Application: Sharing Secrets . . . . . . . . . . . . . . . . . . . . . . . . . 24

2.6 Cultural Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

2.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

2.8 Chapter Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

Chapter 3. On Pace and Patience

35

Chapter 4. Sets

39

4.1 Sets, Functions, and Their -Jections . . . . . . . . . . . . . . . . . . . . . 40

4.2 Clever Bijections and Counting . . . . . . . . . . . . . . . . . . . . . . . 48

4.3 Proof by Induction and Contradiction . . . . . . . . . . . . . . . . . . . . 51

4.4 Application: Stable Marriages . . . . . . . . . . . . . . . . . . . . . . . . 54

4.5 Cultural Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

4.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

4.7 Chapter Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

Chapter 5. Variable Names, Overloading, and Your Brain

63

Chapter 6. Graphs

69

6.1 The Definition of a Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

6.2 Graph Coloring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

6.3 Register Allocation and Hardness . . . . . . . . . . . . . . . . . . . . . . 73

6.4 Planarity and the Euler Characteristic . . . . . . . . . . . . . . . . . . . . 75

6.5 Application: the Five Color Theorem . . . . . . . . . . . . . . . . . . . . 78

6.6 Approximate Coloring . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 6.7 Cultural Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 6.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 6.9 Chapter Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

Chapter 7. The Many Subcultures of Mathematics

89

Chapter 8. Calculus with One Variable

95

8.1 Lines and Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

8.2 Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100

8.3 The Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107

8.4 Taylor Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

8.5 Remainders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117

8.6 Application: Finding Roots . . . . . . . . . . . . . . . . . . . . . . . . . . 119

8.7 Cultural Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125

8.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125

Chapter 9. On Types and Tail Calls

129

Chapter 10. Linear Algebra

135

10.1 Linear Maps and Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . 136

10.2 Linear Maps, Formally This Time . . . . . . . . . . . . . . . . . . . . . . 141

10.3 The Basis and Linear Combinations . . . . . . . . . . . . . . . . . . . . . 143

10.4 Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

10.5 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149

10.6 Conjugations and Computations . . . . . . . . . . . . . . . . . . . . . . . 155

10.7 One Vector Space to Rule Them All . . . . . . . . . . . . . . . . . . . . . 158

10.8 Geometry of Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . 159

10.9 Application: Singular Value Decomposition . . . . . . . . . . . . . . . . . 164

10.10 Cultural Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179

10.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179

10.12 Chapter Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182

Chapter 11. Live and Learn Linear Algebra (Again)

185

Chapter 12. Eigenvectors and Eigenvalues

191

12.1 Eigenvalues of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193

12.2 Limiting the Scope: Symmetric Matrices . . . . . . . . . . . . . . . . . . 195

12.3 Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198

12.4 Orthonormal Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202

12.5 Computing Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205

12.6 The Spectral Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207

12.7 Application: Waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209

12.8 Cultural Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225

12.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 12.10 Chapter Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229

Chapter 13. Rigor and Formality

233

Chapter 14. Multivariable Calculus and Optimization

239

14.1 Generalizing the Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . 239

14.2 Linear Approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241

14.3 Vector-valued Functions and the Chain Rule . . . . . . . . . . . . . . . . 246

14.4 Computing the Total Derivative . . . . . . . . . . . . . . . . . . . . . . . 248

14.5 The Geometry of the Gradient . . . . . . . . . . . . . . . . . . . . . . . . 251

14.6 Optimizing Multivariable Functions . . . . . . . . . . . . . . . . . . . . . 253

14.7 Gradient Descent: an Optimization Hammer . . . . . . . . . . . . . . . . 261

14.8 Gradients of Computation Graphs . . . . . . . . . . . . . . . . . . . . . . 262

14.9 Application: Automatic Differentiation and a Simple Neural Network . . 265

14.10 Cultural Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281

14.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281

14.12 Chapter Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284

Chapter 15. The Argument for Big-O Notation

291

Chapter 16. Groups

301

16.1 The Geometric Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . 303

16.2 The Interface Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . 307

16.3 Homomorphisms: Structure Preserving Functions . . . . . . . . . . . . . 309

16.4 Building Blocks of Groups . . . . . . . . . . . . . . . . . . . . . . . . . . 312

16.5 Geometry as the Study of Groups . . . . . . . . . . . . . . . . . . . . . . 314

16.6 The Symmetry Group of the Poincaré Disk . . . . . . . . . . . . . . . . . 324

16.7 Application: Drawing Hyperbolic Tessellations . . . . . . . . . . . . . . . 329

16.8 Cultural Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345

16.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345

16.10 Chapter Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350

Chapter 17. A New Interface

353

Appendix A. Notation

363

Appendix B. A Summary of Proofs

365

B.1 Propositional and first-order logic . . . . . . . . . . . . . . . . . . . . . . 365

B.2 Methods of proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367

B.3 How does one actually prove things? . . . . . . . . . . . . . . . . . . . . 368

Appendix C. Annotated Resources

373

C.1 Fundamentals and Foundations . . . . . . . . . . . . . . . . . . . . . . . 373

C.2 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374 C.3 Graph Theory and Combinatorics . . . . . . . . . . . . . . . . . . . . . . 375 C.4 Calculus and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375 C.5 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376 C.6 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377 C.7 Abstract Algebra (Groups, etc.) . . . . . . . . . . . . . . . . . . . . . . . . 377 C.8 Topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378 C.9 Computer Science, Theory, and Algorithms . . . . . . . . . . . . . . . . . 378 C.10 Fun and Recreation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380

About the Author and Cover

381

Index

383

Our Goal
This book has a straightforward goal: to teach you how to engage with mathematics. Let’s unpack this. By “mathematics,” I mean the universe of books, papers, talks, and
blog posts that contain the meat of mathematics: formal definitions, theorems, proofs, conjectures, and algorithms. By “engage” I mean that for any mathematical topic, you have the cognitive tools to make progress toward understanding that topic. I will “teach” you by introducing you to—or having you revisit—a broad foundation of topics and techniques that support the rest of mathematics. I say “with” because mathematics requires active participation.
We will define and study many basic objects of mathematics, such as polynomials, graphs, and matrices. More importantly, I’ll explain how to think about those objects as seasoned mathematicians do. We will examine the hierarchies of mathematical abstraction, along with many of the softer skills and insights that constitute “mathematical intuition.” Along the way we’ll hear the voices of mathematicians—both famous historical figures and my friends and colleagues—to paint a picture of mathematics as both a messy amalgam of competing ideas and preferences, and a story with delightfully surprising twists and connections. In the end, I will show you how mathematicians think about mathematics.
So why would someone like you1 want to engage with mathematics? Many software engineers, especially the sort who like to push the limits of what can be done with programs, eventually realize a deep truth: mathematics unlocks a lot of cool new programs. These are truly novel programs. They would simply be impossible to write (if not inconceivable!) without mathematics. That includes programs in this book about cryptography, data science, and art, but also to many revolutionary technologies in industry, such as signal processing, compression, ranking, optimization, and artificial intelligence. As importantly, a wealth of opportunity makes programming more fun! To quote Randall Munroe in his XKCD comic Forgot Algebra, “The only things you HAVE to know are how to make enough of a living to stay alive and how to get your taxes done. All the fun parts of life are optional.” If you want your career to grow beyond shuffling data around to meet arbitrary business goals, you should learn the tools that enable you to write programs that captivate and delight you. Mathematics is one of those tools.
Programmers are in a privileged position to engage with mathematics. Your comfort
1 Hopefully you’re a programmer; otherwise, the title of this book must have surely caused a panic attack.
i

ii
with functions, logic, and protocols gives you an intuitive familiarity with basic topics such as boolean algebra, recursion, and abstraction. You can rely on this to make mathematics less foreign, progressing all the faster to more nuanced and stimulating topics. By contrast, most educational math content is for students with no background. Such content focuses on rote exercises and passing tests. This book will omit most of that. Programming also allows me to provide immediate applications that ground the abstract ideas in code. In each chapter, we’ll fashion our mathematical designs into a program you couldn’t have written before. All programs are written in Python 3. The code is available on Github,2 with a directory for each chapter.
All told, this book is not a textbook. I won’t drill you with exercises, though drills have their place. We won’t build up any particular field of mathematics from scratch. Though we’ll visit calculus, linear algebra, and many other topics, this book is far too short to cover everything a mathematician ought to know about these topics. Moreover, while much of the book is appropriately rigorous, I will occasionally and judiciously loosen rigor when it facilitates a better understanding and relieves tedium. I will note when this occurs, and we’ll discuss the role of rigor in mathematics more broadly.
Indeed, rather than read an encyclopedic reference, you want to become comfortable with the process of learning mathematics. In part that means becoming comfortable with discomfort, with the struggle of understanding a new concept, and the techniques that mathematicians use to remain productive and sane. Many people find calculus difficult, or squeaked by a linear algebra course without grokking it. After this book you should have a core nugget of understanding of these subjects, along with the cognitive tools that will enable you dive as deeply as you like.
As a necessary consequence, in this book you’ll learn how to read and write proofs. The simplest and broadest truth about mathematics is that it revolves around proofs. Proofs are both the primary vehicle of insight and the fundamental measure of judgment. They are the law, the currency, and the fine art of mathematics. Most of what makes mathematics mysterious and opaque—the rigorous definitions, the notation, the overloading of terminology, the mountains of theory, and the unspoken obligations on the reader—is due to the centrality of proofs. A dominant obstacle to learning math is an unfamiliarity with this culture. In this book I’ll cover the basic methods of proof, and each chapter will use proofs to build the subject matter. To be sure, you don’t have to understand every proof to finish this book, and you will probably be confounded by a few. Embrace your humility. Each proof contains layers of insight that are genuinely worthwhile, but few gain a complete picture of a topic in a single sitting. As you grow into mathematics, the act of reading even previously understood proofs provides both renewed and increased wisdom. So long as you identify the value gained by your struggle, your time is well spent.
I’ll also teach you how to read between the mathematical lines of a text, and understand the implicit directions and cultural cues that litter textbooks and papers. As we proceed through the chapters, we’ll gradually become more terse, and you’ll have many opportu-
2 pimbook.org

iii
nities to practice parsing, interpreting, and understanding math. All of the topics in this book are explained by hundreds of other sources, and each chapter’s exercises include explorations of concepts beyond these pages. Finally, I’ll discuss how mathematicians approach problems, and how their process influences the culture of math.
You will not learn everything you want to know in this book, nor will you learn everything this book has to offer in one sitting. Those already familiar with math may find early chapters offensively slow and detailed. Those genuinely new to math may find the later chapters offensively fast. This is by design. I want you to be exposed to as much mathematics as possible. Learn the main definitions. See new notations, conventions, and attitudes. Take the opportunity to explore topics that pique your interest.
A number of readers have reached out to me to describe their struggles with proofs. They found it helpful to read a companion text on the side with extra guidance on sets, functions, and methods of proof—particularly for the additional exercises and consistently gradual pace. In this second edition, I added two appendices that may help with readers struggling with the pace. Appendix B contains more detail about the formalities underlying proofs, along with strategies for problem solving. Appendix C contains a list of books, and specifically a section for books on “Fundamentals and Foundations” that cover the basics of set theory, proofs, and problem solving strategies.
A number of topics are conspicuously missing from this book, my negligence of which approaches criminal. Except for a few informal cameos, we ignore complex numbers, probability and statistics, differential equations, and formal logic. In my humble opinion, none of these topics is as fundamental for mathematical computer science as those I’ve chosen to cover. After becoming comfortable with the topics in this book, for example, probability will be very accessible. Chapter 12 on eigenvalues includes a miniature introduction to differential equations. The notes for Chapter 16 on groups briefly summarizes complex numbers. Probability underlies our discussion of random graphs in Chapter 6 and machine learning in Chapter 14. Moreover, many topics in this book are prerequisites for these other areas. And, of course, as a single human self-publishing this book on nights and weekends, I have only so much time.
The first step on our journey is to confirm that mathematics has a culture worth becoming acquainted with. We’ll do this with a comparative tour of the culture of software that we understand so well.

Chapter 1
Like Programming, Mathematics has a Culture
Mathematics knows no races or geographic boundaries; for mathematics, the cultural world is one country.
–David Hilbert
Do you remember when you started to really learn programming? I do. I spent two years in high school programming games in Java. Those two years easily contain the worst and most embarrassing code I have ever written. My code absolutely reeked. Hundred-line functions and thousand-line classes, magic numbers, unreachable blocks of code, ridiculous comments, a complete disregard for sensible object orientation, and type-coercion that would make your skin crawl. The code worked, but it was filled with bugs and mishandled edge-cases. I broke every rule, but for all my shortcomings I considered myself a hot-shot (at least, among my classmates!). I didn’t know how to design programs, or what made a program “good,” other than that it ran and I could impress my friends with a zombie shooting game.
Even after I started studying software in college, it was another year before I knew what a stack frame or a register was, another year before I was halfway competent with a terminal, another year before I appreciated functional programming, and to this day I still have an irrational fear of systems programming and networking. I built up a base of knowledge over time, with fits and starts at every step.
In a college class on C++ I was programming a Checkers game, and my task was to generate a list of legal jump-moves from a given board state. I used a depth-first search and a few recursive function calls. Once I had something I was pleased with, I compiled it and ran it on my first non-trivial example. Despite following test-driven development, I saw those dreaded words: Segmentation fault. Dozens of test cases and more than twenty hours of confusion later, I found the error: my recursive call passed a reference when it should have been passing a pointer. This wasn’t a bug in syntax or semantics—I understood pointers and references well enough—but a design error. As most programmers can relate, the most aggravating part was that changing four characters (swapping a few ampersands with asterisks) fixed it. Twenty hours of work for four characters! Once I begrudgingly verified it worked, I promptly took the rest of the day off to play Starcraft.
1

2
Such drama is the seasoning that makes a strong programmer. One must study the topics incrementally, learn from a menagerie of mistakes, and spend hours in a befuddled stupor before becoming “experienced.” This gives rise to all sorts of programmer culture, Unix jokes, urban legends, horror stories, and reverence for the masters of C that make the programming community so lovely. It’s like a secret club where you know all the handshakes, but should you forget one, a crafty use of grep and sed will suffice. The struggle makes you appreciate the power of debugging tools, slick frameworks, historically enshrined hacks, and new language features that stop you from shooting your own foot.
When programmers turn to mathematics, they seem to forget these trials. The same people who invested years grokking the tools of their trade treat new mathematical tools and paradigms with surprising impatience. I can see a few reasons why. For one, we were forced to take math classes for many year in school. That forced investment shouldn’t have been pointless. But the culture of mathematics and the culture of mathematics education—elementary through lower-level college courses—are completely different.
Even college math majors have to reconcile this. I’ve had many conversations with such students, including friends, colleagues, and even family, who by their third year decided they didn’t really enjoy math. The story often goes like this: a student who was good at math in high school reaches the point of a math major at which they must read and write proofs in earnest. It requires an ambiguous, open-ended exploration that they don’t enjoy. Despite being a stark departure from the rigid structure of high school math, incoming students are not warned in advance. After coming to terms with their unfortunate situation, they decide that their best option is to persist until they graduate, at which point they return to the comfortable setting of pre-collegiate math, this time in the teacher’s chair.
I don’t mean to insult teaching as a profession—I love teaching and understand why one would choose to do it full time. There are many excellent teachers who excel at both the math and the trickier task of engaging aloof teenagers to think critically about it. But this pattern of disenchantment among math teachers is prevalent, and it widens the conceptual gap between secondary and “college level” mathematics. Programmers often have similar feelings. The subject they once were good at is suddenly impenetrable. It’s a negative feedback loop in the education system. Math takes the blame.
Another reason programmers feel impatient is because they do so many things that relate to mathematics in deep ways. They use graph theory for data structures and search. They study enough calculus to make video games. They hear about the Curry-Howard correspondence between proofs and programs. They hear that Haskell is based on a complicated math thing called category theory. They even use mathematical results in an interesting way. I worked at a “blockchain” company that implemented a Bitcoin wallet, which is based on elliptic curve cryptography. The wallet worked, but the implementer didn’t understand why. They simply adapted pseudocode found on the internet. At the risk of a dubious analogy, it’s akin to a “script kiddie” who uses hacking tools as black boxes, but has little idea how they work. Mathematicians are on the other end of the spectrum. Why things work takes priority over practical implementation.

3
There’s nothing inherently wrong with using mathematics as a black box, especially the sort of applied mathematics that comes with provable guarantees. But many programmers want to dive deeper. This isn’t surprising, given how much time engineers spend studying source code and the internals of brittle, technical systems. Systems that programmers rely on, such as dependency management, load balancers, search engines, alerting systems, and machine learning, all have rich mathematical foundations. We’re naturally curious.
A second obstacle is that math writers are too terse. The purest fields of mathematics take a sort of pretentious pride in how abstract and compact their work is. I can think of a handful of famous books, for which my friends spent weeks or months on a single chapter! This acts as a barrier to entry, especially since minute details matter for applications.
Yet another hindrance is that mathematics has no centralized documentation. Instead it has a collection of books, papers, journals, and conferences, each with subtle differences, citing each other in a haphazard manner. Dealing with this is not easy. One often needs to translate between two different notations or jargons. Students of mathematics solve these problems with knowledgeable teachers. Working mathematicians “just do it.” They reconcile the differences themselves with coffee and contemplation.
What programmers consider “sloppy” notation is one symptom of the problem, but there there are other expectations on the reader that, for better or worse, decelerate the pace of reading. Unfortunately I have no solution here. Part of the power and expressiveness of mathematics is the ability for its practitioners to overload, redefine, and omit in a suggestive manner. Mathematicians also have thousands of years of “legacy” math that require backward compatibility. Enforcing a single specification for all of mathematics—a suggestion I frequently hear from software engineers—would be horrendously counterproductive.
Ideas we take for granted today, such as algebraic notation, drawing functions in the Euclidean plane, and summation notation, were at one point actively developed technologies. Each of these notations had a revolutionary effect on science, and also, to quote Bret Victor, on our capacity to “think new thoughts.” One can draw a line from the proliferation of algebraic notation to the invention of the computer.1 Borrowing software terminology, algebraic notation is among the most influential and scalable technologies humanity has ever invented. And as we’ll see in Chapter 10 and Chapter 16, we can find algebraic structure hiding in exciting places. Algebraic notation helps us understand this structure not only because we can compute, but also because we can visually see the symmetries in the formulas. This makes it easier for us to identify, analyze, and encapsulate structure when it occurs.
1 Leibniz, one of the inventors of calculus, dreamed of a machine that could automatically solve mathematical problems. Ada Lovelace (up to some irrelevant debate) designed the first program for computing Bernoulli numbers, which arise in algebraic formulas for sums of powers of integers. In the early 1900’s Hilbert posed his Tenth Problem on algorithms for solving Diophantine equations, and later his Entscheidungsproblem, which was solved concurrently by Church and Turing and directly led to Turing’s code-breaking computer.

4
Finally, the best mathematicians study concepts that connect decades of material, while simultaneously inventing new concepts which have no existing words to describe them. Without flexible expression, such work would be impossible. It reduces cognitive load, a theme that will follow us throughout the book. Unfortunately, it only does so for the readers who have already absorbed the basic concepts of discussion. By contrast, good software practice assumes a lower bar. Code is encouraged to be simple enough for new grads to understand, and heavily commented otherwise. Surprising behavior is considered harmful. As such, the uninitiated programmer often has a much larger cognitive load when reading math than when reading a program.
There are good reasons why mathematics is the way it is, though the reasons may not always be clear. I like to summarize the contrast by claiming that mathematical notation is closer to spoken language than to code. There is a historical and cultural context missing from many criticisms of math. It’s a legacy system, yes, but a well-designed one. We should understand it, learn from its advantages, and discard the obsolete parts. Those obsolete parts are present, but rarer than they seem.
To fairly evaluate mathematics, we must first learn some mathematics. Only then can we compare and contrast programming and mathematics in terms of their driving questions, their values, their methods, their measures of success, and their cultural expectations. Programming, at its core, focuses on how to instruct a computer to perform some task. But the broader driving questions include how to design a flexible system, how to efficiently store and retrieve data, how to design systems that can handle various modes of failure, how to scale, and how to tame growth and complexity.
Contrast this with mathematics which, at its core, focuses on how to describe a mathematical object and how to prove theorems about its behavior. The broader driving questions include how to design a unified framework for related patterns, how to find computationally useful representations of an object, how to find interesting patterns to study, and most importantly, how to think more clearly about mathematical subjects.
A large chunk of this book expands on this summary through interludes between each chapter and digressions after introducing technical concepts. The rest covers the fundamental objects and methods of a typical mathematical education. So let’s begin our journey into the mathematical mists with an open mind.
Read on, and welcome to the club.

Chapter 2
Polynomials
We are not trying to meet some abstract production quota of definitions, theorems and proofs. The measure of our success is whether what we do enables people to understand and think more clearly and effectively about mathematics.
–William Thurston
We begin with polynomials. In studying polynomials, we’ll discuss mathematical definitions, work carefully through two nontrivial proofs, and implement a system for “sharing secrets” using something called polynomial interpolation. To whet your appetite, this secret sharing scheme allows one to encode a secret message in 10 parts so that any 6 can be used to reconstruct the secret, but with fewer than 6 pieces it’s impossible to determine even a single bit of the original message. The numbers 10 and 6 are just examples, and the scheme we’ll present works for any pair of integers.
2.1 Polynomials, Java, and Definitions
We start with the definition of a polynomial. The problem, if you’re the sort of person who struggled with math, is that reading the definition as a formula will make your eyes glaze over. In this chapter we’re going to overcome this.
The reason I’m so confident is that I’m certain you’ve overcome the same obstacle in the context of programming. For example, my first programming language was Java. And my first program, which I didn’t write but rather copied verbatim, was likely similar to this monstrosity.
5

6
/****************************************** * Compilation: javac HelloWorld.java * Execution: java HelloWorld * * Prints "Hello, World". ******************************************/ public class HelloWorld {
public static void main(String[] args) { // Prints "Hello, World" to stdout on the terminal. System.out.println("Hello, World");
} }
It was roughly six months before I understood what all the different pieces of this program did, despite the fact that I had written ‘public static void main’ so many times I had committed it to memory. Computers don’t generally require you to understand a code snippet to run. But at some point, we all stopped to ask, “what do those words actually mean?” That’s the step when my eyes stop glazing over. That’s the same procedure we need to invoke for a mathematical definition, preferably faster than six months.
Now I’m going to throw you in the thick of the definition of a polynomial. But stay with me! I want you to start by taking out a piece of paper and literally copying down the definition (the entire next paragraph), character for character, as one would type out a program from scratch. This is not an idle exercise. Taking notes by hand uses a part of your brain that both helps you remember what you wrote, and helps you read it closely. Each individual word and symbol of a mathematical definition affects the concept being defined, so it’s important to parse everything slowly.
Definition 2.1. A single variable polynomial with real coefficients is a function f that takes a real number as input, produces a real number as output, and has the form
f (x) = a0 + a1x + a2x2 + · · · + anxn,
where the ai are real numbers. The ai are called coefficients of f . The degree of the polynomial is the integer n.
Let’s analyze the content of this definition in three ways. First, syntactically, which also highlights some general features of written definitions. Second, semantically, where we’ll discuss what a polynomial should represent as a concept in your mind. Third, we’ll inspect this definition culturally, which includes the unspoken expectations of the reader upon encountering a definition in the wild. As we go, we’ll clarify some nuance to the definition related to certain “edge cases.”
Syntax
A definition is an English sentence or paragraph in which italicized words refer to the concepts being defined. In this case, Definition 2.1 defines three things: a polynomial with real coefficients (the function f ), coefficients (the numbers ai), and a polynomial’s degree (the integer n).

7
A proper mathematical treatment might also define what a “real number” is, but we simply don’t have the time or space.1 For now, think of a real number as a floating point number without the emotional baggage that comes from trying to fit all decimals into a finite number of bits.
An array of numbers a, which in most programming languages would be indexed using square brackets like a[i], is almost always indexed in math using subscripts ai. For twodimensional arrays, we comma-separate the indices in the subscript, i.e. ai,j is equivalent to a[i][j]. Hence, the coefficients are an array of real numbers. Many mathematicians index arrays from 1 instead of 0, and we will do both in this book.
We used a strange phrase in Definition 2.1, that “f has the form” of some expression. This means that we’re choosing specific values for the data defining f . It’s making a particular instance of the definition, as if it were a class definition in a program. In this case the choices are:
1. The names for all the variables involved. The definition has chosen f for the function, x for the input variable name, a for the array of coefficients, and n for the degree. One can choose other names as desired.
2. A value for the degree.
3. A value for the array of coefficients a0, a1, a2, . . . , an, where n must match the chosen degree.
Specifying all of these results in a concrete polynomial.
Semantics
Let’s start with a simple example polynomial, where I pick g for the function name, t for the input name, b for the coefficients, and define n = 3, and b0, b1, b2, b3 = 2, 0, 4, −1. By definition, g has the form
g(t) = 2 + 0t + 4t2 + (−1)t3.
We take some liberties and usually write g more briefly as g(t) = 2 + 4t2 − t3. As you might expect, g is a function you can evaluate, and evaluating it at an input t = 2 means substituting 2 for t and doing the requisite arithmetic to get
g(2) = 2 + 4(22) − 23 = 10.
According to the definition, a polynomial is a function that is written in a certain form. Really what’s being said is that a polynomial is any function of a single input that can be written in the required form, even if you might write it a different way sometimes. This
1 If you’re truly interested in how real numbers are defined from scratch, Spivak’s text Calculus devotes Chapter 29 to a gold-standard treatment. You might be ready for it after working through a few chapters of this book, but be warned: Spivak starts Chapter 29 with, “The mass of drudgery which this chapter necessarily contains…”

8
makes our internal concept of a polynomial more general than the letter of Definition 2.1. A polynomial is any function of a single numeric input that can be expressed using only addition and multiplication and constants, along with the input variable itself. So the following is a polynomial:
g(t) = (t − 1)(t + 6)2
You recover the precise form of Definition 2.1 by algebraically simplifying and grouping terms. The form described in Definition 2.1 is not ideal for every occasion! For example, if you want to evaluate a polynomial quickly on a computer, you might represent the polynomial so that evaluating it doesn’t redundantly compute the powers t1, t2, t3, . . . , tn. One such scheme is called Horner’s method, which we’ll return to in an Exercise. The form in Definition 2.1 might be called a “canonical” or “standard” form, and it’s often useful for manipulation in proofs. As we’ll see later in this chapter, it’s easy to express a generic sum or difference of two polynomials in the standard form.
Suffice it to say, there are many representations of the same abstract polynomial. You can do arithmetic and renaming to get to a standard representation. f (x) = x + 1 is the same polynomial as g(t) = 1 + t, though they differ syntactically.
There are other ways to think about polynomials, and we’ll return to polynomials in future chapters with new and deeper ideas about them. Here are some previews of that. The first is that a polynomial, as with any function, can be represented as a set of pairs called points. That is, if you take each input t and pair it with its output f (t), you get a set of tuples (t, f (t)), which can be analyzed from the perspective of set theory. We will return to this perspective in Chapter 4.
Second, a polynomial’s graph can be plotted as a curve in space, so that the horizontal direction represents the input and the vertical represents the output. Figure 2.1 shows a plot of one part of the curve given by the polynomial f (x) = x5 − x − 1.
Using the curves they “carve out” in space, polynomials can be regarded as geometric objects with geometric properties like “curvature” and “smoothness.” In Chapter 8 we’ll return to this more formally, but until then one can guess how they might faithfully describe a plot like the one in Figure 2.1. The connection between polynomials as geometric objects and their algebraic properties is a deep one that has occupied mathematicians for centuries. For example, the degree gives some information about the shape of the curve. Figure 2.2 shows plots of generic polynomials of degrees 3 through 6. As the degree goes up, so does the number of times the polynomial “changes direction” between increasing and decreasing. Making this mathematically rigorous requires more nuance—after all, the degree five polynomial in Figure 2.1 only changes direction twice—but the pattern suggested by Figure 2.2 is no coincidence.
Finally, polynomials can be thought of as “building blocks” for complicated structures. That is, polynomials are a family of increasingly expressive objects, which get more complex as the degree increases. This idea is the foundation of the application for this chapter (sharing secrets), and it will guide us to use Taylor polynomials to approximate things in Chapters 8 and 14.

9
Figure 2.1: A polynomial as a curve in the plane. Polynomials occur with stunning ubiquity across mathematics. It makes one wonder exactly why they are so central. It’s because polynomials encapsulate the full expressivity of addition and multiplication. As programmers, we know that even such simple operations as binary AND, OR, and NOT, when combined arbitrarily, allow us to build circuits that make a computer. Those three operations yield the full gamut of algorithms. Polynomials fill a similar role for arithmetic. Indeed, polynomials with multiple variables can represent AND, OR, and NOT, if you restrict the values of the variables to be zero and one (interpreted as false and true, respectively).
AND(x, y) = xy NOT(x) = 1 − x OR(x, y) = 1 − (1 − x)(1 − y)
Any logical condition can be represented using a combination of these polynomials. Polynomials are expressive enough to capture all of boolean logic. This suggests that even single-variable polynomials should have strikingly complex behavior. The rest of the chapter will display bits of that dazzling performance.

10
Figure 2.2: Polynomials of varying degrees.
Culture
The most important cultural expectation, one every mathematician knows, is that the second you see a definition in a text you must immediately write down examples. Generous authors provide examples of genuinely new concepts, but an author is never obligated to do so. The unspoken rule is that the reader should not continue unless the reader understands what the definition is saying. That is, you aren’t expected to master the concept, most certainly not at the same speed you read it. But you should have some idea going forward of what the defined words refer to.
Software testing provides a good analogy. You start with the simplest possible tests, usually setting as many values as you can to zero or one, then work your way up to more complicated examples. Later, when you get stuck on some theorem or proof—an unavoidable occupational hazard—you return to those examples and test how the claims in the proof apply to them. This is how one builds so-called “mathematical intuition.” In the long term, that intuition allows you to absorb new ideas faster.
So let’s write down some examples of polynomials according to Definition 2.1, starting from the simplest. To make you pay attention, I’ll slip in some examples that are not

11
polynomials and your job is to check them against the definition. Take your time, and you can check your answers in the Chapter Notes.
f (x) = 0
g(x) = 12
h(x) = 1 + x + x2 + x3
i(x) = x1/2
j(x) = 1 + x2 − 2x4 + 8x8 2 15
k(x) = 4.5 + x − x2 l(x) = π − 1 x5 + eπ3x10
e m(x) = x + x2 − xπ + xe
Like software testing, examples weed out pesky edge cases and clarify what is permitted by the definition. For example, the exponents of a polynomial must be nonnegative integers, though I only stated it implicitly in the definition.
When reading a definition, one often encounters the phrase “by convention.” This can refer to a strange edge case or a matter of taste. A common example is the factorial n! = 1 · 2 · · · · · n, where 0! = 1 by convention. This makes formulas cleaner and provides a natural default value of an “empty product,” a sensible base case for a loop that computes the product of a (possibly empty) list of numbers.
For polynomials, convention strikes when we inspect the example f (x) = 0. What is the degree of f ? On one hand, it makes sense to say that the zero polynomial has degree n = 0 and a0 = 0. On the other hand, it also makes sense (in a strict, syntactical sense) to say that f has degree n = 1 with a0 = 0 and a1 = 0, or n = 2 with three zeros. But we don’t want a polynomial to have multiple distinct possibilities for degree. Indeed, this would allow f (x) = 0 to have every positive degree (by adding extra zeros), depriving the word “degree” of a consistent interpretation.
To avoid this, we amend Definition 2.1 so that the last coefficient an is required to be nonzero. But then the function f (x) = 0 is not allowed to be a polynomial! So, by convention, we define a special exception, the function f (x) = 0, as the zero polynomial. By convention, the zero polynomial is defined to have degree −1. Note that every time a definition includes the phrase “by convention,” a computer program gains an edge case.2
This edge case made us reconsider the right definition of a polynomial, but it was mostly a superficial change. Other times, as we will confront head on in Chapter 8 when we define limits, dealing with an edge case reveals the soul of a concept. It’s curious how mathematical books tend to start with the final product, instead of the journey to the
2 You may wonder: is it possible to represent the same polynomial with two formulas that have different degrees? Theorem 2.3 can be used to prove this is impossible. Exercise 4 asks you to prove it using elementary means.

12
right definition. Perhaps teaching the latter is much harder and more time consuming, with fewer tangible benefits. But in advanced mathematics, deep understanding comes in fits and starts. Often, no such distilled explanation is known.
In any case, examples are the primary method to clarify the features of a definition. Having examples in your pocket as you continue to read is important, and coming up with the examples yourself is what helps you internalize a concept.
It is a bit strange that mathematicians choose to write definitions with variable names by example, rather than using the sort of notation one might use to define a programming language syntax. Using a loose version of Backus-Naur form (BNF), which is used in parsers to define syntax, I might define a polynomial as:
coefficient = number variable = 'x' term = coefficient * variable ^ int polynomial = term
| term + polynomial
The problem is that this definition doesn’t tell you what polynomials are all about. While Definition 2.1 isn’t perfect, it signals that a polynomial is a function of a single input. BNF only provides a sequence of named tokens. As a human, we want to understand that a polynomial is a function with particular structure, and that’s not captured by a purely syntactic definition. This theme, that most mathematics is designed for human-to-human communication, will follow us throughout the book. Mathematical discourse is about getting a vivid and rigorous idea from your mind into someone else’s mind.
That’s why an author usually starts with a conceptual definition like Definition 2.1 many pages before discussing a computer-friendly representation. It’s why mathematicians will seamlessly convert between representations—such as the functional, settheoretic, and geometric representations I described earlier—as if mathematics were the JavaScript type system on steroids. In Java you have to separate an interface from the class which implements it, and in C++ templates are distinct from their usage. In math, we often have multiple equivalent definitions—some closer to an interface and some closer to a syntactic representation—and we have to prove that they are equivalent to justify switching between them. Once we’ve built up a collection of these definitions, we often settle on one as the “clearest” and most generally useful definition that is presented first. Underlying all the definitions is an abstract concept we keep in our minds. The definition is one way to make that concept concrete while also expressing one particular facet of its properties for the task at hand.
I want to make this extremely clear because in mathematics it’s implicit. My math teachers in college and grad school never explicitly discussed why one would use one definition over another, because somehow along the arduous journey through a math education, the folks who remained understood it. It also explains why understanding a definition is such an important prerequisite to reading the mathematics that follows.
Polynomials may seem frivolous to illustrate the difference between an object-asabstract-concept and the representational choices that go into understanding a defini-

13
tion, but the same pattern lurks behind more complicated definitions. First the author will start with the best conceptual definition—the one that seems to them, with the hindsight of years of study, to be the most useful way to communicate the idea behind the concept. For us that’s Definition 2.1. Often these definitions seem totally useless from a programming perspective.
Then ten pages later (or a hundred!) the author introduces another definition, often a data definition, which turns out to be equivalent to the first. Any properties defined in the first definition automatically hold in the second and vice versa. But the data definition is the one that allows for nice programs. You might think the author was crazy not to start with the data definition, but it’s the conceptual definition that sticks in your mind, generalizes, and guides you through proofs. This interplay between intuitive and data definitions will take center stage in Chapter 10, our first exposure to linear algebra.
It’s also worth noting that the multiplicity of definitions arose throughout history. Polynomials have been studied for many centuries, but parser-friendly forms of polynomials weren’t needed until the computer age. Likewise, algebra was studied before the graphical representations of Descartes allowed us to draw polynomials as curves. Each new perspective and definition was driven by an additional need. As a consequence, the “best” definition of a concept can change. Throughout history math has been shaped and reshaped to refine, rigorize, and distill the core insights, often to ease the fashionable calculations of the time.
In any case, the point is that we will fluidly convert between the many ways of thinking about polynomials: as expressions defined abstractly by picking a list of numbers, or as functions with a special structure. Effective mathematics is flexible in this way.
2.2 A Little More Notation
When defining a function, one often uses the compact arrow notation f : A → B to describe the allowed inputs and outputs. All possible inputs are collectively called the domain, and all possible outputs are called the range. There is one caveat I’ll explain via programming. Say you have a function that doubles the input, such as
int f(int x) { return 2*x;
}
The inputs are integers, and the type of the output is also integer, but 3 is not a possible output of this particular function.
In math we disambiguate this with two words. Range is the set of actual outputs of a function, and the “type” of outputs is called the codomain. The notation f : A → B specifies the domain A and codomain B, while the range depends on the semantics of f . When one introduces a function, as programmers do with type signatures and function headers, we state the notation f : A → B before the function definition.
Because mathematicians were not originally constrained by ASCII, they developed

14
other symbols for types. The symbol for the set of real numbers is R. The font is called “blackboard-bold,” and it’s the standard font for denoting number systems. Applying the arrow notation, a polynomial is f : R → R. A common phrase is to say a polynomial is “over the reals” to mean it has real coefficients. As opposed to, say, a polynomial over the integers that has integer coefficients.
Most famous number types have special symbols. The symbol for integers is Z, and the positive integers are denoted by N, often called the natural numbers.3 There is an amusing dispute of no real consequence between logicians and other mathematicians on whether zero is a natural number, with logicians demanding it is.
Finally, I’ll use the ∈ symbol, read “in,” to assert or assume membership in some set. For example q ∈ N is the claim that q is a natural number. It is literally short hand for the phrase, “q is in the natural numbers,” or “q is a natural number.” It can be used in a condition (preceded by “if”), an assertion (preceded by “suppose”), or a question.
2.3 Existence & Uniqueness
Having seen some definitions, we’re ready to develop the main tool we need for secret sharing: the existence and uniqueness theorem for polynomials passing through a given set of points.
First, a word about existence and uniqueness. Existence proofs are classic in mathematics. They come in all shapes and sizes. Mathematicians like to take interesting properties they see on small objects, write down the property in general, and then ask things like, “Are there arbitrarily large objects with this property?” or, “Are there infinitely many objects with this property?” I imagine a similar pattern in physics. Given equations that govern the internal workings of a star you might ask, would these equations support arbitrarily massive stars?
One simple uniqueness question is quite famous: are there are infinitely many pairs of prime numbers of the form p, p + 2? For example, 11 and 13 work, but 23 is not part of such a pair.4 It’s an open question whether there are infinitely many such pairs. The assertion that there are is called the Twin Prime Conjecture.
In some cases you get lucky, and the property you defined is specific enough to single out a unique mathematical object. This is what will happen to us with polynomials. Other times, the property (or list of properties) you defined are too restrictive, and there are no mathematical objects that can satisfy it. For example, Kleinberg’s Impossibility Theorem for Clustering lays out three natural properties for a clustering algorithm (an algorithm that finds dense groups of points in a geometric dataset) and proves that no algorithm can satisfy all three simultaneously. See the Chapter Notes for more on this. Though such theorems are often heralded as genius, more often than not mathematicians avoid impossibility by turning small examples into broad conjectures.
That’s how we’ll approach existence and uniqueness for polynomials. Here is the theo-
3 The Z stands for Zahlen, the German word for “numbers.” 4 See how I immediately wrote down examples?

15
rem we’ll prove, stated in its most precise form. Don’t worry, we’ll go carefully through every bit of it, but try to read it now.
Theorem 2.2. For any integer n ≥ 0 and any list of n + 1 points (x1, y1), (x2, y2), . . . , (xn+1, yn+1) in R2 with x1 < x2 < · · · < xn+1, there exists a unique polynomial p(x) of degree at most n such that p(xi) = yi for all i.
The one piece of new notation is the exponent on R2. This means “pairs” of real numbers. Likewise, Z3 would be triples of integers, and N10 length-10 tuples of natural numbers.
A briefer, more informal way to state the theorem: there is a unique degree n polynomial passing through a choice of n + 1 points.5 Now just like with definitions, the first thing we need to do when we see a new theorem is write down the simplest possible examples. In addition to simplifying the theorem, it will give us examples to work with while going through the proof. Write down some examples now. As mathematician Alfred Whitehead said, “We think in generalities, but we live in details.”
Back already? I’ll show you examples I’d write down, and you can compare your process to mine. The simplest example is n = 0, so that n + 1 = 1 and we’re working with a single point. Let’s pick one at random, say (7, 4). The theorem asserts that there is a unique degree zero polynomial passing through this point. What’s a degree zero polynomial? Looking back at Definition 2.1, it’s a function like a0 +a1x+a2x2 +· · ·+adxd (I’m using d for the degree here because n is already taken), where we’ve chosen to set d = 0. Setting d = 0 means that f has the form f (x) = a0. So what’s such a function with f (7) = 4? There is no choice but f (x) = 4. It should be clear that it’s the only degree zero polynomial that does this. Indeed, the datum that defines a degree-zero polynomial is a single number, and the constraint of passing through the point (7, 4) forces that one piece of data to a specific value.
Let’s move on to a slightly larger example which I’ll allow you to work out for yourself before going through the details. When n = 1 and we have n + 1 = 2 points, say (2, 3), (7, 4), the theorem claims a unique degree 1 polynomial f with f (2) = 3 and f (7) = 4. Find it by writing down the definition for a polynomial in this special case and solving the two resulting equations.6
Alright. A degree 1 polynomial has the form
f (x) = a0 + a1x.
Writing down the two equations f (2) = 3, f (7) = 4, we must simultaneously solve:
a0 + a1 · 2 = 3
a0 + a1 · 7 = 4
5 To say a function f (x) “passes” through a point (a, b) means that f (a) = b. When we say this we’re thinking of f as a geometric curve. It’s ‘passing’ through the point because we imagine a dot on the curve moving along it. That perspective allows for colorful language in place of notation. 6 If you’re comfortable solving basic systems of equations, you may want to skip ahead to the next section.

16
If we solve for a0 in the first equation, we get a0 = 3 − 2a1. Substituting that into the second equation we get (3 − 2a1) + a1 · 7 = 4, which solves for a1 = 1/5. Plugging this back into the first equation gives a0 = 3 − 2/5. This has forced the polynomial to be exactly
() 2 1 13 1
f (x) = 3 − + x = + x. 5 5 55
Geometrically, a degree 1 polynomial is a line. Our example above reinforces a fact we already know, that there is a unique line between any two points. Well, it’s not quite the same fact. What is different about this scenario? The statement of the theorem said, “x1 < x2 < · · · < xn+1”. In our example, this means we require x1 < x2. So this is where we run a sanity check. What happens if x1 = x2? Think about it, and if you can’t tell then you should try to prove it wrong: try to find a degree 1 polynomial passing through the points (2, 3), (2, 5).
The problem could be that there is no degree 1 polynomial passing through those points, violating existence. Or, the problem might be that there are many degree 1 polynomials passing through these two points, violating uniqueness. It’s your job to determine what the problem is. And despite it being pedantic, you should work straight from the definition of a polynomial! Don’t use any mnemonics or heuristics you may remember; we’re practicing reading from precise definitions.
In case you’re stuck, let’s follow our pattern from before. If we call a0 + a1x our polynomial, saying it passes through these two points is equivalent to saying that there is a simultaneous solution to the following two equations f (2) = 3 and f (2) = 5.
a0 + a1 · 2 = 3
a0 + a1 · 2 = 5
What happens when you try to solve these equations like we did before? Try it. What about for three points or more? Well, that’s the point at which it might start to get difficult to compute. You can try by setting up equations like those I wrote above, and with some elbow grease you’ll solve it. Such things are best done in private so you can make plentiful mistakes without being judged for it. Now that we’ve worked out two examples of the theorem in action, let’s move on to the proof. The proof will have two parts, existence and uniqueness. That is, first we’ll show that a polynomial satisfying the requirements exists, and then we’ll show that if two polynomials both satisfied the requirements, they’d have to be the same. In other words, there can only be one polynomial with that property.
Existence of Polynomials Through Points
We will show existence by direct construction. That is, we’ll “be clever” and find a general way to write down a polynomial that works. Being clever sounds scary, but the process is actually quite natural, and it follows the same pattern as we did for reading and understanding definitions: you start with the simplest possible example (but this time the

17
example will be generic) and then you work up to more complicated examples. By the time we get to n = 2 we will notice a pattern, that pattern will suggest a formula for the general solution, and we will prove it’s correct. In fact, once we understand how to build the general formula, the proof that it works will be trivial.
Let’s start with a single point (x1, y1) and n = 0. I’m not specifying the values of x1 or y1 because I don’t want the construction to depend on my arbitrary specific choices. I must ensure that f (x1) = y1, and that f has degree zero. Simply enough, we set the first coefficient of f to y1, the rest zero.

f (x) = y1
On to two points. Call them (x1, y1), (x2, y2) (note the variable is just plain x, and my example inputs are x1, x2, . . . ). Now here’s an interesting idea: I can write the polynomial in this strange way:

f (x)

=

y1

x x1

− x2 − x2

+

y2

x − x1 x2 − x1

Let’s verify that this works. If I evaluate f at x1, the second term gets x1 − x1 = 0 in the numerator and so the second term is zero. The first term, however, becomes

Ay1lsxxo11−−nxxo22te=thya1t

· 1, we

which is what we wanted: have explicitly disallowed

we x1

gave = x2

x1 as input and the output was y1. by the conditions in the theorem,

so the fractions will never be 0/0.

Likewise, if you evaluate f (x2) the first term is zero and the second term evaluates

to y2. So we have both f (x1) = y1 and f (x2) = y2, and the expression is a degree 1 polynomial. How do I know it’s degree one when I wrote f in that strange way? For one,

I could rewrite f like this:

f (x)

=

x1

y1 −

(x x2

−

x2)

+

x2

y2 −

x1

(x

−

x1),

and simplify with typical algebra to get the form required by the definition:

(

)

f (x) = x1y2 − x2y1 + y1 − y2 x

x1 − x2

x1 − x2

What a headache! Instead of doing all that algebra, I could observe that no powers of x appear in the formula for f that are larger than 1, and we never multiply two x’s together. Since these are the only ways to get degree bigger than 1, we can skip the algebra and be confident that the degree is 1.
The key to the above idea, and the reason we wrote it down in that strange way, is so that each constraint (e.g., f (x1) = y1) could be isolated in its own term, while all the other terms evaluate to zero. For three points (x1, y1), (x2, y2), (x3, y3) we just have to beef up the terms to maintain the same property: when you plug in x1, all terms except the first evaluate to zero and the fraction in the first term evaluates to 1. When you plug in x2, the second term is the only one that stays nonzero, and likewise for the third. Here is the generalization that does the trick.

18

f (x)

=

y1

(x−x2)(x−x3) (x1−x2)(x1−x3

)

+

y2

(x−x1 (x2−x1

)(x−x3) )(x2−x3

)

+

y3

(x−x1 (x3−x1

)(x−x2) )(x3−x2

)

Now you come in. Evaluate f at x1 and verify that the second and third terms are zero, and that the first term simplifies to y1. The symmetry in the formula should convince you that the same holds true for x2, x3 without having to go through all the steps two more times. Then argue why f is degree 2.

The general formula for n points (x1, y1), . . . , (xn, yn), should follow the same pattern. Add up a bunch of terms, and for the i-th term you multiply yi by a fraction you construct according to the rule: the numerator is the product of x − xj for every j except i, and the denominator is a product of all the (xi − xj) for the same j’s as the numerator. It works for the same reason that our formula works for three terms above. By now, the process

is clear enough that you could write a program to build these polynomials quite easily,

and we’ll walk through such a program together at the end of the chapter.

Here is the notation version of the process we just described in words. It’s a mess, but

we’ll break it down.





f (x)

=

∑n
i=1

yi

·

∏ 
j̸=i

x − xj xi − xj



What

a

mouthful!

I’ll

assume

the

∑ ,

∏

symbols

are

new

to

you.

They

are

read

seman-

tically as “sum” represent loops

and “product,” of arithmetic.

oTrhtaytpios,grthapehsitcaatlelymaesnt“s∑igmni=a1”(eaxnpdr)“pisi”.eqTuhievyaleesnstentotiatlhlye

following code snippet.

int i; sometype theSum = defaultValue;
for (i = 1; i <= n; i++) { theSum += expr(i);
}
return theSum;

Note by indexing from 1 and including the upper limit of the deviating from the standard programming style. Indexing from

fzoerrolo, olipkeco∑ndni=it0io, pnr,owdeuaceres

n + 1 terms in the resulting sum.

I used the undefined tokens defaultValue and sometype to highlight that the mean-

ing of the sum depends on what the conventional ‘zero object’ is in that setting. For

adding numbers the zero object is zero, and for adding polynomials it’s the zero polyno-

wmeiasl.tuItdgyegtsroeuxposti.cTwheithpominotreisatdhvaatn∑ceddmoeasthneomt iamtipcsly, wahtiycphew. Iet’’lsl

see in Chapter 16 when merely a shorthand for

the symbol +. Moreover, explaining

∑

using

code allows

me

to

define ∏ by analogy:

you just

re-

place += with *= and reinterpret the “default value” as what makes sense for multiplica-

19

tion. Functional programmers will know this pattern well, because both are a “fold” (or

“reduce”) function with a particular choice of binary operation and initial value.

The

notation

∏
j̸=i

adds

three

caveats.

First,

recall

that

in

this

context

i

is

fixed

by

the

outer loop, so j is the looping variable (unfortunately, the reader is required to keep track

of scope when it comes to nested sums and products). Second, the bounds on j are not

stated; we have to infer them from the context. There are two hints: we’re comparing j

to i, so it should probably have the same range as i unless otherwise stated, and we can

see where in the expression we’re using j. We’re using it as an index on the x’s. Since

the x indices go from 1 to n, we’d expect j to have that range. Being so loose might seem

hazardous, but if mathematicians consider it “easy” to infer the intent of a notation, then

it is considered rigorous enough.7

Though it sometimes makes me cringe to say it, give the author the benefit of the doubt. When things are ambiguous, pick the option that doesn’t break the math. In this respect, you have to act as both the tester, the compiler, and the bug fixer when you’re reading math. The best default assumption is that the author is far smarter than we are, and if you don’t understand something, it’s likely a user error and not a bug. In the occasional event that the author is wrong, it’s often a simple mistake or typo, to which an experienced reader would say, “The author obviously meant ‘foo’ because otherwise none of this makes sense,” and continue unscathed.

Finally, the j ̸= i part is an implied filter on an extra if statement to skip that iteration if

the range of j = i. Read

jo.uItnlsoiudde,th∏ejf̸=oirwloouopldyboeu“atdhde

product over j not equal to i.” If we wanted to write out the product-nested-in-a-sum as

a nested loop, it would look like this:

int i, j; sometype theSum = defaultSumValue;
for (i = 1; i <= n; i++) { othertype product = defaultProductValue;
for (j = 1; j <= n; j++) { if (j != i) { product *= foo(i, j); }
}
theSum += bar(i) * product; }
return theSum;





∑n

∏

f (x) = bar(i)  foo(i, j)

i=1

j̸=i

Compare the math and code, and make sure you can connect the structural pieces. Othfeternigthhteoifnane∑r poarre∏nthiseisnesthaereboomdyitotefdt,hwatitlhoothp.e default assumption that everything to
7 Another reason is that mathematicians get tired of writing these “obvious” details over and over again. Mathematicians don’t have text editor tools like programmers do.

20

If the formula on the right still seems impenetrable, take solace in your own experience: the reason you find the left side so easy to read is that you’ve spent years building up the cognitive pathways in your brain for reading code. You can identify what’s filler and what’s important; you automatically filter out the noise in the syntax. Over time, you’ll achieve this for mathematical formulas, too. You’ll know how to zoom in to one expression, understand what it’s saying, and zoom out to relate it to the formula as a whole. Everyone struggles with this, myself included.
One additional difficulty of reading mathematics is that the author will almost never go through these details for the reader. It’s a rather subtle point to be making so early in our journey, but it’s probably the first thing you notice when you read math books. Instead of doing the details, a typical proof of the existence of these polynomials looks like this.
Proof. Let (x1, y1), . . . , (xn+1, yn+1) be a list of n + 1 points with no two xi the same. To show existence, construct f (x) as

f (x)

=

n∑+1
i=1

yi

∏
j̸=i

x − xj xi − xj

Clearly the constructed polynomial f (x) has degree at most n because each term has
degree at most n. For each i, plugging in xi causes all but the i-th term in the sum to vanish,8 and the i-th term evaluates to yi, as desired.
… Uniqueness part (we’ll complete this proof in the next section) …

The square □ is called a tombstone and marks the end of a proof. It’s a modern replacement for QED borrowed from magazines.
The proof writer gives a relatively brief overview and you are expected to fill in the details to your satisfaction. It sucks, but if you do what’s expected of you—that is, write down examples of the construction before reading on—then you build up those neural pathways, and eventually you realize that the explanation is as simple and clear as it can be. Meanwhile, your job is to evaluate the statements made in the proof on your examples. Practice allows you to judge how much work you need to put into understanding a construction or definition before continuing. And, more importantly, you’ll understand it more thoroughly for all your testing.
Uniqueness of Polynomials Through Points
Now for the uniqueness part. This is a straightforward proof, but it relies on a special fact about polynomials. We’ll state the fact as a theorem that we won’t prove. Some terminology: a root of a polynomial f : R → R is a value z for which f (z) = 0.
Theorem 2.3. The zero polynomial is the only polynomial over R of degree at most n which has more than n distinct roots.
8 To “vanish” means to evaluate to zero.

21

On to the uniqueness proof. It works by supposing we actually have two polynomials f and g, both of degree n, passing through the desired set of points (x1, y1), . . . , (xn+1, yn+1). We don’t assume we know anything else about the polynomials ahead of time. They could be different, or they could be the same. If you wrote down two different looking polynomials with the two properties, they might just look different (maybe one is in factored form). So the proof operates by making no other assumptions, and showing that actually f and g have to be the same.
So suppose f, g are two such polynomials. Consider the polynomial (f − g)(x), which we define as (f − g)(x) = f (x) − g(x). Note that f − g is a polynomial because, if the coefficients of f are ai and the coefficients of g are bi, the coefficients of f − g are ci = ai − bi. If f and g have different degrees, then ci is simply ai or −bi, depending on which of f, g has a larger degree. It is crucial to this proof that f − g is a polynomial.
What do we know about f − g? It’s degree is certainly at most n, because you can’t magically produce a coefficient of x7 if you subtract two polynomials whose highestdegree terms are x5. Moreover, we know that (f − g)(xi) = 0 for all i. Recall that x is the generic input variable, while xi are the input values of the specific list of points (x1, y1), . . . , (xn+1, yn+1) that f and g are assumed to agree on. Indeed, for every i, f (xi) = g(xi) = yi, so subtracting them gives zero.
Now we apply Theorem 2.3. If we call d the degree of f − g, we know that d ≤ n, and hence that f − g can have no more than n roots unless it’s the zero polynomial. But there are n + 1 points xi where f − g is zero! Theorem 2.3 implies that f − g must be the zero polynomial, meaning f and g have the same coefficients.
Just for completeness, I’ll write the above argument more briefly and put the whole proof of the theorem together as it would show up in a standard textbook. That is, extremely tersely.

Theorem 2.4. For any integer n ≥ 0 and any list of n + 1 points
(x1, y1), (x2, y2), . . . , (xn+1, yn+1) in R2 with x1 < x2 < · · · < xn+1, there exists a unique polynomial p(x) of degree at most n such that p(xi) = yi for all i.

Proof. Let (x1, y1), . . . , (xn+1, yn+1) be a list of points with no two xi the same. To show existence, construct f (x) as





f (x)

=

n∑+1
i=1

yi

∏ 
j̸=i

x − xj xi − xj



Clearly the constructed polynomial f (x) is degree ≤ n because each term has degree
at most n. For each i, plugging in xi causes all but the i-th term in the sum to vanish, and the i-th term clearly evaluates to yi, as desired.
To show uniqueness, let g(x) be another polynomial that passes through the same set
of points given in the theorem. We will show that f = g. Examine f −g. It is a polynomial
with degree at most n which has all of the n + 1 values xi as roots. By Theorem 2.3, we conclude that f − g is the zero polynomial, or equivalently that f = g.

22
We spent quite a few pages expanding the details of a ten-line proof. This is par for the course. When you encounter a mysterious or overly brief theorem or proof it becomes your job to expand and clarify it as needed. Much like with reading programs written by others, as your mathematical background and experience grows you’ll need less work to fill in the details.
Now that we’ve shown the existence and uniqueness of a degree at most n polynomial passing through a given list of n + 1 points, we’re allowed to give “it” a name. It’s called the interpolating polynomial of the given points. The verb interpolate means to take a list of points and find the unique minimum-degree polynomial passing through them.
2.4 Realizing it in Code
Let’s write a Python program that computes the interpolating polynomial. I’m going to assume the existence of a polynomial class that accepts as input a list of coefficients (in the same order as Definition 2.1, starting from the degree zero term) and has methods for adding, multiplying, and evaluating at a given value. All of this code, including the polynomial class, is available at this book’s Github repository.9 Note the polynomial class is not intended to be perfect. The goal is not to be industry-strength, but to help you understand the constructions we’ve seen in the chapter.
Here are some examples of constructing polynomials.
# special syntax for the zero polynomial ZERO = Polynomial([])
f = Polynomial([1, 2, 3]) # 1 + 2 x + 3 x^2 g = Polynomial([-8, 17, 0, 5]) # -8 + 17 x + 5 x^3
f + g == Polynomial([-7, 19, 3, 5]) f(1) == 6
Now we write the main interpolate function. It uses the yet-to-be-defined function single_term that computes a single term of the interpolating polynomial for a given degree. Note we use Python list comprehensions, for which [EXPRESSION for x in my_list] is a shorthand expression for the following.
output_list = []
for x in my_list: output_list.append(EXPRESSION)
# the list comprehension expression evaluates to this list output_list
Now the interpolate function:
9 See pimbook.org.

23
def interpolate(points): """ Return the unique polynomial of degree at most n passing through the given n+1 points. """ if len(points) == 0: raise ValueError('Must provide at least one point.')
x_values = [p[0] for p in points] if len(set(x_values)) < len(x_values):
raise ValueError('Not all x values are distinct.')
terms = [single_term(points, i) for i in range(0, len(points))] return sum(terms, ZERO)
The first two blocks check for the edge cases: an empty input or repeating x-values. The last block creates a list of terms of the sum from the proof of Theorem 2.2. The return statement sums all the terms, using the zero polynomial as the starting value. Now for the single_term function.
def single_term(points, i): """ Return one term of an interpolated polynomial.
Arguments: - points: a list of (float, float) - i: an integer indexing a specific point
""" the_term = Polynomial([1.]) xi, yi = points[i]
for j, p in enumerate(points): if j == i: continue xj = p[0] the_term = the_term * Polynomial( [-xj / (xi - xj), 1.0 / (xi - xj)] )
return the_term * Polynomial([yi])
We had to break up the degree-1 polynomial (x − xj)/(xi − xj) into its coefficients, which are a0 = −xj/(xi − xj) and a1 = 1/(xi − xj). The rest computes the product over the relevant terms. Some examples:

24
>>> points1 = [(1, 1)] >>> points2 = [(1, 1), (2, 0)] >>> points3 = [(1, 1), (2, 4), (7, 9)] >>> interpolate(points1) 1.0 >>> interpolate(points2) 2.0 + -1.0 x^1 >>> f = interpolate(points3) >>> f -2.666666666666666 + 3.9999999999999996 x^1 + -0.3333333333333334 x^2 >>> [f(xi) for (xi, yi) in points3] [1.0, 3.999999999999999, 8.999999999999993]
Ignoring the rounding errors, we can see the interpolation is correct.
2.5 Application: Sharing Secrets
Next we’ll use polynomial interpolation to “share secrets” in a secure way. Here’s the scenario. Say I have five daughters, and I want to share a secret with them, represented as a binary string and interpreted as an integer. Perhaps the secret is the key code for a safe which contains my will. The problem is that my daughters are greedy. If I just give them the secret one might do something nefarious, like forge a modified will that leaves her all my riches at the expense of the others.
Moreover, I’m afraid to even give them part of the key code. They might be able to brute force the rest and gain access. Any daughter of mine will be handy with a computer. Even worse, three of the daughters might get together with their pieces of the key code, guess the rest, and exclude the other two daughters. So what I really want is a scheme that has the following properties.
1. Each daughter gets a “share,” i.e., some string unique to them.
2. If four of the daughters collude without the fifth, they cannot use their shares to reconstruct the secret.
3. If all five of the daughters combine their shares, they can reconstruct the secret.
In fact, I’d be happier if I could prove, not only that any four out of the five daughters couldn’t pool their shares to determine the secret, but that they’d provably have no information at all about the secret. They can’t even determine a single bit of information about the secret, and they’d have an easier time breaking open the safe with a jackhammer.
The magical fact is that there is such a scheme. Not only is it possible, but it’s possible no matter how many daughters I have (say, n), and no matter what minimum size group I want to allow to reconstruct the secret (say, k ≤ n). So I might have 20 daughters, and I may want any 14 of them to be able to reconstruct the secret, but prevent any group of 13 or fewer from doing so.

25
Polynomial interpolation gives us all of these guarantees. Here is the scheme. First represent the secret s as an integer. Construct a random polynomial f (x) so that f (0) = s. We’ll say in a moment what degree d to use for f (x). If we know d, generating f is easy. Call a0, . . . , ad the coefficients of f . Set a0 = s and randomly pick the other coefficients, while ensuring ad ̸= 0. If you have n people, the shares you distribute are values of f (x) at f (1), f (2), . . . , f (n). In particular, to person i you give the point (i, f (i)).
What do we know about subsets of points? If any k people get together, they can construct the unique degree k − 1 polynomial g(x) passing through all those points. The question is, will the resulting g(x) be the same as f (x)? If so, they can compute g(0) = f (0) to get the secret! This is where we pick d, to control how many shares are needed. If we want k to be the minimum number of shares needed to reconstruct the secret, we make our polynomial degree d = k − 1. Then if k people get together and interpolate g(x), they can appeal to Theorem 2.2 to be sure that g(x) = f (x).
Let’s be more explicit and write down an example. Say we have n = 5 daughters, and we want any k = 3 of them to be able to reconstruct the secret. Pick a polynomial f (x) of degree d = k − 1 = 2. If the secret is 109, we generate f as
f (x) = 109 + random · x + random · x2
Note that if you’re going to actually use this to distribute secrets that matter, you need to be a bit more careful about the range of these random numbers. For the sake of this example let’s say they’re random 10-bit integers, but in reality you’d want to do everything with modular arithmetic. See the Chapter Notes for further discussion.
Next, we distribute one point to each daughter as their share.
(1, f (1)), (2, f (2)), (3, f (3)), (4, f (4)), (5, f (5))
To give concrete numbers to the examples, if
f (x) = 109 − 55x + 271x2,
then the secret is f (0) = 109 and the shares are
(1, 325), (2, 1083), (3, 2383), (4, 4225), (5, 6609).
The polynomial interpolation theorem tells us that with any three points we can completely reconstruct f (x), and then plug in zero to get the secret.
For example, using our polynomial interpolation algorithm, if we feed in the first, third, and fifth shares we reconstruct the polynomial exactly:
>>> points = [(1, 325), (3, 2383), (5, 6609)] >>> interpolate(points) 109.0 + -55.0 x^1 + 271.0 x^2 >>> f = interpolate(points); int(f(0)) 109

26
At this point you should be asking yourself: how do I know there’s not some other way to get f (x) (or even just f (0)) if you have fewer than k points? You should clearly understand the claim being made. It’s not just that one can reconstruct f (0) when given enough points on f , but also that no algorithm can reconstruct f (0) with fewer than k points.
Indeed it’s true, and two little claims show why. Say f is degree d and you have d points (just one fewer than the theorem requires to reconstruct). The first claim is that there are infinitely many different degree d polynomials passing through those same d points. Indeed, if you pick any new x value, say x = 0, and any y value, and you add (x, y) to your list of points, then you get an interpolated polynomial for that list whose “decoded secret” is different. Due to Theorem 2.2, each choice of y gives a different interpolating polynomial.
The second claim is a consequence of the first. If you only have d points, then not only can f (0) be different, but it can be anything you want it to be! For any value y that you think might be the secret, there is a choice of a new point that you could add to the list to make y the “correct” decoded value f (0).
Let’s think about this last claim. Say your secret is an English sentence s = “Hello, world!” and you encode it with a degree 10 polynomial f (x) so that f (0) is a binary representation of s, and you have the shares f (1), . . . , f (10). Let y be the binary representation of the string “Die, rebel scum!” Then I can take those same 10 points, f (1), f (2), . . . , f (10), and I can make a polynomial passing through them and for which y = f (0). In other words, your knowledge of the 10 points gives you no information to distinguish between whether the secret is “Hello world!” or “Die, rebel scum!” Same goes for the difference between “John is the sole heir” and “Joan is the sole heir,” a case in which a single-character difference could change the entire meaning of the message.
To drive this point home, let’s go back to our small example secret 109 and encoded polynomial
f (x) = 109 − 55x + 271x2
I give you just two points, (2, 1083), (5, 6609), and a desired “fake” decrypted message, 533. The claim is that I can come up with a polynomial that has f (2) = 1083 and f (5) = 6609, and also f (0) = 533. Indeed, we already wrote the code to do this! Figure 2.3 demonstrates this with four different “decoded secrets.”
>>> points = [(2, 1083), (5, 6609)] >>> interpolate(points + [(0, 533)]) 533.0 + -351.7999999999999 x^1 + 313.4 x^2 >>> f = interpolate(points + [(0, 533)]); int(f(0)) 533.0
Note that the coefficients of the fake secret polynomial are no longer integers, but this problem is fixed when you do everything with modular arithmetic instead of floating point numbers (again, see the Chapter Notes).

27
Figure 2.3: A plot of four different curves that agree on the two points (2, 1083), (5, 6609), but have a variety of different “decoded secret” values.
The property of being able to “decode” to any possible plaintext given an encrypted text is called perfect secrecy, and it’s an early topic on a long journey through mathematical cryptography.
2.6 Cultural Review
1. Whenever you see a definition, you must immediately write down examples. They are your test cases and form a foundation for intuition.
2. In mathematics, we place a special emphasis on the communication of ideas from human to human.
3. A mathematical concept usually has multiple definitions. We prefer to work with the conceptual definition that is easiest to maintain in our minds, and we often don’t say when we switch between two representations.
2.7 Exercises
2.1. Prove the following:

28
1. If f is a degree-2 polynomial and g is a degree-1 polynomial, then their product f · g is a degree-3 polynomial.
2. Generalize the above: if f is a degree-n polynomial and g is a degree-m polynomial, then their product f · g has degree n + m.
3. Does the above fact work when f or g are the zero polynomial, using our convention that the zero polynomial has degree −1? If not, can you think of a better convention?
4. Prove that two polynomial formulas with different degrees cannot be equal as functions. That is, there must be some input on which they disagree.

2.2. Write down examples for the following definitions:
1. Two integers a, b are said to be relatively prime if their only common divisor is 1. Let n be a positive integer, and define by φ(n) (for n > 1) the number of positive integers less than n that are relatively prime to n. Describe why one might reasonably add the restriction n > 1.
2. A polynomial is called monic if its leading coefficient an is 1.
3. A factor of a polynomial f is a polynomial g of smaller degree so that f (x) = g(x)h(x), for some polynomial h. It is said that f can be “factored” into g and h. Note that g and h must both have real coefficients and be of smaller degree than f.
4. Two polynomials are called relatively prime if they have no non-constant smallerdegree polynomial factors in common. A polynomial is called irreducible if it cannot be factored into smaller polynomials. The greatest common divisor of two polynomials f, g is the monic polynomial of largest degree that is a factor of both f and g.

2.3. Verify the following theorem using the examples from the previous exercise. That is, write down examples and check that the theorem works as stated. If a, n are relatively prime integers, then aφ(n) has remainder 1 when dividing by n. This result is known as Euler’s theorem (pronounced “OY-lurr”), and it is the keystone of the RSA cryptosystem.

2.4. Look up Horner’s method for evaluating a polynomial as a function. Implement a polynomial data structure that uses Horner’s method for evaluation, and compare its runtime against naive evaluation methods.

2.5. A number x is called algebraic if it is the root of a polynomial whose coefficients are

lrriaaktteiioo√nisa2ltahnreuemnaulbgmeerbbsrea(rfircϕa, cw=tiho1inl+es2√no5uf.miInsbteietgrsaelrlgiske).ebrOπatiahcn?edrWweihasaeretiatfabimsoucoatul√slely2d

transcendental. Numbers n+ot√a3lg?ebraic. The golden

2.6. Prove the product of two algebraic numbers is algebraic. Similarly (but much harder), prove the sum of two algebraic numbers is algebraic. Despite the fact that π

29
and e are not algebraic, it is not known whether π + e or πe are algebraic. Look up a proof that they cannot both be algebraic. Note that many such proofs appeal to vector spaces, the topic of Chapter 10.

2.7. Let f (x) = a0 + a1x + · · · + anxn be a degree n polynomial, and suppose it has n real roots r1, . . . , rn.10 Prove Vieta’s formulas, which are

∑n
i=1

ri

=

− an−1 an

∏n
i=1

ri

=

(−1)n

a0 . an

Hint: if r is a root, then f (x) can be written as f (x) = (x−r)g(x) for some smaller degree g(x). This formula shows one way the coefficients of a polynomial encode information about the roots.

2.8. Look up a proof of Theorem 2.3. There are many different proofs. Either read one and understand it using the techniques we described in this chapter (writing down examples and tests), or, if you cannot, then write down the words in the proofs that you don’t understand and look for them later in this book.

2.9. There are many ways to skin a cat. The polynomial interpolation construction from this chapter is just one, often called Lagrange interpolation. Another is called Newton interpolation. Find a source that explains what it is, try to understand how these two interpolation methods differ, and implement Newton interpolation. Compare the two interpolation methods in terms of efficiency.

2.10. Bézier curves are single-variable polynomials that draw a curve controlled by a given set of “control points.” The polynomial separately controls the x and y coordinates of the Bézier curve, allowing for complex shapes. Look up the definition of quadratic and cubic Bézier curves, and understand how it works. Write a program that computes a generic Bézier curve, and animates how the curve is traced out by the input. Bézier curves are most commonly seen in vector graphics and design applications as the “pen tool.”
2.11. It is a natural question to ask whether the roots of a polynomial f are sensitive to changes in the coefficients of f . Wilkinson’s polynomial, defined below, shows that they are

∏ 20 w(x) = (x − i)
i=1
10 This also works for possibly complex roots.

30
The coefficient of x19 in w(x) is −210, and if it’s decreased by 2−23 the position of many of the roots change by more than 0.5. Read more details online, and find an explanation of why this polynomial is so sensitive to changes in its coefficients.11
2.12. Write a web app that implements the distribution and reconstruction of the secret sharing protocol using the polynomial interpolation algorithm presented in this chapter, using modular arithmetic with a 32-bit modulus p.
2.13. The extended Euclidean algorithm computes the greatest common divisor of two numbers, but it also works for polynomials. Write a program that implements the Euclidean algorithm to compute the greatest common divisor of two monic polynomials. Note that this requires an algorithm to compute polynomial long division as a subroutine.
2.14. The Chinese Remainder Theorem is stated as follows. Suppose M > 1 is an integer and M = m1 · m2 · · · mk where each mi > 1 is an integer. Suppose further that for each i, j, the greatest common divisor of mi and mj is 1. Let r1, . . . , rk be integers such that 0 ≤ ri < mi (ri is considered a desired remainder when dividing by mi). Then there is a unique x with 0 ≤ x < M such that x = ri mod mi for all i. One can construct the desired number directly, provided one knows how to find multiplicative inverses, and the proof is identical to the polynomial interpolation theorem. Find a source that expands on the details and try to understand them.
2.15. Perhaps the biggest disservice in this chapter is ignoring the so-called Fundamental Theorem of Algebra, that every single-variable monic polynomial of degree k can be factored into linear terms p(x) = (x − a1)(x − a2) · · · (x − ak). The reason is that the values ai are not necessarily real numbers. They might be complex. Moreover, all of the proofs of the Fundamental Theorem are quite hard. In fact, one litmus test for the “intellectual potency” of a new mathematical theory is whether it provides a new proof of the Fundamental Theorem of Algebra! There is an entire book dedicated to these oftenrepeated proofs.12 Sadly, we avoid complex numbers in this book. Luckily, there is a “baby” fundamental theorem, which says that every single-variable polynomial with real coefficients can be factored into a product of linear and degree-2 terms
p(x) = (x − a1)(x − a2) · · · (x − am)(x2 + bm+1x + am+1) · · · (x2 + bkx + ak),
where none of the quadratic terms can be factored into smaller degree-1 terms. One of history’s most famous mathematicians, Carl Friedrich Gauss, provided the first proof as his doctoral thesis in 1799. As part of this exercise, look up some different proofs of the Fundamental Theorem, but instead of trying to understand them, take note of the different areas of math that are used in the proofs.
11 In “The Perfidious Polynomial,” Wilkinson wrote, “I regard [the discovery of this polynomial] as the most traumatic experience in my career as a numerical analyst.”
12 Fine & Rosenberger’s “The Fundamental Theorem of Algebra.”

31
2.8 Chapter Notes

Which are Polynomials?

iTshbeepcaoulysneo√mxia=ls

were f (x), g(x), h(x), j(x), and l(x). The reason i is x1/2 does not have an integer power. Similarly, k(x) is

not not

a a

polynomial polynomial

because its terms have negative integer powers. Finally, m(x) is not because its powers,

π, e, are not integers. Of course, if you were to define π and e to be particular constants

that happened to be integers, then the result would be a polynomial. But without any

indication, we assume they’re the famous constants.

Twin Primes
The Twin Prime Conjecture, the assertion that there are infinitely many pairs of prime numbers of the form p, p + 2, is one of the most famous open problems in mathematics. Its origin is unknown, though the earliest record of it in print is in the mid 1800’s in a text of de Polignac. In an exciting turn of events, in 2013 an unknown mathematician named Yitang Zhang13 published a breakthrough paper making progress on Twin Primes.
His theorem is not about Twin Primes, but a relaxation of the problem. This is a typical strategy in mathematics: if you can’t solve a problem, make the problem easier until you can solve it. Insights and techniques that successfully apply to the easier problem often work, or can be made to work, on the harder problem. Zhang successfully solved the following relaxation of Twin Primes, which had been attempted many times before.

Theorem. There is a constant M , such that infinitely many primes p exist such that the next prime q after p satisfies q − p ≤ M .

If M is replaced with 2, then you get Twin Primes. The thinking is that perhaps it’s easier to prove that there are infinitely many primes pairs with distance 6 of each other, or 100. In fact, Zhang’s paper established it for M approximately 70 million. But it was the first bound of its kind, and it won Zhang a MacArthur “genius award” in addition to his choice of professorships.
As of this writing, subsequent progress, carried out by some of the world’s most famous mathematicians in an online collaboration called the Polymath Project, brought M down to 246. Assuming a conjecture in number theory called the Elliott-Halberstam conjecture, they reduced this constant to 6.

Impossibility of Clustering
A clustering algorithm is a program f that takes as input:

• A list of points S,
• A distance function d that describes the distance between two points d(x, y) where x, y are in S,

13 Though he had a Ph.D, early in his career Zhang had been unable to find academic work, and had stints in a motel, as a delivery driver, and at a Subway sandwich shop before he found a position as a lecturer at the University of New Hampshire.

32
and produces as output a clustering of S, i.e., a choice of how to split S into nonoverlapping subsets. The individual subsets are called “clusters.”
The function d is also required to have some properties that make it reasonably interpretable as a “distance” function. In particular, all distances are nonnegative, d(x, y) = d(y, x), and the distance between a point and itself is zero.
The Kleinberg Impossibility Theorem for Clustering says that no clustering algorithm f can satisfy all of the following three properties, which he calls scale-invariance, richness, and consistency.14
• Scale-invariance: The output of f is unchanged if you stretch or shrink all distances in d by the same multiplicative factor.
• Richness: Every partition of S is a possible output of f , (for some choice of d).
• Consistency: The output of f on input (S, d) is unchanged if you modify d by shrinking the distances between points in the same cluster and enlarging the distances between points in different clusters.
One can interpret this theorem as an explanation (in part) for why clustering is a hard problem. While there are hundreds of clustering algorithms to choose from, none “just works” the way we humans intuitively want one to. This may be, as Kleinberg suggests, because our naive brains expect these three properties to hold, despite the fact that they are mathematically incompatible.
It also suggests that the “right” clustering function depends more on the application you use it for, which raises the question: how can one pick a clustering function with principle?
It turns out, if you allow the required number of output clusters to be an input to the clustering algorithm, you can avoid impossibility and instead achieve uniqueness. For more, see the 2009 paper “A Uniqueness Theorem for Clustering” of Zadeh and Ben-David. The authors proceeded to study how to choose a clustering algorithm “in principle” by studying what properties uniquely determine various clustering algorithms; meaning if you want to do clustering in practice, you have to think hard about exactly what properties your application needs from a clustering. Suffice it to say, this process is a superb example of navigating the border separating impossibility, existence, and uniqueness in mathematics.
More on Secret Sharing
The secret sharing scheme presented in this chapter was originally devised by Adi Shamir (the same Shamir of RSA) in a two-page 1979 paper called “How to share a secret.” In this paper, Shamir follows the terse style and does not remind the reader how the interpolating polynomial is constructed.
14 Of incidental interest to readers of this book, Jon Kleinberg also developed an eigenvector-based search ranking algorithm that was a precursor to Google’s PageRank algorithm.

33
He does, however, mention that in order to make this scheme secure, the coefficients of the polynomial must be computed using modular arithmetic. Here’s what is meant by that, and note that we’ll return to understand this in Chapter 16 from a much more general perspective.
Given an integer n and a modulus p (in our case a prime integer), we represent n “modulo” p by replacing it with its remainder when dividing by p. Most programming languages use the % operator for this, so that a = n%p means a is the remainder of n/p. Note that if n < p, then n%p = n is its own remainder. The standard notation in mathematics is to use the word “mod” and the ≡ symbol (read “is equivalent to” or “is congruent to”), as in
a ≡ n mod p.
The syntactical operator precedence is a bit weird here: “mod” is not a binary operation, but rather describes the entire equation, as if to say, “everything here is considered modulo p.”
We chose a prime p for the modulus because doing so allows you to “divide.” Indeed, for a given n and prime p, there is a unique k such that (n · k) ≡ 1 mod p. Again, an interesting example of existence and uniqueness. Note that it takes some work to find k, and the extended Euclidean algorithm is the standard method. When evaluating a polynomial function like f (x) at a given x, the output is taken modulo p and is guaranteed to be between 0 and p.
Modular arithmetic is important because (1) it’s faster than arithmetic on arbitrarily large integers, and (2) when evaluating f (x) at an unknown integer x not modulo p, the size of the output and knowledge of the degree of f can give you some information about the input x. In the case of secret sharing, seeing the sizes of the shares reveals information about the coefficients of the underlying polynomial, and hence information about f (0), the secret. This is unpalatable if we want perfect secrecy.
Moreover, when you use modular arithmetic you can prove that picking a uniformly random (d + 1)-th point in the secret sharing scheme will produce a uniformly random decoded “secret” f (0). That is, uniformly random between 0 and p. Without bounding the allowed size of the integers, it doesn’t make sense to have a “uniform” distribution. As a consequence, it is harder to define and interpret the security of such a scheme.
Finally, from discussions I’ve had with people using this scheme in industry, polynomial interpolation is not fast enough for modern applications. For example, one might want to do secret sharing between three parties at streaming-video rates. Rather, one should use so-called “linear” secret sharing schemes, which are based on systems of linear equations. Such schemes are best analyzed from the perspective of linear algebra, the topic of Chapter 10.

Chapter 3
On Pace and Patience
You enter the first room of the mansion and it’s completely dark. You stumble around bumping into the furniture but gradually you learn where each piece of furniture is. Finally, after six months or so, you find the light switch, you turn it on, and suddenly it’s all illuminated. You can see exactly where you were. Then you move into the next room and spend another six months in the dark. So each of these breakthroughs, while sometimes they’re momentary, sometimes over a period of a day or two, they are the culmination of, and couldn’t exist without, the many months of stumbling around in the dark that precede them.
–Andrew Wiles on what it’s like to do mathematics research.
We learned a lot in the last chapter. One aspect that stands out is just how slow the process of learning unfamiliar math can be. I told you that every time you see a definition or theorem, you had to stop and write stuff down to understand it better. But this isn’t all that different from programming. Experienced coders know when to fire up a REPL or debugger, or write test programs to isolate how a new feature works.
The main difference for us is that mathematics has no debugger or REPL. There is no reference implementation. Mathematicians often get around this hurdle by conversation, and I encourage you to find a friend to work through this book with. As William Thurston writes in his influential essay, “On Proof and Progress in Mathematics,” mathematical knowledge is embedded in the minds and the social fabric of the community of people thinking about a topic. Books and papers support this, but the higher up you go, the farther the primary sources stray from textbooks.
If you are reading this book alone, you have to play the roles of the program writer, the tester, and the compiler. The writer for when you’re conjuring new ideas and asking questions; the tester for when you’re reading theorems and definitions; and the compiler to check your intuition and hunches for bugs. This often slows reading mathematics down to a crawl, for novices and experts alike. Mathematicians always read with a pencil and notepad handy.
When you first read a theorem, you expect to be confused. Let me say it again: the rule is that you are confused, the exception is that everything is clear. Mathematical culture requires being comfortable being almost continuously in a state of little to no
35

36
understanding. It’s a humble life, but once you nail down what exactly is unclear, you can make progress toward understanding. The easiest way to do this is by writing down lots of examples, but it’s not always possible to do that. We’ve already seen an example, a theorem about the impossibility of having a nonzero polynomial with more roots than its degree.
In the quote at the beginning of this chapter, Andrew Wiles discusses what it’s like to do mathematical research, but the same analogy holds for learning mathematics. Speaking with experienced mathematicians and reading their books makes you feel like an idiot. Whatever they’re saying is the most basic idea in the world, and you barely stumble along. My favorite dramatic embodiment of this feeling is an episode of a YouTube series called Kid Snippets in which children are asked to pretend to be in a math class, while adult actors act it out using dubbed voices.1 The older child tries to explain to the younger child how to subtract, and the little kid just doesn’t get it. Aside from being absolutely hilarious, the video has a deep and probably unintentional truth, that the more mathematics you try to learn the more you feel like the poor student. The video especially resonates when, toward the end, the teacher asks, “Do you get it now?” and the student pauses and slowly says, “Yes.” That yes is the fledgling mathematician saying, “I obviously don’t understand, but I’ve accepted it and will try to understand it later.”
I’ve been in the student’s shoes a thousand times. Indeed, if I’m not in those shoes at least once a day then it wasn’t a productive day! I say at least a dozen stupid things daily and think countlessly many more stupid thoughts in search of insight. It’s a rare moment when I think, “I’m going to solve this problem I don’t already know how to solve,” and there is no subsequent crisis. Even in reading what should be basic mathematical material (there’s a huge list of things that I am embarrassed to be ignorant about) I find myself mentally crying out, “How the hell does that statement follow⁉”
I had a conversation with an immensely talented colleague, a far more talented mathematician than I, in which she said (I paraphrase), “If I spend an entire day and all I do is understand this one feature of this one object that I didn’t understand before, then that’s a great day.” We all have to build up insight over time, and it’s a slow and arduous process. In Andrew Wiles’s analogy, my friend is still in the dark room, but she’s feeling some object precisely enough to understand that it’s a vase. She still has no idea where the light switch is, and the vase might give her no indication as to where to look next. But if piece by piece she can construct a clear enough picture of the room in her mind, then she will find the switch. What keeps her going is that she knows enough little insights will lead her to a breakthrough worth having.
Though she is working on far more complicated and abstract mathematics than you are likely to, we must all adopt her attitude if we want to learn mathematics. If it sounds like all of this will take way too much of your time (all day to learn a single little thing!), remember two things. First, my colleague works on much more abstract and difficult mathematics than the average programmer interested in mathematics would encounter. She’s looking for the meta-insights that are many levels above the insights found in this
1 You can watch it at http://youtu.be/KdxEAt91D7k

37
book. As we’ll see in Chapter 11, insights are like a ladder, and every rung is useful. Second, the more you practice reading and absorbing mathematics, the better you get at it. When my colleague says she spent an entire day understanding something, she efficiently applied tools she had built up over time. She has a bank of examples to bolster her. She knows how to cycle through applicable proof techniques, and how to switch between different representations to see if a different perspective helps. Some of these techniques are described in Appendix B.
But most importantly, she’s being inquisitive! Her journey is led as much by her task as by her curiosity. As mathematician Paul Halmos said in his book, “I Want to be a Mathematician,”
Don’t just read it; fight it! Ask your own questions, look for your own examples, discover your own proofs.
Mathematician Terence Tao expands on this in his essay, “Ask yourself dumb questions—and answer them!”
When you learn mathematics, whether in books or in lectures, you generally only see the end product—very polished, clever and elegant presentations of a mathematical topic. However, the process of discovering new mathematics is much messier, full of the pursuit of directions which were naive, fruitless or uninteresting.
While it is tempting to just ignore all these “failed” lines of inquiry, actually they turn out to be essential to one’s deeper understanding of a topic, and (via the process of elimination) finally zeroing in on the correct way to proceed.
So one should be unafraid to ask “stupid” questions, challenging conventional wisdom on a subject; the answers to these questions will occasionally lead to a surprising conclusion, but more often will simply tell you why the conventional wisdom is there in the first place, which is well worth knowing.
So you’ll get confused. We all do. A good remedy is finding the right pace to make steady progress. And when in doubt, start slow.

Chapter 4
Sets
God created infinity, and man, unable to understand infinity, created finite sets.
– Gian-Carlo Rota
In this chapter we’ll lay foundation for the rest of the book. Most of the chapter is devoted to the mathematical language of sets and functions between sets. Sets and functions serve not only as the basis of most mathematics related to computer science, but also as a common language shared between all mathematicians. Sets are the modeling language of math. The first, and usually simplest, way to convert a real world problem into math involves writing down the core aspects of that problem in terms of sets and functions. Unfortunately set theory has a lot of new terminology. The parts that are new to you are best understood by writing down lots of examples.
After converting an idea into the language of sets, you may use the many existing tools and techniques for working with sets. As such, the work one invests into understanding these techniques pays off across all of math. It’s largely the same for software: learning how to decompose a complex problem into simple, testable, maintainable functions pays off no matter the programming language or problem you’re trying to solve. The same goes for the process of modeling business rules in software in a way that is flexible as the business changes. Sets are a fundamental skill.
At the end of the chapter we’ll see the full modeling process for an application called stable marriages, which is part of an interdisciplinary field of mathematics and economics called market design. In economics, there are occasionally markets in which money can’t be used as a medium of exchange. In these instances, one has to find some other mechanism to allow the market to function efficiently. The example we’ll see is the medical residency matching market, but similar ideas apply to markets like organ donation and housing allocation. As we’ll see, the process of modeling these systems so they can be analyzed with mathematics requires nothing more than fluency with sets and functions. The result is a Nobel-prize winning algorithm used by thousands of medical students every year.
39

40
4.1 Sets, Functions, and Their -Jections
A set is a collection of unique objects. You’ve certainly seen sets before in software. In Python they are simply called “sets.” In Java they go by HashSet, and in C++ by unordered_set. Functionally they are all equivalent: a collection of objects without repetition. While set implementations often have a menagerie of details—such as immutability of items, collision avoidance techniques, complexity of storing/lookup—mathematical sets “just work.” In other words, we don’t care how items enter and leave sets, and mutability is not a concern because we aren’t hashing anything to look it up. Efficiency is irrelevant.
To start, we need to know how to describe sets. The simplest way is with words. For example, I can describe the set of integers divisible by seven, or the set of primes, or the set of all syntactically correct Java programs.1 Often the goal of analyzing a mathematical object is to come up with a concrete description of a set, but implicit definitions are a great starting point.
Set-builder notation provides a more syntactic way to describe sets. For example, the set of all positive integers divisible by seven can be written:
S = {x : x ∈ N, x is divisible by 7}
The notation reads like the sentence in words, where the colon stands for “such that.” I.e., “The set of values x such that x is in N and x is divisible by 7.” Sometimes a vertical bar | is used in place of the colon. The symbols separate the constructive expression from the membership conditions (it’s not an output-input pipe as in shell scripting). The ∈ symbol denotes membership in a set, and the objects in a set are called elements.
Fans of functional programming are cheering as they read, because set-builder notation exists in many programming languages as comprehension syntax. In a language with infinite list comprehensions, say Haskell, the above would be implemented as follows:
[x | x <- [1..], mod x 7 == 0]
Lists made with list comprehensions need not have unique elements, while mathematical sets must. In set-builder notation is also more expressive. Put whatever conditions you like after the colon, even if you don’t know how to compute them! The left hand side of the colon may also be an expression, as in
{(x, 2x + 1) : 0 ≤ x < 10}
Now we turn to some definitions you may already be familiar with. If not, remember it’s your job to write down examples. In either case, mathematical texts typically define
1 There are some strange “meta” things you are not allowed to describe as sets, such as the set of all sets. It turns out this is not a set, and it caused a lot of grief to early 20th century mathematicians who really cared about the logical foundations of mathematics. This book omits these topics, since all of our sets will be comfortably finite or concrete like R.

41
something once and only once. I will occasionally repeat definitions that are used across chapters, but generally authors will not. You’re expected to have understood a definition to an appropriate degree of comfort before continuing.
Definition 4.1. The cardinality or size of a set A, denoted |A|, is the number of elements in A when that number is finite, and otherwise we say A has infinite cardinality.2 A set with no elements is called the empty set, and it has cardinality zero.
Definition 4.2. A set B is a subset of another set A if every element b ∈ B is also an element of A. This relationship is denoted B ⊂ A. Two sets are said to be equal if they contain the same elements. Equivalently, two sets A, B are equal if both A ⊂ B and B ⊂ A. Set equality is denoted by A = B.3
Proving one set is a subset of another is usually easy, but not always. The standard technique is to fix b to be an arbitrary element of B, and use whatever characteristic defines B to show that b ∈ A as well. Here’s a brief example: the set of integers divisible by 57 is a subset of the set of integers divisible by 3, because any number b divisible by 57 has the form b = 57 · k = 3 · (19 · k), which means it’s also divisible by 3. No alarms and no surprises.
If I have a binary boolean-valued operator like ∈, then putting a slash through it like ̸∈ denotes the negation of that claim or query. Other slashed operators include ̸=, ̸⊂, ̸∼.
Definition 4.3. Given two sets A and B, the complement of B in A is the set {a ∈ A : a ̸∈ B}. The complement is denoted either by A \ B or A − B, and sometimes BC when B ⊂ A and A is clear from context.
You can already see I’m starting to be creatively flexible with set-builder notation. Here a ∈ A might be interpreted as a boolean-valued expression, suggesting the set has only boolean-valued members. However, reading it as a sentence makes sense of it instead as an assertion: “The set of a in A such that a is not in B.” Writing it more verbosely, {a : a ∈ A and a ̸∈ B} is extra work without significant gain for the reader. If you prefer the verbose version, it’s likely because you’ve spent so long phrasing your thoughts to be machine readable. Appeal to your inner voice here, not your inner type-checker.
Definition 4.4. Given sets A, B, their union, denoted A ∪ B is the set {x : x ∈ A or x ∈ B} (inclusive or). The intersection, denoted A ∩ B, is the set {x : x ∈ A and x ∈ B}.4
2 It is not trivial to prove formally that every set has a well-defined size. This fact is intertwined with the formal axiomatic framework set theory is based on, called the Zermelo-Fraenkel set theory, often abbreviated as ZF or ZFC. Axiomatic set theory is beyond the scope of this book, but it is one of those topics that every mathematician has seen at least once. 3 Mathematicians are divided on whether A ⊂ B allows A to be equal to B. Some authors insist, drawing from the ≤ and < notation for numbers, that only A ⊆ B allows for A = B, and they call ⊂ the “strict subset” operator. I have never heard a convincing argument that the matter warrants debate, and so I opt for the briefest: ⊂ allows equality. This book never requires a strict subset operator, but if it did I would use ⊊. 4 Hence the name of my blog, Math ∩ Programming.

42
If you want some practice working with basic set definitions, prove that for any two sets A, B, the following containments hold: A ∩ B ⊂ A and A ⊂ A ∪ B.
Definition 4.5. The product of two sets A, B denoted A × B, is the set of all ordered pairs of elements in A and elements in B. In set-builder notation it is:
A × B = {(a, b) : a ∈ A and b ∈ B}
The parentheses denote a tuple, i.e., an ordered list allowing repetition.
The product is the usual way we turn the real line R into the real plane R2. That is, R2 is defined to be R × R, and R3 = R × R × R. Unpacking this, there is a little confusion over where the parentheses go. That is, should it be (R × R) × R, or R × (R × R)? These give rise to two different sets. The first is
(R × R) × R = {((a, b), c) : a ∈ R, b ∈ R, c ∈ R}
and the second is
R × (R × R) = {(a, (b, c)) : a ∈ R, b ∈ R, c ∈ R}
We want these sets to be considered the same. Indeed, the difference between the two is the kind of distinction that programmers are very familiar with, because compilers will refuse to proceed unless the parentheses align. But mathematicians, for reasons we’ll see shortly,5 brush aside the difference and just say they’re the “same” set, and they’re both equivalent to
(R × R) × R = R × (R × R) = {(a, b, c) : a ∈ R, b ∈ R, c ∈ R}
We will return later in this chapter, and again in Chapters 9 and 16 when complexity will beg for a rigorous and useful abstraction called the quotient, to understand why it’s okay to call these two sets “the same.” For now, simply define an n-fold product to collapse pairs into tuples of length n:
Rn = R × · · · × R = {(a1, . . . , an) : ai ∈ R for every i}
n times
This notation can be used for any set. Next we define functions as special subsets of a product.
Definition 4.6. Let A, B be sets, and let F be a subset of A × B. We say that F is a function if it satisfies the following property: for each a ∈ A, there is a unique pair (a, b) ∈ F (an input must have exactly one output). The set A is called the domain of F and B is called the codomain of F . To denote this, we use the arrow notation F : A → B.
5 There’s a bijection!

43
You should be writing down examples, but this one needs some help. We think of functions computationally as mappings from inputs to outputs. So much so that the nouns function and map are synonyms. But this definition of a function is a set. I’m going to convince you that the distinction is merely a matter of notation. It exists to fill the role of a “bare metal” implementation of a function in the modeling language of sets.
For the example, say F is the set of pairs of positive integers and their squares.
F = {(1, 1), (2, 4), (3, 9), (4, 16), . . . } = {(x, x2) : x ∈ N}.
It’s a subset of N × N. Now we can add a bit of notation: instead of saying that (3, 9) ∈ F we use the mapping notation F (3) = 9. With this, we could describe F the way we wanted to all along, as F (x) = x2. The conditions in Definition 4.6 ensure that every input x has some output F (x), and that each input x has only one output F (x). Providing a concrete algorithm to compute the output from the input makes these conditions trivial, as is the case with squared integers, but an algorithm is not needed to define a function.
Reiterating a note from Chapter 2, the codomain B is not strictly encoded in the data of a function F : A → B. The codomain is the set of allowed outputs.
So why go through all the trouble of defining functions in terms of sets? Part of the answer is historical. The concept of sets as a modeling tool has probably existed for as long as mathematics, but it was primarily used in its language form (“I declare, considereth only those heavenly numbers whose factorisation into prymes containeth nary a repeated factor!”). The notation y = f (x) was invented in the 1700’s by Leonhard Euler, and in those times most functions were only defined in terms of formulas that were easy to write down. It was not until the late 19th century that mathematicians formally studied sets, and proposed them as a logical foundation for all of mathematics. To do so requires restating all existing concepts in terms of sets. Definition 4.6 does this for functions. Similar definitions exist defining integers and ordered tuples in terms of sets. How tedious.
In this light, our initial definition of a set was completely imprecise. There is a more precise definition, but it is the sort that only a logician would love, called Zermelo-Fraenkel set theory. In brief, its base concepts are the empty set, set membership, a notion of infinity, and a restricted choice of ways to build sets from other sets. Using this one can define numbers, functions—even all of calculus—from “first principles.” To instill this idea in future mathematicians, many introductory proof textbooks define everything in terms of sets, and do formal proofs to a degree of precision most mathematicians avoid in their day to day work.
In theory, mathematicians like the idea that everything can be reduced to sets. Actually doing it in practice will drive you mad. It’s like writing all your programs in pure binary. Few do it, but we all take comfort in the idea that we could peel back the layers to reveal the raw assembly instructions. In reality, abstractions keep us productive. Likewise, defining the entirety of mathematics in sets is like “bare metal” programming, but without any of the speed benefits of the finished program. Someone ironed out set theory it once, and we have a record of their work. Now we can get back to doing mathematics.

44
The special notation for functions highlights our conceptual emphasis. We think of functions differently than regular sets, with a semantic input-output dependence that set notation doesn’t natively convey.
Now we turn to a few useful definitions about subsets of inputs and outputs of a function. A seasoned programmer is less likely to be familiar with the remainder of the definitions in this chapter, but we will rely on them throughout the book.
Definition 4.7. Given a function f : A → B, we define the image of f (or the image of A under f ) as the set
f (A) = {f (a) : a ∈ A}
This is denoted f (A) to signal that we’re putting everything in A through f , though it is also denoted im(f ) or just im f . If C ⊂ A is a subset, we can similarly define the image of C, denoted f (C), as f (C) = {f (c) : c ∈ C}. The image of A is equivalent to the range of a function with domain A, but we use a different word so we can speak of the image of a particular subset as well.
As a shorthand for “there exists,” mathematicians often use the symbol ∃. So an equivalent definition of im f is
im f = {b ∈ B : ∃a ∈ A with f (a) = b}.
We won’t rely heavily on the ∃ notation, but it is quite common. Now we define the preimage, the set of inputs mapping to a specified set of outputs.
Definition 4.8. Let A, B be sets and f : A → B a function. Let b ∈ B. The preimage of b under f , denoted f −1(b) is the set {a ∈ A : f (a) = b}. Likewise, if C ⊂ B is a subset, then f −1(C) is defined to be {a ∈ A : f (a) ∈ C}.
For F (x) = x2 as a mapping R → R, the preimage of 4 is F −1(4) = {−2, 2}. The superscript −1 is intended to invoke the concept of an inverse function. The preimage generalizes an inverse to operate on any element or subset of the codomain. The preimage always exists, though it may be the empty set.
The next three definitions are quite special.
Definition 4.9. A function f : A → B is called an injection (adjectivally, is injective) if whenever a, a′ ∈ A are different elements of A, then f (a), f (a′) are different elements of B.
An injection “injects” a copy of A inside B by way of f , so that no two elements of A get mapped to the same thing in B. For example, F (x) = x2 is an injection from N → N, but if we defined it for all integers Z → Z it would not be injective because, by way of counterexample, (−4)2 = 42 = 16. Figure 4.1 is the picture you should have in your head whenever you think of an injection. To put injectivity another way, f : A → B is an injection exactly when the preimage of every element b ∈ B has size 0 or 1.

45

A

B

Figure 4.1: An example of an injection, where different inputs are mapped to different outputs. The dots are elements of the set, and the arrows show the mapping. This example is also a non-surjection.

A

B

Figure 4.2: An example of a surjection, where every element of the codomain is hit by some element of the domain mapped through f . The dots are elements of the set, and the arrows show the mapping. This example is also a non-injection.

46
Definition 4.10. A function f : A → B is called a surjection (adjectivally, is surjective) if for every b ∈ B, there is some a ∈ A, with f (a) = b. In other words, f is surjective if im f = B.
Surjections “hit everything” in B by things mapped from A. So our squaring function on integers F (x) = x2 is not a surjection, because 2 has no integer square root. However, if we redefined it for positive real numbers it would be: every positive real number has a positive square root. To phrase it in terms of preimages, a surjection f : A → B has the property that every b ∈ B has a nonempty preimage f −1(b).
Another bit of notation, just like ∃ meaning “there exists,” the symbol ∀ is a shorthand for “for all.” I remember it by the backwards E standing for Exists, while the upside-down A stands for All. So the surjective property can be written hyper-compactly as
∀b ∈ B, ∃a ∈ A such that f (a) = b.
The symbols ∀, ∃ are called quantifiers and an expression in which every variable is bound by a quantifier is called “fully quantified.”
I will shy away from such dense notation in this book, though it will come in handy when we study Calculus in Chapter 8. While this example is not particularly difficult to parse, unrestrained use of ∀, ∃ can quickly spin out of control. Just as programmers shouldn’t cram a lot of complex logic into a single line of code, bad mathematical writers cram many quantifiers into a single line of math when it’s not necessary. That being said, familiarity with the symbols is broadly assumed.
Finally, f : A → B is called a bijection if it is both a surjection and an injection. Adjectivally f is called bijective. A bijection is also called a one-to-one correspondence.6 Bijections are nice because they can be used to say that two sets have the same cardinality (size), and it makes sense for infinite sets. If there is a bijection A → B then |A| = |B|. Likewise, if there is an injection A → B then |A| ≤ |B|, and the opposite works for surjections. See the exercises for more on this. Figure 4.3 shows the typical picture for a bijection.
Being a bijection f : A → B means every b ∈ B has a preimage of size exactly 1. In this case, the idea of an “inverse” to f makes sense: to invert f , map b ∈ B to the unique element a with f (a) = b. One denotes this function f −1 : B → A. A more precise definition goes as follows.
Definition 4.11. An inverse of a function f : A → B is a function g : B → A satisfying both g(f (a)) = a for every a ∈ A and f (g(b)) = b for every b ∈ B. If such a g exists, we say f is invertible.
All bijections are invertible, and vice versa invertible functions must be bijections. Computing the inverse function given only a description of a function can be notoriously difficult. Indeed, most of cryptography rests on the assumption that some functions are
6 Injections are sometimes called one-to-one and surjections are sometimes called onto. I won’t use those terms in this book, but they are common.

47

A

B

Figure 4.3: An example of a bijection, which is both an injection and a surjection.
computationally infeasible to invert. On the other hand, in linear algebra it is feasible, though often expensive, to compute the inverse of a matrix. As such, it is worthwhile to study the notion of an inverse in generality. This can grease the wheels of a complicated proof in an advanced setting, but more importantly it separates the mere set-theoretic aspects of a function from application-specific properties.
Here are two such propositions we’ll use much later in our study of linear algebra concerning the existence and structure of inverses. If you feel emotionally drained by all the definitions in this chapter so far, feel free to skip these and come back when we refer to them in Chapter 12.
Proposition 4.12. Inverses are unique.
Proof. Let f : A → B be a bijection and suppose that both g1 : B → A and g2 : B → A are inverses. We will show g1(b) = g2(b) for every b ∈ B. Fix any b ∈ B. Let a be an element of A such that f (a) = b. Then g1(b) = g1(f (a)) = a and the same reasoning proves g2(b) = a. So g1 and g2 are the same function.
The next proposition says that a “left-sided” inverse—satisfying just one of the two requirements to be an inverse—that happens to be a bijection is automatically a two-sided inverse.
Proposition 4.13. Let A, B be sets and f : A → B a bijection. Suppose g : B → A is a function satisfying g(f (a)) = a for every a ∈ A. Then g is the inverse for f , i.e., f (g(b)) = b for every b ∈ B.
Proof. It’s crucial here that f is surjective (otherwise the theorem is not true!). Given b ∈ B, we need to show that f (g(b)) = b. Start by choosing an a ∈ A for which f (a) = b. Then g(b) = g(f (a)) = a. Apply f to both sides to get f (g(b)) = f (a) = b, as desired.

48
Before we move on let me explain an earlier comment. I said we call (R × R) × R = R×(R×R) by “brushing aside” the differences between the two. There is a rigorous way to do this, but I’ll only explain half of the rigor right now. The essential reason is because there is a bijection (R × R) × R → R × (R × R) that maps ((a, b), c) to (a, (b, c)). Often when mathematicians want to “call” two things the same, they’ll come up with such a bijection, and say the two things on either side of such a bijection should be considered the same. It’s like an implicit typecast, always reversible in this case. The formal idea is called a “quotient,” which we’ll see in Chapter 9.
4.2 Clever Bijections and Counting
Now that we have the basic language of sets to model our problems, on to some problems. Say you want to count the size of a set. Since sets can be defined implicitly, it may not be obvious how. A useful tool used all over math is the trick of coming up with a clever bijection. This can transform a seemingly difficult counting problem into an elegantly trivial one.7
Our first problem concerns a tournament of tennis players. The tournament is singleelimination, meaning when two players finish a match the winner stays in the tournament and the loser is out. As the tournament host, you want to know how many games will be played in total. That is, given a set of games (each game is a set of two players) generated by this elimination process, we want to count its size.
Say you start with a thousand players. Let’s entertain a naive computation. In the first round of the tournament, each player is paired up with another and 500 games are played. In the second round there are 500 remaining players, and they again pair off to play 250 games. In the third, 125 games. In the fourth round you hit an edge case, because there are an odd number of players and one must sit out. Fine, you keep going, diligently tracking the players who sit out, and eventually you get to a number. You should try this yourself, and verify that the answer is 999 games. Isn’t that a weird coincidence? We got 1 less than the total number of players. Does this pattern hold for other tournament sizes?
The answer is yes. To prove it, we apply the technique of finding a clever bijection. It will make you feel like our computation was a complete waste of time, but if you did the exercise you’ll appreciate the elegance of this method that much more.
The primary observation is that every loser loses exactly one game. So if we want to count the number of games, we can instead count the number of losers. But there is only one player who is not a loser: the winner. Hence 999 games.
7 Here’s a neat fact I learned from John D. Cook: in the Middle Ages, people studied a “quadrivium” of mathematical arts: arithmetic, geometry, music, and astronomy. This followed the “trivium” of grammar, rhetoric and logic. So when I say a result is “trivial,” I’m not trying to insult anyone, but rather informing that no new ideas are needed above basic logic. The best and most pleasing mathematics takes a hard-seeming problem, and rephrases it in a clever way so that the proof is trivial.

49

Let’s rephrase that elegant argument in the language of sets. Let X be the set of games

and Y the set of players. Define a function f : X → Y by calling f (x) the loser of

game x. This function is not a surjection. Rather, the image f (X) is the subset L ⊂ Y of

losers. However, f is an injection (different games have different losers), and f defines

a bijection between X and L. This means that X and L have the same size, and the fact

that there is only one winner of the entire tournament means that |L| = |Y | − 1. So if

there are n players then there will always be n − 1 games.

To make sure you understand this argument, extend it to the case of a double-

elimination tournament. In double-elimination, you are ousted from the tournament once

you’ve lost two games, and a player who loses one game might still ultimately win the

tournament. In this case you won’t have an injection, but a so-called “double-cover” of

the set of players. What I mean by double-cover is that every y ∈ Y has a preimage

f −1(y) = {x ∈ X : f (x) = y} of size (almost) exactly 2. “Almost,” because the winner

may have lost zero games or one game. This also means you can’t count the number of

games exactly, but will be forced to provide bounds.

This general strategy for counting has applications any time you need to count or

estimate the size of a set. Imagine you want to estimate the number of homeless people in

a city, a problem the US Census Bureau faces regularly. You might implicitly count them

by observing the residual effects of their actions. This is precisely looking for functions

between sets that are close to bijections, or double- or triple-covers of the set you want

to count.

defHineerethiseaqnuoathnetirtym(aXg2n)i,firceeandt

example of “X choose

finding a clever two,” to be the

bijection. set of all

Given a set X let’s unordered pairs of

distinct elements of X. I.e.,

() X = {{x, y} : x, y ∈ X and x ̸= y}. 2
tdaoreipIctfhehXnmodoesiotseincatftwohfiorenmipotuaberljaestiecfcttousrolaff(rrno2sei)mzleienmantseeen=rttmso|sfiXnonf|X,onwb,?jjeueWcsdtteesi’n.tl8slosTstheihzoetewh.peIrbnosyiwbzwleoeamrodyfsio,(sf,(X2nc2a)a)bnbiisjywetch(etn2eico)no,numwtmhehabiuctephritwo’dsfoietwehqsaunaya’nstl to the quantity

1 + 2 + · · · + n − 1.

In fact, the bijection is easiest to understand by the picture in Figure 4.4. Here’s how

we read this picture. We’re setting n = 7 and calling the lightly shaded balls Y , and

cga:lliYng→the(X2n):sqguivaernesainnytbhaelllayst∈roYw,

X. you

The picture shows how to define a bijection draw two diagonals as in the picture and you

get g(y) as the pair of squares at the end of both diagonals. The picture should convince

you that two different choices of balls give you different diagonals, i.e., g is an injection.

8

In

general,

(n)
k

is

the

number

of

different

ways

to

choose

k

objects

from

an

n

object

set.

50

Figure 4.4: A picture proof that |(X2 )| = 1 + 2 + · · · + n − 1 when |X| = n. Each pair of squares in the bottom row corresponds to a unique ball in the triangular arrangement above it.

Likewise, given a pair of squares x1 ̸= x2 ∈ X, the inner diagonals meet at a ball y that maps under g back to (x1, x2). So g is a surjection, and together with being an injection this makes g a bijection.

Now we count: how many balls and squares are there? The last row has n−1 = 6 balls,

and each row Moreover, X

has has

one fewer n squares

ball than in it, so

|t(hX2e)r|ow=

u(nn2d)e. rnTehaethbiijte,cstoio|nY

| = 1+ tells us

2+· that

··+n these

− 1. two

values must be equal.

You may wonder: how can we use a picture as the central part of our proof? Didn’t we only prove that this bijection works for n = 7? Technically you’re right: no mathematician would consider a picture as a rigorous proof in and of itself. However, when the goal is to communicate the central nugget of wisdom in a proof, a small example with all the essential features of a general proof is often good enough. Consider one alternative. You could represent the balls as points inside R2. You’d need a generic way to construct coordinates for them, and a generic way to describe the diagonals. That’s a huge pain in the ass for something so simple! Every mathematician would agree it could be done but it would be a colossal waste of time to actually do it.

This is a common feature of more advanced mathematics. Mathematicians are constantly reading papers, and there is rarely enough time to verify all the details of every argument. If you’re not an official reviewer of the paper before it’s been published, it is usually enough to be convinced that something should be true, especially if the details are messy but clear, while focusing on the high level picture. An example with all the essential features of a general solution is an effective substitute. And this doubles for readers of mathematics too: finding a simple example with the essential features of a general solution, and testing claims on the example, is one of the best ways to read a proof !

51
4.3 Proof by Induction and Contradiction
Next we’re going to see two rigorous methods of proof that are used in all areas of math. The first is induction, but you’re likely familiar with it by a different name: recursion.
We understand recursion: a function is defined in such a way that it invokes itself for some smaller set of parameters, with a “base case” to process the smallest allowed parameters. The classic example is the Fibonacci sequence fib(n), defined recursively as

fib(n) = fib(n − 1) + fib(n − 2),
with fib(0) = fib(1) = 1. Most programmers have implemented some version of this function early on in their career, since it is a common instrument to teach recursion.9
Likewise, induction is a proof technique that allows you to prove a statement by invoking the same statement for smaller parameters, with a similar base case. One difficulty is identifying when and where induction is likely to be used. It’s usually when someone is trying to prove a statement which holds for all natural numbers (or all positive integers above some number). So a statement might look like, “For all integers n ≥ 6, the statement P (n) is true.” A proof by induction operates in two steps:

1. First show the base case, in this case that P (6) is true.
2. Second, do the inductive step, where one uses the assumption that P (n) is true to prove that P (n + 1) is true. Equivalently, one can use P (n − 1) to prove P (n).

Just like with recursion, you get a chain of proofs: P (6) implies P (7) implies . . . im-

plies P (n) for any n you like. One bit of terminology: one often invokes the inductive

hypothesis, which is the assumption that P (n) is true. It’s helpful when P (n) is cumber-

some to restate. Let’s use induction

for

a

second

proof

that

(n)
2

=

1

+

2

+

·

·

·

+

n

−

1.

Proof. Call the statement to be proved P (n). We prove this by induction for n ≥ 2. For the base case10 n = 2, we need to prove

() 2
P (2) : = 1 2

We

argue

(2)
2

is

trivially

1.

There

is

only

one

way

to

choose

two

items

from

a

set

of

two items. Now assume the inductive hypothesis P (n) holds:

9 It also displays some of the variety of programming approaches. Fibonacci sequences can be computed inplace with an array, using recursion (and hopefully memoization), or with a closed-form formula. Each has advantages and disadvantages that show how we think about tradeoffs in software. 10 When n = 0 or 1 we are asking how many ways there are to choose two things from a set of fewer than two things. According to our definition this is zero (which you saw if you wrote your test cases starting from the simplest ones), and one usually calls an empty summation to be zero. But the first n that’s not “vacuously” true is n = 2 so we start there.

52

() n
P (n) : 2 = 1 + 2 + · · · + n − 1.

We must now prove that P (n + 1) follows, i.e.:

() n+1
P (n + 1) : 2 = 1 + 2 + · · · + n.

Take

the

set

X

=

{1,

2, .

.

.

,n

+

1}

of

size

n

+

1,

and

consider

the

set

(X)
2

of

ways

to

pick two elements from X. Note that we are using numbers as elements of X instead of

“arbitrary objects.” We might have instead called them “ball 1, ball 2, ball 3” and discuss

thhoewmsmealvneys.wNaoyws t(oX2s)eliescat our (inductively assumed)

ftsowertomobufalsalilzsfeofrr(o(nmn2+2)1.a)Pbainicnkd.1aw1neyFowerlaesnmitmetnpotlieocxifptyXrew,sssea’tylhl enuss+eize1th,ieannntdeurdmmebfsienoresf

Y to be the set that remains after removing that element from X.

Y = X − {n + 1} = {1, 2, . . . , n}.

Now

let’s

split

the

elements

of

(X)
2

into

two

parts:

the

part

where

both

chosen

elements

are in Y , and the part where one of the two chosen elements is n + 1. Since there are

no other options and no parts to count the size of

o(vX2e)r.lap

between

the

The first part, where both chosen elements

two are

options, we can add

in Y , has size

(Y )
2

the sizes of both = (n2), which by

the inductive hypothesis is 1 + 2 + · · · + n − 1. The second part, where one of the chosen

elements is guaranteed to be n + 1, has size n by the following reasoning: if you had to

choose n + 1 as one of the two elements, then there are only n remaining choices for the

second element.12

Adding up the sizes of the two parts gives exactly

1 + 2 + · · · + (n − 1) + n, which is what we set out to prove.

Is the proof still a bit write down the elements

mofu(rXk2y)?.

Go back and set n = 4, Follow the steps through

X the

= {1, 2, 3, 4, 5}, inductive step of

and then the proof

on this example, and your understanding of the general case will feel like an epiphany.

Interestingly, proof by induction has a bad reputation in mathematics. The reason is

that proofs by induction often convey little insight to the reader. As the mathematician

Gian-Carlo Rota once said, “If we have no idea why a statement is true, we can still prove

it by induction.” Be that as it may, induction is a central tool for proving theorems.

11 The process of choosing a numbering of elements in a set is called an enumeration.

12

If you read f ({a, b}) =

this part carefully, min(a, b). Then f

you’ll notice we’re defining a bijection. is a bijection between Y and the subset {S

O∈n(eX2c)an:

define the mapping n + 1 ∈ S}.

as

53
The second proof technique is called “proof by contradiction.” There’s a simple puzzle I often use to illustrate the technique.
You’re at a party. You’re chatting with your friend, and out of curiosity you ask how many friends he has at the party. He counts them up, there are five, and you realize that you also have five friends at the party. What a coincidence! Putting on your mathematician hat, you poll everyone at the party and you’re shocked to find that a few other people also have five friends at the party. The puzzle is: is this true of every party? Maybe not five exactly, but will there always be at least two people with the same number of friends who are at the party?
Before I give the solution by contradiction, let’s iron out what I mean by “friendship.” I insist that friendship is symmetric: you can’t be friends with someone who is not friends with you. And moreover you can’t be friends with yourself.13
You’ll appreciate the answer to this problem best if you spend some time trying to solve it first.
Back already? The answer is yes, there will always be a pair of people with the same number of friends. The technique we use to prove it is called proof by contradiction. It works by assuming the opposite of what you want to prove is true, and using that assumption to deduce nonsense.
Proof. Suppose for the sake of contradiction that there is some party where everybody has a different number of friends at the party. Say the party has n > 1 people, then everyone must have between zero and n − 1 friends. Since there are n people and n different numbers between zero and n − 1, we can map each person to the number of friends they have, and this map will be a bijection. Now here comes the contradiction: someone must have zero friends at the party, and someone must have n − 1 friends, i.e., someone must be friends with everyone. But the person who is friends with everyone must be friends with the person that has no friends! The only way to resolve this contradiction is if the original assumption is actually false. That is, there must be two people with the same number of friends.
This is how every proof by contradiction goes, but they’re usually a bit more concise. They always start with, “Suppose to the contrary” to signal the method. And there is no warning when the contradiction will come. A proof writer usually just states the contradiction and follows it with “which is a contradiction,” ending the proof.14
The point of a proof by contradiction is to get an object with a property that you can work with. If you’re trying to prove that no object with some special property exists, a proof by contradiction gives you an instance of such an object, and you can use its special
13 Looking forward to Chapter 6 on graph theory, we’re saying that the social connections at our “party” form a simple, undirected graph.
14 A professor of mine had a funny refrain to end his proofs by contradiction. If, say, x was assumed to be prime, he’d arrive at a contradiction and say, “and this is very embarrassing for x because it was claiming to be prime.”

54
property to go forward in the proof. In this case the object was a special friendship count among partygoers, and in the next section we’ll apply the same logic to “marriages.”
For those readers who are interested in a bit more details about what makes a mathematical proof, or how to approach proving things, in this second edition I added two appendices that may help. Appendix B contains a bit more details about the formalities underlying proofs, along with a section at the end called “How does one actually prove things?” Appendix C contains a list of books under “Fundamentals and Foundations” that cover the basics of set theory, proofs, and problem solving strategies. Readers of the first edition have told me that following along with these books has helped immensely.
4.4 Application: Stable Marriages
Now we’re ready to apply the tools in this chapter to implement a Nobel Prize-winning algorithm for the stable marriage problem. The problem is set up as follows. Say you have n men and n women. Your end goal is to choose who should marry whom. Samesex marriages are excluded, not for political or religious reasons but because it’s a more difficult problem. So if we call M the men and W the women, our output will be a bijection M → W describing the marriages (or equivalently W → M ). I will freely switch between “bijection” and “marriage” in this section.
Of course, we don’t just want any bijection. This is where the “stable” part comes in. We want to choose the marriage so that everyone is happy in some sense. Let’s make this precise. Say that each man has a ranking of the women, mathematically a bijection W → {1, 2, . . . , n}, with 1 being the most preferred and n being the least. In other words, if we call the bijection p then p(w) < p(x) means that this particular man prefers woman w over woman x. Likewise, each woman has a ranking of the men M → {1, 2, . . . , n}. Now we obviously can’t ensure that every woman gets her top choice and vice versa; the men could all prefer the same woman. So we need a subtler notion of happiness: that no (man, woman) pair mutually prefer each other over their assigned partners.
Marriages are a colorful, if somewhat silly, setting for this problem. Realistically, this algorithm applies to different sorts of ‘marriage’, such as the assignment of a student to an apprenticeship. A widely known example is medical residency,15 in which medical students work in a hospital before becoming a doctor. This is the perfect example of a market in which money should not play a part. As a society we want all our hospitals filled with talented apprentices. We don’t want the students with the richest parents or best connections to get the most prestigious positions in the best cities, while poorer areas suffer. We want to spread the talent around. So we need a market with a protocol that respects student and hospital preferences in a way that no (student, hospital) pair is incentivized to make their own arrangements. This version of the problem is a natural extension of the marriage version. So we’ll explore marriages in depth here, and dive into medical residency matching in the exercises.
Define a ranking function as a bijection between {1, 2, . . . , n} and either M or W .
15 In the US, it’s the National Medical Residency Matching Program.

55
Before I state what “not cheating” means mathematically for the marriage problem, I encourage you to write down a small example of sets M, W of size n = 4, rankings prefw(m) for each w ∈ W and prefm(w) for each m ∈ M , and a candidate marriage f : M → W . I’ll call the marriage from the women’s perspective f −1 : W → M .
What I mean by “no mutually desired cheating” is the following.
Definition 4.14. A bijection f : M → W is called stable if there is no pair m ∈ M and w ∈ W such that the following two conditions hold:
1. f (m) ̸= w, i.e., the two are not matched by f .
2. The pair m and w mutually prefer each other over their assigned matches.16 I.e., both prefm(w) < prefm(f (m)) and prefw(m) < prefw(f −1(w)).
In other words, the bijection is called stable if there is no pair of people with mutual incentive to cheat on their assigned spouses. This is not to say cheating can’t happen, but if it does one of the two involved will be “lowering their standards.”
The algorithmic question is, given lists of preferences as input, can we find a stable marriage? Can we even guarantee a stable marriage will exist for any set of preferences? The answer to both questions is yes, and it uses an algorithm called deferred acceptance.
Here is an informal description of the algorithm. It goes in rounds. In each round, each man “proposes” to the highest-preferred woman that has not yet rejected him. On the other side, each woman holds a reference to a man at all times. If a woman gets new proposals in a round, she immediately rejects every proposer except her most preferred, but does not accept that proposal. She “defers” the acceptance of the proposal until the very end.
The rejected men are sad, but in the next round they recover and propose to their next most preferred woman, and again the women reject all but one. The men keep proposing until every man is tentatively held by some woman, or until all women have rejected them. That is not a happy place to imagine. But actually, the theorem that we’ll prove says that this process always ends with each woman holding onto a man, and no men are left out; the set of women’s held picks forms a stable bijection.
Before we prove that the algorithm works, let’s state it more formally in Python code. A complete working program is available on this book’s Github repository.17 In the interest of generality, I’ve defined classes Suitor and Suited to differentiate: Suitors propose to Suiteds.
16 Remember, a lower number in pref means a higher preference! 17 See pimbook.org

56
class Suitor: def __init__(self, id, preference_list): self.preference_list = preference_list self.index_to_propose_to = 0 self.id = id
def preference(self): return self.preference_list[self.index_to_propose_to]
def post_rejection(self): self.index_to_propose_to += 1
The Suitor class is simple. Instances are uniquely identified by an id, which I’m defining to be the index in a global list of Suitors. A Suitor has a preference_list, which is a list of Suited ids sorted from most preferred to least preferred. The index_to_propose_to variable simultaneously counts the number of rejections and which index in the preference_list to use for the next proposal.
A bit more complicated is the Suited class:
class Suited: def __init__(self, id, preference_list): self.preference_list = preference_list self.held = None self.current_suitors = set() self.id = id
def reject(self): """Return the subset of Suitors in self.current_suitors to reject, leaving only the held Suitor in self.current_suitors. """ if len(self.current_suitors) == 0: return set()
self.held = min( self.current_suitors, key=lambda suitor: self.preference_list.index(suitor.id))
rejected = self.current_suitors - set([self.held]) self.current_suitors = set([self.held])
return rejected
def add_suitor(self, suitor): self.current_suitors.add(suitor)
Here current_suitors are the new proposals in a given round, and held is the Suited’s held pick. In the method reject, a Suited looks at all her current suitors, chooses the best in her preference_list, and returns all others as rejected Suitors.
Finally, we have the main routine for the deferred acceptance algorithm.

57
def stable_marriage(suitors, suiteds): """ Construct a stable marriage between Suitors and Suiteds. """ unassigned = set(suitors)
while len(unassigned) > 0: for suitor in unassigned: next_to_propose_to = suiteds[suitor.preference()] next_to_propose_to.add_suitor(suitor) unassigned = set()
for suited in suiteds: unassigned |= suited.reject() # python set union operator
for suitor in unassigned: suitor.post_rejection() # have some ice cream
return dict([(suited.held, suited) for suited in suiteds])
The dictionary at the end is the type we use to represent a bijection. Now let’s prove this algorithm always produces a stable marriage.
We will argue that the algorithm terminates by monotonicity. Here’s what I mean by that: say you have a sequence of integers a1, a2, . . . which is monotonically increasing, meaning that a1 < a2 < · · · . Say moreover that you know none of the ai are larger than 50 (ai is bounded from above) but each ai+1 ≥ ai + C for some constant C > 0. Then it’s trivial to see that either the sequence stops before it hits 50, or eventually it hits 50.
To show an algorithm terminates, you can cleverly choose an integer at for each iteration t of the core loop, and show that at is monotonically increasing (or decreasing) and bounded. Then show that if the algorithm hits the bound then it’s forced to finish, and otherwise it finishes on its own.
Theorem 4.15. The deferred acceptance algorithm always terminates, and the bijection produced at the end is stable.
Proof. For the deferred acceptance algorithm we have a nice monotonic sequence. For round t set at to be the sum of all the Suitor’s index_to_propose_to variables. Recall that this variable also represents the number of rejections of each Suitor. Since there are exactly n preferences in the list and exactly n Suitors, we get the bound at ≤ n2 (each Suitor could be at the very end of their list; come up with an example to show this can happen!).
Moreover, in each round one of two things happens. Either no Suitor is rejected by a Suited and by definition the algorithm finishes, or someone is rejected and their index_to_propose_to variable increases by 1, so at+1 ≥ at +1. Now in the case where all the Suitors are at the end of their lists, that means that every Suited was proposed to by every Suitor. In other words, each of the Suiteds gets their top pick: they only reject when they see a better option, and they got to consider all proposals! Clearly the algorithm will stop in this case.

58
Now that we’ve shown the algorithm will stop, we need to show the bijection f produced as output is stable. The definition of stability says there is no Suitor m and Suited w with mutual incentive to cheat, so for contradiction’s sake we’ll suppose that the f output by the algorithm does have such a pair, i.e., for some m, w, prefm(w) < prefm(f (m)) and prefw(m) < prefw(f −1(w)).
What had to happen to w during the algorithm? Well, m ended up with f (m) instead of w, and if prefm(f (m)) > prefm(w), then m must have proposed to w at some earlier round. Likewise, the held pick of w only increases in quality when w rejects a Suitor, but w ended up with some Suitor f −1(w) while prefw(m) < prefw(f −1(w)). So at some point in between being proposed to by m and choosing to hold on to f −1(w), w had to go the wrong way in her preference list, contradicting the definition of the algorithm.
We close with an example run:
>>> suitors = [ Suitor(0, [3, 5, 4, 2, 1, 0]), Suitor(1, [2, 3, 1, 0, 4, 5]), Suitor(2, [5, 2, 1, 0, 3, 4]), Suitor(3, [0, 1, 2, 3, 4, 5]), Suitor(4, [4, 5, 1, 2, 0, 3]), Suitor(5, [0, 1, 2, 3, 4, 5]),
] >>> suiteds = [
Suited(0, [3, 5, 4, 2, 1, 0]), Suited(1, [2, 3, 1, 0, 4, 5]), Suited(2, [5, 2, 1, 0, 3, 4]), Suited(3, [0, 1, 2, 3, 4, 5]), Suited(4, [4, 5, 1, 2, 0, 3]), Suited(5, [0, 1, 2, 3, 4, 5]), ] >>> stable_marriage(suitors, suiteds) { Suitor(0): Suited(3), Suitor(1): Suited(2), Suitor(2): Suited(5), Suitor(3): Suited(0), Suitor(4): Suited(4), Suitor(5): Suited(1), }
4.5 Cultural Review
1. Sets and functions between sets are a modeling language for mathematics.
2. Bijections show up everywhere, and they’re a central tool for understanding the same object from two different perspectives.
3. Mathematicians usually accept silent type conversions between sets when it makes sense to do so, i.e., when there is a very clear and natural bijection between the two

59
sets.
4. Induction is a similar idea to recursion in programming, but applied to proofs.
5. A picture or example that captures the spirit of a fully general proof is often good enough.
4.6 Exercises
4.1. Write down examples for the following definitions. A set A (finite or infinite) is called countable if it is empty, or if there is a surjection N → A. The power set of a set A, denoted 2A, is the set of all subsets of A. For two sets A, B, we denote by BA the set of all functions from A to B. This makes sense with the previous notation 2A if we think of “2” as the set of two elements 2 = {0, 1}, and think of a function f : A → {0, 1} as describing a subset C ⊂ A by sending elements of C to 1 and elements of A − C to 0. In other words, the subset defined by f is C = f −1(1).
4.2. Recall that for A ⊂ X, AC sometimes denotes the complement18 of A in X. Prove De Morgan’s law for sets, which for A, B ⊂ X states that (A ∩ B)C = AC ∪ BC, and (A ∪ B)C = AC ∩ BC. Draw the connection between this and the corresponding laws for negations of boolean formulas (e.g., not (a and b) == (not a) or (not b)). 4.3. Look up a formula online for the quantity (nk), the number of ways to choose k elements from a set of size n, in terms of factorials m! = 1 · 2 · 3 · · · · · m. Find a proof that explains why this formula is true.
4.4. Look up a statement of the pigeonhole principle, and research how it is used in proofs.
4.5. Prove that N × N is countable, i.e., there is a surjection N → N × N.
4.6. For each n ∈ N, let An be a countably infinite set, such that all the An have empty intersection. Prove that the union of all the An is countable. Hint: use the previous problem.
4.7. Is there a bijection between 2N and the interval [0, 1] of real numbers x with 0 ≤ x ≤ 1? Is there a bijection between (0, 1] = {x ∈ R : 0 < x ≤ 1} and [1, ∞) = {x ∈ R : x ≥ 1}?
4.8. I would be remiss to omit Georg Cantor from a chapter on set theory. Cantor’s Theorem states that the set of real numbers R is not countable. The proof uses a famous technique called “diagonalization.” There are many expositions of this proof on the internet ranging in difficulty. Find one that you can understand and read it. The magic of this
18 Note, this ambiguous notation conflicts with the previous exercise, and takes a different meaning here. ABC: Always Be Contextualizing.

60
theorem is that it means there is more than one kind of infinity, and some infinities are bigger than others.
4.9. The principle of inclusion-exclusion is a technique used to aid in counting the size of a set. Look for a description of this principle (it is a family of theorems) and find ways it is used to help count.
4.10. There is a large body of mathematics related to configurations of sets with highly symmetric properties. Let n, k, t be integers. A Steiner system is a family F of size-k subsets of an n-element set S, say {1, . . . , n}, such that every size-t subset of S is in exactly one member of F . For example, for (n, k, t) = (7, 3, 2), the corresponding Steiner system is a choice of triples in {1, 2, 3, 4, 5, 6, 7}, such that every pair of numbers is in exactly one of the chosen triples. Find an explicit description of a (7, 3, 2)-system.
4.11. Continuing the previous exercise, a Steiner system may not exist for every choice of n > k > t. Prove that if an (n, k, t)-system exists, then so must an (n − 1, k − 1, t − 1)system. Determine under what conditions on n may a Steiner (n, 3, 2)-system exist.
4.12. Continuing the previous exercise, the non-existence of Steiner systems for some choices of n suggests a modified problem of finding a minimal size family F of size-k subsets such that every t-size subset is in at least one set in F . For (n, k, t) arbitrary, find a lower bound on the size of F . Try to come up with an algorithm that gets close to this lower bound for small values of k, t.
4.13. A generalization of Steiner systems are called block designs. A block design F is again a family of size-k subsets of X = {1, . . . , n} covering all size-t subsets, but also with parameters controlling: the number of sets in F that contain each x ∈ X, and the number of sets covering each size-t subset (i.e., it can be more than one). Block designs are used in the theory of experimental design in statistics when, for example, one wants to test multiple drugs on patients, but the outcome could be confounded by which subset of drugs each patient takes, as well as which order they are taken in, among other factors. Research how block designs are used to mitigate these problems.
4.14. A Sperner family is a family F of subsets of {1, . . . , n} for which no member of F is a subset of any other member of F . Sperner’s theorem gives an upper bound on the maximum size of a Sperner family. Find a proof of this theorem. There are multiple proofs, though one of them has at its core an inequality called the Lubell–Yamamoto–Meshalkin inequality, which is proved using a double-counting argument (and Exercise 4.3).
4.15. The formal mathematical foundations for set theory are called the ZermeloFraenkel axioms (also called ZF-set theory, or ZFC). Research these axioms and determine how numbers and pairs are represented in this “bare metal” mathematics. Look up Russell’s paradox, and understand why ZF-set theory avoids it.

61
4.16. A fuzzy set S ⊂ X is a function mS : X → [0, 1] that measures the (possibly partial) membership of an x ∈ X in the set S. One can think of mS(x) as representing the “confidence,” or “probability” that an x is in S. Show that every set can be represented as a fuzzy set. Research fuzzy sets, and determine a sensible definition for the cardinality of a fuzzy set.
4.17. Write a program that extends the deferred acceptance algorithm to the setting of “marriages with capacity.” That is, imagine now that instead of men and women we have medical students and hospitals. Each hospital may admit multiple students as residents, but each student attends a single hospital. Find the most natural definition for what a stable marriage is in this context, and modify the algorithm in this chapter to find stable marriages in this setting. Then implement it in code. See the chapter notes for historical notes on this algorithm.
4.18. Come up with a version of stable marriages that includes the possibility of same-sex marriage. This variant is sometimes called the stable roommate problem. In this setting, there is simply a pool of people that must be paired off, and everybody ranks everyone else. Perform the full modeling process: write down the definitions, design an algorithm, prove it works, and implement it in code.
4.19. Is the stable marriage algorithm biased? Come up with a concrete measure of how “good” a bijection is for the men or the women collectively, and determine if the stable marriage algorithm is biased toward men or women for that measure.
4.7 Chapter Notes
Residency Matching
Medical residency matching was the setting for one of the major accomplishments of Alvin Roth, currently an economics professor at Stanford. He applied this and related algorithms to kidney exchange markets and schooling markets. Along with Lloyd Shapley, one of the original designers of the deferred acceptance algorithm, their work designing and implementing these systems in practice won the 2012 Nobel Prize in economics. Measured by a different standard, their work on kidney markets has saved thousands of lives, put students in better schools, and reduced stress among young doctors.
Roth gives a fascinating talk19 about the evolution of the medical residency market before he stepped in, detailing how students and hospitals engaged in a maniacal daylong sprint of telephone calls, and all the ways unethical actors would try to game the protocol in their favor.
19 https://youtu.be/wvG5b2gmk70

62
Marriage
Please don’t treat marriage as an allocation problem in real life. I hope it’s clear that the process of doing mathematics—and the modeling involved in converting real world problems to sets—involves deliberately distilling a problem down to a tractable core. This often involves ignoring features that are quite crucial to the real world. A quote often attributed to Albert Einstein speaks truth here, that “a problem should be made as simple as possible, but no simpler.” Indeed, the unstated hope is that by analyzing the simplified, distilled problem, one can gain insights that are applicable to the more complex, realistic problem. Don’t remove the core of the problem when phrasing it in mathematics, but remove as much as you need to make progress. Then gradually restore complexity until you have solved the original problem, or fail to make more progress. Marriage is used as a communication device for this particular simplification. It’s not the problem being solved.
The idea that one can reduce complex human relationships to a simple allocation problem is laughable, and borderline offensive. In the stable marriage problem the actors are static, unchanging symbols that happen to have preferences. In reality, the most important aspect of human relationships is that people can grow and improve through communication, introspection, and hard work.

Chapter 5
Variable Names, Overloading, and Your Brain
Math is the art of giving the same name to different things.
– Henri Poincaré
Programmers often complain about how mathematicians use single-letter variable names, how they overload and abuse notation, and how the words they use to describe things are essentially nonsense words made up for the sole purpose of having a new word. This causes bizarre sentences like “Map each co-monad to the Hom-set of quandle endomorphisms of X.” I just made that up, by the way, though each word means something individually. One question programmers rarely ask is why mathematicians do this. Is it to feign complexity? Historical precedence? A hint of malice?
Of course there are bad writers out there, along with people who like to sound smart. There is certainly a somewhat unhealthy pattern of mathematicians who think a dose of emotional and intellectual pain is the best way to learn. But that’s true of every field. I want to take a quick moment to explain the mathematician’s perspective. As you’ve probably guessed by now, a central issue is culture. I won’t try to convince you that this is the only explanation, but rather show you a different reasonable angle on the debate.
In producing mathematics, the mathematician has two goals: discover insight about a mathematical thing, and then communicate that truth to others in an intuitive and elegant way. While the second goal implies that mathematicians do care about style, what makes a proof or mathematical theory elegant is first and foremost the degree to which it facilitates understanding.
On the other hand, good software is measured (after it’s deemed to work) by maintainability, extensibility, modularization, testability, robustness, and a whole host of other metrics which are primarily business metrics. You care about modularization because you want to be able to delegate work to many different programmers without stepping on each other’s toes. You want extensibility because customers never know what features they actually want until you finish designing the features they later decide are no good. You want to ensure that your software is idiot-proof because your company just hired three idiots. These metrics are good targets because they save time and money.
Mathematicians don’t experience these scaling problems to the same degree of tedium because mathematics isn’t a business. Mathematics isn’t idiot-proof because the success
63

64
of a mathematical theory doesn’t depend on whether the next idiot that comes along understands it.1 In fact, mathematical sophistication in the business world is extraordinary. And while having tests (providing worked-out examples) is a sign of a good mathematical writer, there’s no manager staking their job or a salary bonus on the robustness of a bit of notation. If someone gets confused reading your paper, it doesn’t siphon money out the window the same way it does at Twitter during an outage. There’s just not the same sense of urgency in mathematics.
I should make a side note that saying “mathematics isn’t a business” is overly naive. Mathematicians need to make money just like everyone else, and this manifests itself in some strange practices in academic journals, conferences, and the multitude of committees that decide who is worth hiring and giving tenure. Mathematicians, like folks in industry, bend over backwards to game (or accommodate) the system. But all of that is academia. What I’m talking about is established mathematics which has been around for decades, or even centuries, which has been purified of political excrement. This applies to basically every topic in this book.
That’s not to say that mathematics isn’t designed to scale. To the contrary, the invention of algebraic notation was one of humanity’s first massively scalable technologies. On the other end of the spectrum, category theory—which you can think of as a newer foundation for math roughly based on a new notation that goes beyond what sets and functions can offer—provides the foundation for much of modern pure mathematics. It’s considered by many as a major advancement.
Rather than being designed to scale to millions of average users, mathematics aims to scale far up the ladder of abstraction. Algebra—literally, the marks on paper—boosted humanity from barely being able to do arithmetic through to today’s machine learning algorithms and cryptographic protocols. Sets, which were only invented in the late 1800’s, hoisted mathematical abstraction even further. Category theory is a relative rocket fuel boosting one through the stratosphere of abstraction (for better or worse).
The result of this, as the argument goes, is that mathematicians have optimized their discourse for more relevant metrics: maximizing efficiency and minimizing cognitive load after deep study.
Let me map out a few areas where this shows up:
• Variable names
• Operator overloading
• Sloppy notation
Variable names. Variable names are designed to transmit a lot of information: types, behavior, origin, and more. Programmers do this as well, but the conventions differ. Every mathematician knows that n is a natural number, and that f is a function. Or at least,
1 I mean this in a practical sense, not a social sense. If your math is so hard to understand that nobody but you learns it, it will be lost to history. But from a practical standpoint, calculus doesn’t stop being a good foundation for a video game engine just because the programmer doesn’t understand the math.

65
they know that when they see these letters out of context, they should at least behave like a natural number and a function, respectively. Seeing n(f ) out of context would momentarily startle me, though I can imagine situations making it appropriate.2 Similarly, if f is a function and you can use f to construct another function in a “canonical” (forced, unique) way, then a mathematician might typically adorn f with a star like f ∗. Two related objects often inhabit the same letter with a tick, like x and x′. Even if you forget what they represent, you know they’re related.
Every field of mathematics has its own little conventions that help save time. This is especially true since mathematics is often done in real time (talking with colleagues in front of a blackboard, or speaking to a crowd). The time it takes to write f ∗ while saying out loud “the canonical induced homomorphism,3” is much faster than writing down InducedHomomorphismF in ten places. And then when you need an h∗ to compose f ∗h∗, half of the characters help you distinguish it from h∗f ∗. Whereas determining the order of
InducedHomomorphismF.compose(InducedHomomorphismH)
is harder with more characters, and Gauss forbid you have to write down an identity about the composition of three of these things! A single statement would fill up an entire blackboard, and you’d never get to the point of your discussion.
More deeply, there is often nothing more a name can do to elucidate the nature of a mathematical object. Does saying f ∗ really tell you less about what an object is than something like InducedF? It’s related to f , its definition is somehow “induced,” and what? The further up the ladder of abstraction you go, the more contrived these naming conventions would get. Rather than say, for example, FirstCohomologyGroupOfInvertibleSubsheavesOfX, you say H1(X, O∗) because you would rather claw your eyes out than read the first thing, which could easily be just one part of a larger expression, with maybe ten more similar copies of the notation. For example, here is an actual snippet from a chapter of a graduate algebra textbook cheekily titled, “Algebra: Chapter 0.”
νL : L0F(M ) = H0(C(F)(P •)) → F(M )
It is a bit ridiculous that L and L refer to different mathematical things, despite being the same letter. Here L is an object and L (short for “left”) describes a kind of function. But this is a trade-off. You can use long words that make it difficult to put everything you want to say in front of your face at the same time—thus making it harder to reason. Or, you can use fonts and foreign alphabets to differentiate concepts. Sans-serif is for one purpose, the curly-scripty font is for another.
2 For example, n could represent some integer-valued property of a function, like the so-called winding number. 3 For example. You don’t need to know what a homomorphism is.

66
Why not invent a better name? They do! Just later. In fact, because the expression H1(X, O∗) is so important in the study of algebraic geometry, it was renamed to Pic(X) named after Picard who studied them. But it might take decades to get to the point where you realize this object is worth giving a name, and in the mean time you just can’t use 80-character names and expect to get things done.
One reason mathematicians can get away with single-character variable names is that they spend so much time studying them. When a mathematician comes up with a new definition, it’s usually the result of weeks of labor, if not months or years! These objects aren’t just variables in some program whose output or process is the real prize. The variables represent the cool things! It’s as if you returned to rewrite and recheck and retest the same twenty-line program every day for a month. You’d have such an intimate understanding of every line that you could recite it while drunk or asleep. Now imagine that the intimate understanding of every line of that program was the basis of every program you wrote for the next year, and you see how ingrained this stuff is in the mind of a mathematician.
Mathematicians don’t just write a proof and file it away under “great tool; didn’t read.” They constantly revisit the source. It’s effective to gild meaning and subtext into the bones of single letters, because after years you don’t have to think about it any more. It eliminates the need to keep track of types. Clearly f is a function, z is probably a complex variable, and everyone knows that ℵ0 is the countably infinite cardinal. If you use b and β in the same place, I will know that they are probably related, or at least play analogous roles in two different contexts, and that will jump-start my understanding in a way that descriptive variable names do not.
Operator overloading. Much of what I said above for variable names holds for operator overloading too. One key feature that stands out for operator overloading is that it highlights the intended nature of an operation.
We’ll get to this more in Chapter 9, but mathematicians use just a handful of boolean logic operations for almost everything. There are the standard inequalities and equalities. Then there are operators that look like ∼= or ≃ that represent equality “up to some differences that we don’t care about.” In Java terms, mathematicians regularly roll their own .equals() methods, with proofs that their notions behave. Specifically, they prove it satisfies the properties required of an equivalence relation, which is the mathematical version of saying “equals agrees with hashing and toString.”
And so typically mathematicians will drop whatever the original operator symbol was and replace it with the equal sign. The core properties of = are respected even if not identity. We’ll see this in detail in Chapters 9 and 16, but the same idea goes behind the reuse of standard arithmetic operations like addition and multiplication. It suggests what behavior to expect from the operation. For example, it is considered bad form to use the + operator for an operation that doesn’t satisfy a + b = b + a for every choice of a and b—the commutative property—because this is true of addition. Many multiplications are not commutative, such as matrix multiplication, and so a generic multiplication uses × or juxtaposition to signal this.
With this in mind it’s the mathematician’s turn to criticize programmers. For example,

67
reading programming style guides has always amused me. It makes sense for a company to impose a style guide on their employees (especially when your IDE is powerful enough to auto-format your programs) because you want your codebase to be uniform. In the same way, a mathematician would never change notational convention in the same paper, except to introduce a new notation. But to have a programming language designer declare style edicts for the entire world, like the following from the Python Style Guide, is just ridiculous:
Imports should usually be on separate lines, e.g.:
Yes: import os import sys
No: import sys, os

Okay, so you have an arbitrary idea of what a pretty program looks like, but wouldn’t

you rather spend that time and energy on actually understanding and writing a good pro-

gram? Besides, if there were truly a good reason for the first option, why wouldn’t the

language designer just disallow the second option in the syntax? Of course, programmers

get away with it because they use automated tools to apply style guides automatically.

It’s much harder to do that in math, where the worst offenses are not resolvable (or dis-

coverable!) from syntax alone. Still, I don’t doubt there could be some progress made in

automating some aspects of a mathematical style guide.

In an ideal world, a compiler would see how I use the “stdout” variable and be able to

infer the semantics from a shared understanding about the behavior of standard output in

basically every program ever. This would eliminate the need to declare module imports or

even define stdout! That’s basically how math solves the problem of overloaded operators.

There is a clarifying and rigorous definition somewhere, but if you’ve forgotten it you can

still understand the basic intent and infer appropriate meaning.

Sloppy notation. This is probably the area where mathematicians get the most flak,

andTawkhe esruemthmeayticoonulndoetaastiiolyn,imthpero∑ve

their communication with those aiming to learn. symbol. Officially this symbol has three parts: an

isnudmemx evda.riaSbole∑, a9i=m0i2niim+um1

and maximum value for the index, and an expression being sums the first ten positive odd integers. This is the kind of

syntactical rigidity that makes one itch to write a parser.

However, this notation is so convenient that it’s been overloaded to include many other

syntax forms. A simple one is to replace the increment-by-one range “all elements in this set” notation. For example, if B is a set, you can

owfriintete∑gebr∈s Bwbit2htoa

sum the squares of all elements of B.

But wait, there’s more! It often happens that B has an implicit, or previously defined

o∑rdiebr2i

of the elements (“the sum over

B = {b1, . relevant i”)

. . , bn}, in which with no mention

case one takes the liberty of writing of the set in the (local) syntax at all!

As we saw in Chapter 2 with polynomials, one can additionally add conditions below

the index to filter only desired values, or even have the constraint implicitly define the

68

variable range! So you can say the following to sum all odd bi ∈ B

∑ b2i + 3
bi odd

The reason this makes any sense is because, as is often the case, the math notation often comes from speech. You’re literally speaking, “over all bi that are odd, sum the terms b2i + 3.” Equations are written to mimic conversation, not the other way around. You see it when you’re in the company of mathematicians explaining things. They’ll write their formulas down as they talk, and half the time they’ll write them backwards! For a sum, they might write the body of the summation first, then add the sum sign and

the index. Because out loud they’ll be emphasizing the novel parts of the equation, filling the surrounding parts for completeness.
Finally, the things being summed need not be numbers, so long as addition is defined

for those objects and it satisfies the properties addition should satisfy. In Chapter 10 we’ll

s∑ee

a new kind of summation for vectors, and it will in that context. The summing operation needs

be to

clear have

why it’s okay for us to reuse properties that result in the

final sum not depending on the order the operations are applied.

Another prominent example of summation notation being adapted for an expert audi-

ennoctaetiios nthteheso∑-caslylemdbEoilnissteitisnelnf oitmatpioline.d

This notation from context!

is popular in For example,

physics. In rather than

Einstein write

∑n y = akxk,
k=1
The sum and the bounds on the indices are implied from the presence of the indices, as in

y = akxk.
To my personal sensibilities this is extreme. But I can’t fault proponents of the abuse when they find it genuinely useful.
What makes all of this okay is when the missing parts are fixed throughout the discussion or clear from context. What counts as context is (tautologically) context dependent. More often than not, mathematicians will preface their abuse to prepare you for the new mental hoop. The benefit of these notational adulterations is to make the mathematics less verbose, and to sharpen the focus on the most important part: the core idea being presented. These “abuses” reduce the number of things you see, and as a consequence reduce the number of distractions from the thing you want to understand.

Chapter 6
Graphs
One will not get anywhere in graph theory by sitting in an armchair and trying to understand graphs better. Neither is it particularly necessary to read much of the literature before tackling a problem: it is of course helpful to be aware of some of the most important techniques, but the interesting problems tend to be open precisely because the established techniques cannot easily be applied.
– Tim Gowers
So far we’ve learned about a few major mathematical tools:
• Using sets for modeling.
• Proof by contradiction, induction, and “trivial” proofs.
• Bijections for counting.
In this chapter we won’t learn any new tools. Instead we’ll apply the tools above to study graphs. Most programmers have heard about graphs before, perhaps in the context of breadth-first and depth-first search or data structures like heaps. Instead of discussing the standard applications of graphs to computer science, we’ll focus on a less familiar topic that still finds use in computer science: graph coloring.
In addition to having interesting applications, graph coloring has important theorems one can prove using only the tools we’ve learned so far. The main theorem we’ll prove in this chapter is that every planar graph is 5-colorable (I will explain these terms soon). So think of this chapter as a sort of checkpoint exam. If you’re struggling to understand the definitions, theorems, and proofs here—and you’ve set your pace appropriately—then you should go back and review the previous chapters.
6.1 The Definition of a Graph
The definition of a graph is best done by picture, as in Figure 6.1. Take some “things” and describe which things are “connected.” The result is a graph. As a simple example, the “things” might be airports, and two airports are “connected” if there is a flight between
69

Figure 6.1: An example of a graph

70

v2

v4

e6

e4 e2 v1

e5

e1

v5 e7 v3 e3

v6

Figure 6.2: A graph with labeled vertices and edges.

the two. Or the things are people and friends have connections. We draw the things and connections using dots and lines to erase the application from our minds. All we care about is the structure of the connections.
Let’s lay out the definitions, using sets as the modeling language. The “things” are called vertices (or often nodes) and the “connections” are called edges (or links). For shorthand in the definition, I’ll reuse a definition from Chapter 4 for the set of all ways to choose two things from a set.

() V 2 = {{v1, v2} : v1 ∈ V, v2 ∈ V, v1 ̸= v2}.

This is like V × V , but the order of the pair does not matter.

Definition 6.1. A graph G consists of a set V entire package is denoted G = (V, E).1

of vertices,

a set E

⊂

(V
2

)

of

edges.

The

Alternatively,

one

can

think

of

E

as

just

any

set,

and

require

a

function

f

:

E

→

(V )
2

to

describe which edges connect which pairs of vertices. This view is used when one wants

to define a graph in a context where the vertices are complicated. We will briefly see

one from compiler design later in this chapter. Despite the definition of an edge e ∈ E as a set of size two like {u, v}, mathematicians will sloppily write it as an ordered pair e = (u, v).2

Here’s some notation and terminology used for graphs. We always call n = |V | the number of vertices and m = |E| the number of edges, and for us these values will always be finite. When two vertices u, v ∈ V are connected by an edge e = (u, v) we call the

1 This is not the most general definition for a graph, but we will not need graphs with self loops, weights, double edges, or direction. You’ll explore some of these extensions in the exercises. 2 I have suspicions about why this abuse is commonplace. Curly braces are more cumbersome to draw than parentheses, and in the typesetting language LaTeX, typesetting braces requires an escape character. They’re also visually harder to parse when nested. Finally, directed edges use the ordering of a tuple.

71
two vertices adjacent, and we say that e is incident to u and v. We call v a neighbor of u and we define the neighborhood of a vertex N (u) to be the set of all neighbors; i.e.,
N (u) = {v ∈ V : (u, v) ∈ E}
The size of a neighborhood (and the number of incident edges) is called the degree of a vertex, and the function taking a vertex v to its degree is called deg : V → Z. To practice the new terms, see Figure 6.2, labeling the graph from Figure 6.1. Vertices have label ‘v’ and edges have label ‘e’. Vertices v1 v3 are adjacent, e2 is incident to v1, deg(v2) = 3, and all of the neighbors of v2 are also neighbors of v3.
Another concept we’ll need in this chapter is the concept of a connected graph. First, a path in a graph is a sequence of alternating vertices and edges (v1, e1, v2, e2, . . . vt) so that each ei = (vi, vi+1) connects the two vertices next to it in the list. Visually, a path is just a way to traverse through the vertices of G by following edges from vertex to vertex. In Figure 6.2, there are many different paths from v4 to v6, four of which do not repeat any vertices. Many authors enforce that paths do not repeat vertices by definition, and give the name “trail” or “walk” to a path which does repeat vertices. For us, the difference won’t matter. A cycle is a path that starts and ends at the same vertex.
A subgraph (H, F ) of a graph (G, E) if a choice of a subset of the vertices and edges of G which also forms a valid graph. I.e., H ⊂ G and F ⊂ E. Crucially, this requires that any edge e = (u, v) in H has both u and v in F . An induced subgraph has the additional property that if two vertices are adjacent in G, they must also be adjacent in H. In that way, the structure of an induced subgraph H is completely determined by the subset of vertices, which is why the term “induce” is appropriate.
A graph is called connected if there is a path from each vertex to each other vertex, and otherwise it is called disconnected. Equivalently (you will prove this in an exercise), G = (V, E) is connected if it is impossible to split V into two nonempty subsets X, Y with no edges between X and Y . A disconnected graph is a union of connected components, where the component of v is the largest connected subgraph containing v. A single vertex which forms a connected component is called an isolated vertex.
6.2 Graph Coloring
The main object of study in this chapter is called a coloring of a graph G = (V, E), which is an assignment of “colors” (really, numbers from {1, 2, . . . , k}) to the vertices of G satisfying some property. We realize this officially as a function.
Definition 6.2. A k-coloring of a graph G = (V, E) is a function φ : V → {1, 2, . . . , k}. We call an edge e = (u, v) properly colored by a k-coloring φ if φ(u) ̸= φ(v), and otherwise we call that edge improperly colored. We call φ proper if it properly colors every edge. If a graph G has a proper k-coloring, we call it k-colorable.
By now you should know to write down examples for small n and k before moving on. Because this is a crucial definition, here is a more complicated example. The Petersen

72
Figure 6.3: The Petersen graph.
graph is shown in Figure 6.3. The Petersen graph has a distinguished status in graph theory as a sort of smallest serious unit test. Conjectures that are false tend to fail on the Petersen graph.3 The Petersen graph is 3-colorable (find a 3-coloring!) but not 2colorable. Definition 6.3. The chromatic number of a graph G, denoted χ(G), is the minimum integer k for which G is k-colorable.
Recall from Chapter 4 that mathematicians often define functions without knowing how to compute them. The chromatic number is an excellent example. We define the concept to clarify what it is we want to study, and the modeling language of sets allows us to start to reason about it.
If you believe that the Petersen graph is not 2-colorable—or you do the exercise that proves this—then we know the Petersen graph has chromatic number 3. Here is a simple fact about the chromatic number. Proposition 6.4. If G = (V, E) is a graph and d is the largest degree of a vertex v ∈ V , then χ(G) ≤ d + 1. Proof. We define a greedy algorithm for coloring a graph. Pick an arbitrary ordering v1, . . . , vn of the vertices of G, and then for each vi pick the first color j which is unused by any of the neighbors of vi. In the worst case, a vertex v of degree d will have all of its neighbors using different colors, and so it will use color d + 1. Otherwise v could reuse one of the first d colors not used by any neighbor. So the worst-case number of colors is at most the largest degree in the graph plus one, as claimed.
3 Why? Part of it is that the Petersen graph is highly symmetric. We’ll revisit this in the exercises for Chapter 16.

73

Figure 6.4: A star graph.

Figure 6.5: A coloring of the Petersen graph.

A simple graph meets this bound and has χ(G) = maxv∈V deg(v) + 1. See if you can find it. On the other hand, this bound can be quite loose. Here “loose” means that there are graphs which meet the conditions of the proposition, but the true χ(G) is much smaller than the proposition enforces. Consider the “star” graph which has n vertices and only one vertex of degree n − 1, pictured in Figure 6.4. Clearly the star graph is 2-colorable, but the max degree is n − 1. The guarantee of the proposition is effectively useless.

One other perspective on graph coloring I want to describe is the partition perspective.

Specifically, if G = (V, E) is a graph and φ is a proper k-coloring, then we can look at

φ−1(j), the set of all among these vertices.

vertices that have color j. Since Moreover, since φ is a function,

tφheisseptro{pφe−r,1(thj)er:eja=re

no 1, .

edge}s ..,k

partitions4 V into “color classes,” and all the edges of G go between the color classes.

Figure 6.5 shows a picture for the Petersen graph.

This perspective can be used to design coloring algorithms. Start with an improper or unfinished coloring, and fiddle with it to correct the improprieties. We will do this in the main application of this chapter, coloring planar graphs. But right now we’re going to take a quick detour to see why graph coloring is useful.

6.3 Register Allocation and Hardness
The wishy-washy way to motivate graph coloring is to claim that many problems can be expressed as an “anti-coordination problem,” where you win when no agent in the system behaves the same as any of their neighbors. A totally made up example is radio
4 A partition of X is a set of non-overlapping (disjoint) subsets Ai ⊂ X the union of all of them being ∪iAi = X.

74
frequencies. Radio towers pick frequencies to broadcast, but if nearby towers are broadcasting on the same frequency, they will interfere. So the vertices of the graph are towers, nearby towers are connected by an edge, and the colors are frequencies.
A more interesting and satisfying application is register allocation. That is, suppose you’re writing a compiler for a programming language. Logically the programmer has no bound on the number of variables used in a program, but on the physical machine there is a constant number of CPU registers in which to store those variables. The register allocation algorithm must decide (at compile time) which registers will store which logical variables as the computation progresses, and which logical variables must be stored in memory. The less often you need to shuffle data back and forth between memory and CPU registers, the faster the program will run.
The connection to graph coloring is beginning to reveal itself: the vertices are the logical variables and the colors are physical registers, but I haven’t yet said how to connect two vertices by an edge. Intuitively, it depends on whether the logical variables “overlap” in the scope of their use. The structure of scope overlap is destined to be studied with graph theory.
To simplify things, we’ll do what a compiler designer might reasonably do, and compile a program down to almost assembly code, where the only difference is that we allow infinitely many “virtual” registers, which we’ll just call variables. So for a particular program P , there is a nP ∈ N that is the number of distinct variable names used in the program. Each of these integers is a vertex in G.
As an illustrative example, say that the almost-compiled program looks like this, where the dollar sign denotes a variable name:
whileBlock: $41 = $41 - 1 $40 = $40 + $42 $42 = $41 - $42 BranchIfZero $41 endBlock whileBlock
endBlock: $43 = $41 + $40
In this example variables 41 and 42 cannot share a physical register. They have different values and are used in the same line to compute a difference. Call a variable live at a statement in the code if its value is used after the end of that statement. Thinking of it in reverse: a variable is dead in all of the lines of code between when it was last read and when it is next written to. Whenever a variable is dead we know it’s safe to reuse its physical register (storing the value of the dead variable in memory).
Now we can define the edges. Two variables $i and $j “interfere,” and hence we add the edge (i, j) to G, if they are ever live at the same time in the program. With a bit of work (uncoincidentally using graphs to do a flow analysis), one can efficiently compute the places in the code where each variable is live and construct this graph G. Then if we can compute the chromatic number of G and find an actual χ(G)-coloring, we can

75

assign physical registers to the variables according to the coloring. Without some deeper semantic analysis, this provides the most efficient possible use of our physical registers.5

Unfortunately, in general you should not hope to compute the chromatic number of an arbitrary graph. This problem is what’s called “NP-hard,” which roughly means there is no known provably correct (in the worst case) and provably efficient algorithm for computing it. Moreover, if there were, the same algorithm could be adapted to solve a whole class of problems that are also believed to be intrinsically hard to solve. The notion of efficiency here is—as usual for algorithm analysis—in terms of the runtime compared to the size of the input as the input grows. This is called “asymptotic analysis” or “big-O.” See Chapter 15 for a longer discussion.

Moreover, it is even NP-hard to get any reasonable approximation of the chromatic

number of a general graph. To be more specific, we can’t hope to find an efficient and

provably correct algorithm for the following problem. Fix any c such that 0 < c < 1.

Given any graph G as input, if G has n vertices, output a number Z with the property

that

Z χ(G)

<

nc.

As mentioned, this is an asymptotic statement, meaning an algorithm that only works

for all graphs with fewer than a thousand nodes is not a solution. A lookup table, though

it would be massive, would solve this problem efficiently. No, a true solution must work

and must work efficiently for any arbitrarily large graph in principle, though working

on small graphs may be sufficient in practice.6 But to put the numbers in perspective

with an example, this theorem says that for graphs with n = 104 vertices and with

c = 1/2, algorithms will struggle to output a number guaranteed to be between χ(G)

and 100 · χ(G).

But I digress. The takeaway is that coloring is a hard problem. This is a sad result for people who really want to color their graphs, but there are other ways to attack the problem. You can assume that your graph has some nice structure. This is what we’ll do in the next section, and there it turns out that the chromatic number will always be at most 4. Alternatively, you could assume that you know your graph’s chromatic number, and try to color it without introducing too many improperly colored edges. We’ll see this approach in Section 6.6.

6.4 Planarity and the Euler Characteristic
The condition we’ll impose on a graph to make coloring easier is called planarity. A graph G = (V, E) is called planar if one can draw it on a plane in such a way that no edges cross. Figure 6.6 contains an example.
Here’s a little exercise: come up with an example of a graph which is not planar. Don’t
5 In fact, it can happen that the chromatic number of G is greater than the total number of registers on the target machine. In this case you have to spill some variables into memory. 6 If you had to compute the chromatic number of a graph in a practical setting, you’d probably write it as a so-called integer linear program and throw an industry-strength solver at it. As they say, NP-hard problems are hard in theory but easy in practice.

76
Figure 6.6: An example of a planar graph which can be drawn with no edges crossing.
be surprised if you’re struggling to prove that a given graph is not planar. You personally failing to draw a specific graph without edges crossing is not a proof that it is impossible to do so. There is a nice rule that characterizes planar graphs, but it is not trivial. See the chapter exercises for more.
Now that you’ve tried the exercise: Figure 6.7 depicts two important graphs that are not planar. The left one is called the complete graph on 5 vertices, denoted K5. The word “complete” here just means that all possible edges between vertices are present. The second graph is called the complete bipartite graph K3,3. “Bipartite” means “two parts,” and the completeness refers to all possible edges going between the two parts. The subscript of Ka,b for a, b ∈ N means there are a vertices in one part and b in the other.
We defined planar graphs informally in terms of drawings in the plane, which doesn’t use sets, functions, or anything you’ve come to expect. Indeed, the hand-wavy definition is the one that belongs in your head, but the official definition of a planar graph is one which has an embedding into R2. The problem is that defining an embedding requires opening a big can of worms, because it applies to spaces more general than a graph. We’ll give you a taste in the chapter notes.
One feature about planar graphs is that when you draw a planar graph in such a way that no edges cross, you get a division of R2 into distinct regions called “faces.” Figure 6.8 shows a graph with four faces, noting that by convention I’m calling the “outside” of the drawing also a face. If we call f the number of faces, and remember n is the number of vertices and m is the number of edges, then we can notice7 a nice little pattern: n − m + f = 2.
The amazing fact is that this equation does not depend on how you draw the graph! So long as your drawing has no crossing edges, the value n − m + f will always be 2. We can prove it quite simply with induction.
Theorem 6.5. For any connected planar graph G = (V, E) with at least one vertex, and any drawing of G in the plane R2 defining a set F of faces, the quantity |V |−|E|+|F | = 2.
7 Why anyone would have reason to analyze this quantity is a historical curiosity; it was discovered by Euler for certain geometric shapes in three dimensions called convex polyhedra. See the following for more: http:
//mathoverflow.net/q/154498/6429

77

K5

K3,3

Figure 6.7: K5 and K3,3, two graphs which are not planar.

F4

F2

F1

F3

Figure 6.8: Faces of a planar graph.

Proof. We proceed by induction on the total number of vertices and edges. The base case is a single isolated vertex, for which |V | = 1, |E| = 0, and |F | = 1, so the theorem works out.
Now suppose we have a graph G for which the theorem holds, i.e. |V | − |E| + |F | = 2, and we will make it larger and show that the theorem still holds. In particular, we will do induction on the quantity |V | + |E|. There are two cases: either we add a new edge connecting two existing vertices, or we add a new edge connected to a new vertex (which now has degree 1). Adding a vertex by itself is not allowed because the graph must stay connected at all times.
In the first case, |V | is unchanged, |E| increases by 1, and |F | also increases by one because the new edge cuts an existing face into two pieces. So
|V | − (|E| + 1) + (|F | + 1) = |V | − |E| + |F | = 2
Notice how it does not matter how we drew the edge, so long as it doesn’t cross any other edges to create more than one additional face. The second case is similar, except adding an edge connected to a new vertex does not create any new faces. Convince yourself that any vertex involved in a path that encloses a face has to have degree at least two. So again we get that for the new graph |V | + 1 − (|E| + 1) + |F | = 2. This finishes the inductive step.
Finally, it should be clear that every connected graph (regardless of whether or not it’s planar) can be built up by a sequence of adding edges by these two cases. This completes the proof.

This is a surprising fact. We have some measurement derived from a drawing of a graph that doesn’t depend on the choices made to draw it! This is called an invariant, and we’ll discuss invariants more in Chapter 10 when we study linear algebra, and Chapter 16 when we study geometry. For now it will remain a deep mathematical curiosity. Lastly,

78
note that the connectivity requirement is crucial for the theorem to hold, since a graph with n vertices and no edges has |V | − |E| + |F | = n + 1.

6.5 Application: the Five Color Theorem

Here is an amazing theorem about planar graphs.

Theorem 6.6. (The four color theorem) Every planar graph can be colored with 4 colors.

This was proved by Kenneth Appel and Wolfgang Haken in 1976 after being open for over a hundred years. You may have heard of it because of its notoriety: it was the first major theorem to be proved with substantial aid from a computer. Unfortunately the proof is very long and difficult (on the order of 400 pages of text!). Luckily for us there is a much easier theorem to prove.

Theorem 6.7. (The five color theorem) Every planar graph can be colored with 5 colors.

If you’re like me and frequently make off-by-one errors, then the five color theorem is just as good as the four color theorem. In order to prove it we need three short lemmas.

Lemma 6.8.

If G is a graph with m edges, then 2m

=

∑
v∈V

deg(v).

Proof. The important observation is that the degree of a vertex is just the number of edges

incident to it, and every edge is incident to exactly two vertices.

This is where the proof would usually end. As a variation on a theme, you can (and

should) think of this as constructing a clever bijection like we did in Chapter 4, but it’s

difficult to clearly define a domain and codomain. Let me try: the domain consists of

“edge stubs” sticking out from each vertex, and the codomain is the set of edges E. We’re

mapping a double

each edge stub cover of E, and

to the edge the size of

that contains the domain is

tehxaatcstltyub∑. Tvh∈iVs

map is a deg(v).

surjection

and

Lemma 6.9. If a planar graph G has m ≥ 2 edges and f faces, then 2m ≥ 3f , i.e., f ≤ (2/3)m.
Proof. Pick your favorite embedding (drawing) of G in the plane. We’ll use a similar counting argument as in Lemma 6.8: for any planar drawing, every face is enclosed by at least three edges, and every edge touches at most two faces.8 In other words, each face is “counted” by each edge it touches, and each face has at least three edges counting it. Hence 3f counts each edge at most twice, while 2m counts each face at least three times.

8 An edge incident to a vertex of degree 1 will touch the “outside” face twice, but this only counts as one face.

79
The requirement that m ≥ 2 is necessary, since if there is only one edge (or zero), then the outside face is the only face. It only gets “counted” twice (or zero times) by the edges it touches. Once we get to two edges, the outside face is counted twice (2m = 4). As you add more edges, either you add dangling edges (or subdivide existing edges) which increases 2m but not 3f , or you add edges that create new faces. In the case of a single edge creating a single new face, the lower bound 3f increases by exactly 3, but the upper bound 2m only increases by 2.
Despite having just read a proof, this may be surprising: can’t we keep adding facecreating edges to make the lower bound of 3f exceed the upper bound of 2m? It’s instructive to take a moment and play with examples. You’ll eventually get to a situation in which all interior faces are triangles, and the inequality is either an equality or very close. Then the creation of new faces requires a sufficient number of non-face-creating edges to be made first, which loosens the inequality. The proof above explains how this loosening and tightening of the inequality corresponds to the geometry of a graph drawn in the plane. It translates the geometry to algebra. When the algebra seems to misbehave, we can call back to the geometry to understand.
You should do what I did for Lemma 6.8 and think about how to express this as an injection from one set to another. The last lemma is the key to the five color theorem.
Lemma 6.10. Every planar graph has a vertex of degree 5 or less.
Proof. Suppose to the contrary that every vertex of G = (V, E) has degree 6 or more. Substituting the inequality from Lemma 6.9 into the Euler characteristic equation gives
2 = |V | − |E| + |F | ≤ |V | − |E| + (2/3)|E|
Rearranging terms to solve for |E| gives |E| ≤ 3|V | − 6. Now we want to use Lemma 6.8, so we multiply by two to get 2|E| ≤ 6|V | − 12. Since 2|E| is the sum of the degrees, and each vertex has degree at least six, 2|E| is at least as large as 6|V |. Adding this to the above inequality gives
6|V | ≤ 2|E| ≤ 6|V | − 12,
which is a contradiction.
As a quick side note that we’ll need in the next theorem, along the way to proving Lemma 6.10 we get a bonus fact: the complete graph K5 is not planar. This is because we proved that all planar graphs satisfy |E| ≤ 3|V | − 6, and for K5, |E| = 10 > 15 − 6. This argument doesn’t work for showing K3,3 is not planar, but if you’re willing to do a bit extra work (and take advantage of the fact that K3,3 has no cycles of length 3), then you can improve the bound from Lemma 6.10 to work. In particular, because K5 is not planar, no planar graph can contain K5 as a subgraph.
Now we can prove the five color theorem.

80
Proof. By induction on |V |. For the base case, every graph which has 5 or fewer vertices is 5-colorable by using a different color for each vertex.
Now let |V | ≥ 6. By Lemma 6.10, G has a vertex v of degree at most 5. If we remove v from G then the inductive hypothesis guarantees us a 5-coloring. We want to extend or modify this coloring and in doing so properly color v. This will finish the proof. When v has degree at most 4, choose one of the unused colors among v’s neighbors. Otherwise v has degree exactly 5, and we have to be more clever, because the neighbors may need all 5 colors a priori.
Call v’s five neighbors w1, w2, w3, w4, w5. Because K5 is not planar and G is, these five neighbors can’t form K5. In particular there must be some i, j for which wi and wj are not adjacent. We can form a new graph G′ (“G prime”9) by deleting v and merging wi and wj, i.e., delete v, wi, wj and add a new vertex x which is adjacent to all the remaining vertices in N (wi) ∪ N (wj). I claim that if G′ is planar then we’re done: G′ has |V | − 1 vertices and so it has a 5-coloring by the inductive hypothesis, and we can use that 5-coloring to color most of G (everything except wi, wj, and v). Then use the color assigned to x for both wi and wj; they had no edge between them in G, so this coloring is proper. These choices ensure the neighbors of v use only 4 of the 5 colors, so finally pick the unused color for v. This produces a proper coloring of G.
So why is G′ planar? To argue this, we can show that for any planar drawing of G, removing v leaves wi and wj in the same face. This is equivalent to being able to trace a curve in the plane from wi to wj without hitting any other edges, since we could then “drag” wi along that curve to wj and “lengthen” the edges incident to wi as we go. When the two vertices merge, and “become” x, we get a planar drawing of G′. The picture in my head is like the strands of a spider web, shown in Figure 6.9.
The key is that G is planar and that v has all of the w’s as neighbors. If we want to merge wi to wj, we can use the curve already traced by the edges from wi to v and from v to wj. By planarity this is guaranteed not to cross any of the other edges of G, and hence of G′. To say it a different way, if we took the drawing above and continued drawing G′, and the result required an edge to cross one of the edges above, then it would have crossed through one of the edges going from v to wi or v to wj!
This proves G′ is planar, which completes the proof.
That proof neatly translates into a recursive algorithm for 5-coloring a planar graph. We’ll finish this section with Python code implementing it. In order to avoid the toil of writing custom data structures for graphs, we’ll use a Python library called igraph to handle our data representation. As a very quick introduction, one can create graphs in igraph as follows.
9 The tick is called the “prime” symbol, and it is used to denote that two things are closely related, usually that the prime’d thing is a minor variation on the un-primed thing. So using G′ here is a reminder to the reader that G′ was constructed from G.

81

wi

wi

v

v

wi= wj

wj

wj

Figure 6.9: The “strands of a spider web” image guide the proof that G′ is planar.

import igraph G = igraph.Graph(n=10) G.add_edges([(0,1), (1,2), (4,5)]) G.vs # a list-like sequence of vertices G.es # a list-like sequence of edges
For example, given a graph and a list of nodes in the graph, one might use the following function to find two nodes which are not adjacent.
from itertools import combinations def find_two_nonadjacent(graph, nodes):
for x, y in combinations(nodes, 2): if not graph.are_connected(x, y): return x, y
Also, the vertices of an igraph graph can have arbitrary “attributes” that are assigned like dictionary indexing. We use this to assign colors to the vertices, using [ ]. For example, this is the base case of our induction: trivially color each vertex of a ≤ 5 vertex graph with all different colors.

82
colors = list(range(5))
def planar_five_color(graph): n = len(graph.vs) if n <= 5: graph.vs['color'] = colors[:n] return graph
...
The igraph library overloads the assignment operator to allow for entry-wise assignments by assigning one list to another. So in the statement G.vs['color'] = colors[:n], the nodes of G are being assigned the first n colors in the list of colors.
The rest of the planar_five_color function involves finding the vertices of the needed degree, forming the graph G′ to recursively color, and keeping track of which vertices were modified to make G′ so you can use its coloring to color G.
Here is the part where we find vertices of the right degree and do bookkeeping:
deg_at_most5_nodes = graph.vs.select(_degree_le=5) deg_at_most4_nodes = deg_at_most5_nodes.select(_degree_le=4) deg5_nodes = deg_at_most5_nodes.select(_degree_eq=5)
g_prime = graph.copy() g_prime.vs['old_index'] = list(range(n))
The select functions are igraph-specific: they allow one to filter a vertex list by various built-in predicates, such as whether the degree of the vertex is equal to 5. The old_index attribute keeps track of which vertex in G′ corresponded to which vertex in G, since when you modify the vertex set of an igraph the locations of the vertices within the data structure change (which changes the index in the list of all vertices).
Next we construct G′. This is where the two cases in the proof show up.
if len(deg_at_most4_nodes) > 0: v = deg_at_most4_nodes[0] g_prime.delete_vertices(v.index)
else: v = deg5_nodes[0] neighbor_indices = [x['old_index'] for x in g_prime.vs[v.index].neighbors()]
g_prime.delete_vertices(v.index) neighbors_in_g_prime = g_prime.vs.select(old_index_in=neighbor_indices)
w1, w2 = find_two_nonadjacent(g_prime, neighbors_in_g_prime) merge_two(g_prime, w1, w2)
We implemented a function called merge_two that merges two vertices, but the implementation is technical and not interesting. The official igraph function we used is called contract_vertices. The remainder of the algorithm executes the recursive call, and

83
Figure 6.10: A planar graph which is 5-regular.
then copies the coloring back to G, computing the first unused color with which to color the originally deleted vertex v.
colored_g_prime = planar_five_color(g_prime) for w in colored_g_prime.vs:
# subset selection handles the merged w1, w2 with one assignment graph.vs[w['old_index']]['color'] = w['color'] neighbor_colors = set(w['color'] for w in v.neighbors()) v['color'] = [j for j in colors if j not in neighbor_colors][0] return graph
The entire program is in the Github repository for this book.10 The second case of the algorithm is not trivial to test. One needs to come up with a graph which is planar, and hence has some vertex of degree 5, but has no vertices of degree 4 or less. Indeed, there is a planar graph in which every vertex has degree 5. Figure 6.10 shows one that I included as a unit test in the repository.
6.6 Approximate Coloring
Earlier I remarked that coloring is probably too hard for algorithms to solve in the worst case. To get around the problem we added the planarity constraint. Though a practical coloring algorithm would likely use an industry standard optimization engine to approximately color graphs, let’s try something different to see the theory around graph coloring. Suppose we’re promised a graph can be colored with 3 colors, and let’s
10 See pimbook.org.

84

try to color The first

it with some larger number of colors.11 algorithm of this kind colors a 3-colorable

graph

with

√ 4n

colors,

where

n = |V |. To make the numbers concrete, for a 3-colorable graph with 1000 vertices, this

algorithm will use no more than 127 quite simple. As long as there is an

uconlcoorlso.reSdouvnedrtsepxrvettwyitrhotdteeng,rebeutatthleeaasltgo√rnith, mpiciks

three new colors. vertices from the

Use one for v, and graph and repeat.

tIfhethoetrheearrtewnootovecrotliocersNo(fvd)e. gTrheeen√renm, tohveenaullstehtehsee

greedy algorithm to color the remaining graph.

Theorem 6.11.

This

algorithm

colors any 3-colorable

graph using at

most

√ 4⌈ n⌉

colors.12

P≥ro√ofn. ,LweteGhabveeat3o-cporloovreabtlheagtrtahpeh.nFeiogrhtbhoerfhirosotdcaNse(,vw) hcearne

there is a vertex be colored with

v of two

degree colors.

But this follows from the assumption that G is 3-colorable: in any 3-coloring of G, v uses

mma cooIosrfletoth⌈trh√etarhnena⌉ta⌈r−√neon1nn,⌉oeaconvofedloritwtrsisceneoepsnirgoothfhvbidesoderggisrrnaempePahr≥yo. pu√osseni.t,iOotnnhle6yn.4ttwhthoeacmtotalhoxerismgrrueemmedadyienag.lrgeoeriothf ma

vertex is at will use no

v√mmeoonrNtssitttociwm43es⌈√ew√fsnr,enobcmh⌉eo,aclavGaoseurs(sdt√.eoesTenciahor·ceeuh√dng.ttnriemhe=oedwywnm)ea.lcagSonoinlyrocirtcehvowmlaoenrusdasdgeitdesst3anutensmieegdwohsbtcotoot⌈rals√ol,.rwnsT⌉ehinecreoefmilarocsorhtsv,cesasttesohepoi,ncstaehtnoi√stoanpnla+lwyrt1ehua≥gspeep√steaanntt

One

might

naturally

ask

whether

we

can

improve

√ n

to

something

like

log(n),

or

even

some very large constant. This is actually an open question. Recent breakthroughs using

a technique called semidefinite programming got the number of colors down to roughly

ann0.2im. Fporrovreefmereenntcoev, aerth1o27uscaonlodr-ns ogdivee3n-cboylotrhaeb4le√gnrabpohuwndo.uld have n0.2 ≈ 4. That’s quite

I should make a clarification here: the open problem is on the existence of an algorithm

which is guaranteed to achieve some number of colors (depending on the size of the

graph) no matter what the graph is. As a programmer you are probably somewhat familiar

with this idea that one often measures an algorithm by its worst-case guarantees, but

the point is important enough to emphasize. So when I say a problem is “possible” or

“impossible” to solve, I mean that there exists (or does not exist, respectively) an efficient

algorithm that achieves the desired worst-case guarantee on all inputs. In particular,

there is no evidence for either claim that it is possible or impossible to color a 3-colorable

graph with log(n) colors (or anything close to that order of magnitude, like (log(n))10).

A ripe problem indeed.

11 Ideally we might hope to color a 3-colorable graph with 4 colors, but this was shown to be NP-hard as well. See http://dl.acm.org/citation.cfm?id=793420.
12 The symbols ⌈−⌉ denotes the ceiling of the argument, which is the smallest integer greater than or equal to the input. Similarly, ⌊−⌋ denotes the floor. These are mathematical ways to say round up or down.

85
6.7 Cultural Review
1. Invariants are measurements intrinsic to a concept, which don’t depend on the choices made for some particular representation of that concept.
2. Sometimes if you want to come up with the right rigorous definition for an intuitive concept (like a planar graph), you need to develop a much more general framework for that concept. But in the mean time, you can still do mathematics with the informal notion.
3. Every conjecture about graphs must be tested on the Petersen graph.
6.8 Exercises
6.1. Write down examples for the following definitions. A graph is a tree if it contains no cycles. Two graphs G, H are isomorphic if they differ only by relabeling their vertices. That is, if G = (V, E) and H = (V ′, E′), then G and H are isomorphic if there is a bijection f : V → V ′ with the property that (i, j) ∈ E if and only if (f (i), f (j)) ∈ E′. Given a subset of vertices S ⊂ V of a graph G = (V, E), the induced subgraph on S is the subgraph consisting of all edges with both endpoints in S. Given a vertex v of degree 2, one can contract it by removing it and “connecting its two edges,” i.e., the two edges (v, w), (v, u) become (w, u). Likewise, one can contract an edge by merging its endpoint vertices, or subdivide an edge by adding a vertex of degree two in the middle of an edge. If H can be obtained from G after some sequence of contractions and subdivisions, it is called a minor of G.
6.2. Look up the statement of Wagner’s theorem, which characterizes planar graphs in terms of contractions and the two graphs K3,3 and K5. Find a proof you can understand.
6.3. In Section 6.1 we claimed that the following two definitions of a connected graph are equivalent: (1) there is a path between every pair of vertices, (2) it is impossible to split V into two nonempty subsets X, Y such that no edge e = (a, b) has a ∈ X and b ∈ Y . Prove this.
6.4. Here’s a simple way to make examples of planar graphs: draw some nonoverlapping circles of various sizes on a piece of paper, call the circles vertices, and put an edge between any two circles that touch each other. Clearly the result is going to be a planar graph, but an interesting question is whether every planar graph can be made with this method. Amazingly the answer is yes! This is called Koebe’s theorem. It is a relatively difficult theorem to prove for the intended reader of this book, but as a consequence it implies Fáry’s theorem. Fáry’s theorem states that every planar graph can be drawn so that the edges are all straight lines. Look up a proof of Fáry’s theorem that uses Koebe’s theorem as a starting point, and rewrite it in your own words.

86
6.5. Given a graph G, the chromatic polynomial of G, denoted PG(x), is the unique polynomial which, when evaluated at an integer k ≥ 0, computes the number of proper colorings of G with k colors. Compute the chromatic polynomial for a path on n vertices, a cycle on n vertices, and the complete graph on n vertices. Look up the chromatic polynomial for the Petersen graph.
6.6. Look up a recursive definition of the chromatic polynomial of a graph in terms of edge contractions, and write a program that computes the chromatic polynomial (for small graphs). Think about a heuristic that can be used to speed up the algorithm by cleverly choosing an edge to contract.
6.7. In the chapter I remarked that the Euler characteristic is a special quantity because it is an invariant. Look up a source that explains why the Euler characteristic is special.
6.8. Find a simple property that distinguishes 2-colorable graphs from graphs that are not 2-colorable. Write a program which, when given a graph as input, determines if it is 2-colorable and outputs a coloring if it is.
6.9. Implement the algorithm presented in the chapter to (4√n)-color a 3-colorable graph. Use the 2-coloring algorithm from the previous problem as a subroutine.
6.10. A directed graph is a graph in which edges are oriented (i.e., they’re ordered pairs instead of unordered pairs). The endpoints of an edge e = (u, v) are distinguished as the source u and the target v. A directed graph gives rise to natural directed paths, which are like normal paths, but you can only follow edges from source to target. A graph is called strongly connected if every pair of vertices is connected by a directed path. Write a program that determines if a given directed graph is strongly connected.
6.11. A directed acyclic graph (DAG) is a directed graph which has no directed cycles (paths that start and end at the same vertex). DAGs are commonly used to represent dependencies in software systems. Often, one needs to resolve dependencies by evaluating them in order so that no vertex is evaluated before all of its dependencies have been evaluated. One often solves this problem by sorting the vertices using what’s called a “topological” sort, which guarantees every vertex occurs before any downstream dependency. Write a program that produces a topological sort of a given DAG.
6.12. A weighted graph is a graph G for which each edge is assigned a number we ∈ R. Weights on edges often represent capacities, such as the capacity of traffic flow in a road network. Look up a description of the maximum flow problem in directed, weighted graphs, and the Ford-Fulkerson algorithm which solves it. Specifically, observe how the maximum flow problem is modeled using a graph. Find real-world problems that are solved via a max flow problem.
6.13. A hypergraph generalizes the size of an edge to contain more than two vertices.

87

Hypergraphs are also called set systems or families of sets. Edges of a hypergraph are

called hyperedges, and a k−uniform hypergraph is one in which all of its hyperedges have

size k. Look up a proof of the Erdős-Ko-Rado theorem: let G be a k-uniform hypergraph

weGxiathhcatnslya≥wt mh2ekonsvtenr(t>nki−−ce211s)k, .ihnywpehriecdhgeevseirny

pair of hyperedges shares a vertex in common. Then total. Find a construction that achieves this bound

6.9 Chapter Notes
Some Topology and the Rigorous Definition of an Embedding
The reason a planar graph is so hard to define rigorously is because the right definition of what it means to “draw” one thing inside another is deep and deserves to be defined in general. And such a definition requires some amount of topology, the subfield of mathematics that deals with the intrinsic shape of space without necessarily having the ability to measure distances or angles.
If you really pressed me to define a planar graph without appealing to topology I could do it with a tiny bit of calculus. Here it goes.
Definition 6.12. An embedding of a graph G = (V, E) in the plane is a set of continuous functions fe : [0, 1] → R2 for each edge e ∈ E mapping the unit interval to the plane with the following properties:
• Every fe is injective.
• There are no two fe1, fe2 and values 0 < t1, t2 < 1 for which fe1(t1) = fe2(t2), i.e., the images of fe1 and fe2 do not intersect except possibly at their endpoints.
• Whenever there are two edges (u, v) and (u, w), the corresponding functions must intersect at one endpoint, and these intersections must be consistent across all the vertices. I.e., every u ∈ V corresponds to a point xu ∈ R2 such that for every edge (u, v) incident to u, either f(u,v)(0) = xu or f(u,v)(1) = xu.
Disgusting! Why did you make me do that? The problem is that the definition is full of a bunch of “except” and special cases (like that the endpoint could either be zero or one). This makes for ugly mathematics, and the mathematical perspective is to spend a little bit more time understanding exactly what we want from this definition. We are humans, after all, who are inventing this mathematics so that we can explain our ideas easily to others and appreciate the beautiful proofs and algorithms. Keeping track of edge cases is dreary. We really want to define an embedding as a single function f whose codomain is R2. And because we said we don’t want any of the edges to cross each other in the plane, we probably want f to be injective. Finally, because the drawing has to be a sensible drawing, we need f to be continuous. Recall from calculus that a continuous function intuitively maps points that are “close together” in the domain to points that remain close together

